{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71b80e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "os.chdir(\"/home/hania/mi2/ai-stretegies-papers-regulations-monitoring/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eeecac95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hania/mi2/ai-stretegies-papers-regulations-monitoring/.venv/bin/python\n",
      "3.7.1 (default, Jul 16 2021, 17:39:59) \n",
      "[GCC 9.3.0]\n",
      "sys.version_info(major=3, minor=7, micro=1, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.version)\n",
    "print(sys.version_info)\n",
    "# !python3 -m spacy download en_core_web_sm\n",
    "# !which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26183abd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import os\n",
    "from mars.scraper.utils import parse_article\n",
    "from collections import defaultdict, Counter\n",
    "from mars.scraper.utils import parse_article\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "texts = defaultdict(lambda: defaultdict(dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fab7ab5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "newspaper_counts = []\n",
    "for filename in os.listdir(\"./data/oecd_html\"):\n",
    "    try:\n",
    "        res = parse_article(\"./data/oecd_html\" + \"/\" + filename)\n",
    "        doc = nlp(res[\"text\"])\n",
    "        texts[filename][\"newspaper3k\"][\"doc\"] = doc\n",
    "        newspaper_counts.append(len(doc))\n",
    "        texts[filename][\"newspaper3k\"][\"len\"] = len(doc)\n",
    "    except:\n",
    "        print(\"failed %s\" %filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4f7fc88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [20, 8]\n",
    "plt.hist(newspaper_counts, alpha=0.5, bins = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a29b039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = [20, 8]\n",
    "plt.hist(newspaper_counts, alpha=0.5, bins = np.arange(0,1000,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d55d149",
   "metadata": {},
   "source": [
    "# dragnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f26b84a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dragnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fce224d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_counts = []\n",
    "for filename in os.listdir(\"./data/oecd_html\"):\n",
    "    try:\n",
    "        with open(\"./data/oecd_html\" + \"/\" + filename) as f:\n",
    "            content = f.read()\n",
    "            \n",
    "        content = dragnet.extract_content(content)     \n",
    "        doc = nlp(content)\n",
    "        texts[filename][\"dragnet\"][\"doc\"] = doc\n",
    "        content_counts.append(len(doc))\n",
    "        texts[filename][\"dragnet\"][\"len\"] = len(doc)\n",
    "    except:\n",
    "        print(\"failed %s\" %filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ceacddaf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = [20, 8]\n",
    "plt.hist(content_counts, alpha=0.5, bins = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7d5f0e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = [20, 8]\n",
    "plt.hist(newspaper_counts, alpha=0.5, bins = np.arange(0,1000,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0d63d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59399636",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in texts.keys():\n",
    "    try:\n",
    "        lens[key] = abs(texts[key][\"dragnet\"][\"len\"] - texts[key][\"newspaper3k\"][\"len\"])\n",
    "    except:\n",
    "        print(\"failed %s\" %key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd4c316f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('223.html', 0),\n",
       " ('585.html', 0),\n",
       " ('430.html', 0),\n",
       " ('54.html', 0),\n",
       " ('4.html', 0),\n",
       " ('57.html', 0),\n",
       " ('595.html', 0),\n",
       " ('196.html', 0),\n",
       " ('486.html', 0),\n",
       " ('79.html', 0),\n",
       " ('467.html', 0),\n",
       " ('2.html', 0),\n",
       " ('262.html', 0),\n",
       " ('602.html', 0),\n",
       " ('524.html', 0),\n",
       " ('29.html', 0),\n",
       " ('61.html', 0),\n",
       " ('352.html', 0),\n",
       " ('591.html', 0),\n",
       " ('110.html', 0),\n",
       " ('418.html', 0),\n",
       " ('32.html', 0),\n",
       " ('605.html', 0),\n",
       " ('317.html', 0),\n",
       " ('250.html', 0),\n",
       " ('217.html', 0),\n",
       " ('363.html', 0),\n",
       " ('532.html', 0),\n",
       " ('198.html', 0),\n",
       " ('50.html', 0),\n",
       " ('273.html', 0),\n",
       " ('441.html', 0),\n",
       " ('475.html', 0),\n",
       " ('70.html', 0),\n",
       " ('496.html', 0),\n",
       " ('53.html', 0),\n",
       " ('25.html', 0),\n",
       " ('360.html', 0),\n",
       " ('552.html', 0),\n",
       " ('168.html', 0),\n",
       " ('259.html', 0),\n",
       " ('499.html', 0),\n",
       " ('46.html', 0),\n",
       " ('143.html', 0),\n",
       " ('402.html', 0),\n",
       " ('428.html', 0),\n",
       " ('90.html', 0),\n",
       " ('588.html', 0),\n",
       " ('346.html', 0),\n",
       " ('164.html', 0),\n",
       " ('427.html', 0),\n",
       " ('411.html', 0),\n",
       " ('476.html', 0),\n",
       " ('228.html', 1),\n",
       " ('530.html', 1),\n",
       " ('226.html', 1),\n",
       " ('229.html', 1),\n",
       " ('577.html', 1),\n",
       " ('540.html', 1),\n",
       " ('488.html', 1),\n",
       " ('243.html', 2),\n",
       " ('484.html', 2),\n",
       " ('465.html', 2),\n",
       " ('331.html', 2),\n",
       " ('296.html', 2),\n",
       " ('570.html', 2),\n",
       " ('434.html', 2),\n",
       " ('63.html', 3),\n",
       " ('114.html', 3),\n",
       " ('181.html', 3),\n",
       " ('613.html', 3),\n",
       " ('8.html', 3),\n",
       " ('301.html', 3),\n",
       " ('180.html', 3),\n",
       " ('248.html', 4),\n",
       " ('154.html', 4),\n",
       " ('487.html', 4),\n",
       " ('479.html', 4),\n",
       " ('470.html', 4),\n",
       " ('586.html', 5),\n",
       " ('252.html', 5),\n",
       " ('561.html', 5),\n",
       " ('389.html', 5),\n",
       " ('407.html', 5),\n",
       " ('253.html', 5),\n",
       " ('235.html', 6),\n",
       " ('41.html', 6),\n",
       " ('369.html', 6),\n",
       " ('574.html', 6),\n",
       " ('276.html', 7),\n",
       " ('578.html', 7),\n",
       " ('365.html', 7),\n",
       " ('238.html', 8),\n",
       " ('415.html', 8),\n",
       " ('3.html', 8),\n",
       " ('247.html', 8),\n",
       " ('382.html', 9),\n",
       " ('28.html', 9),\n",
       " ('171.html', 9),\n",
       " ('506.html', 9),\n",
       " ('371.html', 10),\n",
       " ('241.html', 10),\n",
       " ('357.html', 10),\n",
       " ('587.html', 10),\n",
       " ('323.html', 11),\n",
       " ('405.html', 11),\n",
       " ('334.html', 11),\n",
       " ('277.html', 11),\n",
       " ('620.html', 11),\n",
       " ('518.html', 11),\n",
       " ('278.html', 11),\n",
       " ('627.html', 11),\n",
       " ('35.html', 11),\n",
       " ('523.html', 12),\n",
       " ('435.html', 12),\n",
       " ('367.html', 13),\n",
       " ('437.html', 13),\n",
       " ('393.html', 13),\n",
       " ('594.html', 13),\n",
       " ('45.html', 13),\n",
       " ('157.html', 13),\n",
       " ('355.html', 14),\n",
       " ('96.html', 14),\n",
       " ('17.html', 14),\n",
       " ('450.html', 14),\n",
       " ('616.html', 14),\n",
       " ('417.html', 15),\n",
       " ('404.html', 15),\n",
       " ('546.html', 16),\n",
       " ('330.html', 16),\n",
       " ('11.html', 16),\n",
       " ('554.html', 16),\n",
       " ('227.html', 16),\n",
       " ('290.html', 16),\n",
       " ('456.html', 16),\n",
       " ('97.html', 17),\n",
       " ('279.html', 17),\n",
       " ('311.html', 17),\n",
       " ('93.html', 18),\n",
       " ('254.html', 18),\n",
       " ('286.html', 19),\n",
       " ('236.html', 19),\n",
       " ('284.html', 19),\n",
       " ('413.html', 19),\n",
       " ('576.html', 20),\n",
       " ('193.html', 21),\n",
       " ('504.html', 21),\n",
       " ('294.html', 21),\n",
       " ('81.html', 21),\n",
       " ('544.html', 21),\n",
       " ('72.html', 22),\n",
       " ('461.html', 22),\n",
       " ('307.html', 22),\n",
       " ('115.html', 22),\n",
       " ('375.html', 23),\n",
       " ('224.html', 23),\n",
       " ('285.html', 23),\n",
       " ('15.html', 23),\n",
       " ('282.html', 23),\n",
       " ('27.html', 24),\n",
       " ('581.html', 24),\n",
       " ('608.html', 24),\n",
       " ('78.html', 24),\n",
       " ('218.html', 25),\n",
       " ('443.html', 25),\n",
       " ('47.html', 26),\n",
       " ('542.html', 26),\n",
       " ('173.html', 26),\n",
       " ('556.html', 26),\n",
       " ('557.html', 27),\n",
       " ('582.html', 27),\n",
       " ('492.html', 27),\n",
       " ('234.html', 27),\n",
       " ('390.html', 29),\n",
       " ('268.html', 29),\n",
       " ('362.html', 29),\n",
       " ('271.html', 30),\n",
       " ('350.html', 30),\n",
       " ('511.html', 30),\n",
       " ('446.html', 30),\n",
       " ('505.html', 32),\n",
       " ('416.html', 33),\n",
       " ('464.html', 34),\n",
       " ('153.html', 34),\n",
       " ('333.html', 34),\n",
       " ('188.html', 35),\n",
       " ('555.html', 35),\n",
       " ('150.html', 35),\n",
       " ('74.html', 35),\n",
       " ('201.html', 36),\n",
       " ('538.html', 37),\n",
       " ('308.html', 37),\n",
       " ('424.html', 37),\n",
       " ('558.html', 39),\n",
       " ('370.html', 39),\n",
       " ('473.html', 44),\n",
       " ('580.html', 44),\n",
       " ('619.html', 44),\n",
       " ('113.html', 44),\n",
       " ('231.html', 45),\n",
       " ('489.html', 46),\n",
       " ('566.html', 47),\n",
       " ('563.html', 48),\n",
       " ('200.html', 49),\n",
       " ('211.html', 49),\n",
       " ('144.html', 50),\n",
       " ('410.html', 51),\n",
       " ('158.html', 53),\n",
       " ('117.html', 54),\n",
       " ('118.html', 54),\n",
       " ('289.html', 55),\n",
       " ('398.html', 55),\n",
       " ('391.html', 57),\n",
       " ('205.html', 59),\n",
       " ('274.html', 60),\n",
       " ('329.html', 60),\n",
       " ('327.html', 61),\n",
       " ('493.html', 62),\n",
       " ('99.html', 67),\n",
       " ('288.html', 69),\n",
       " ('596.html', 69),\n",
       " ('445.html', 70),\n",
       " ('618.html', 73),\n",
       " ('83.html', 73),\n",
       " ('364.html', 76),\n",
       " ('267.html', 76),\n",
       " ('293.html', 77),\n",
       " ('474.html', 77),\n",
       " ('372.html', 78),\n",
       " ('401.html', 79),\n",
       " ('281.html', 82),\n",
       " ('163.html', 83),\n",
       " ('403.html', 84),\n",
       " ('151.html', 84),\n",
       " ('69.html', 86),\n",
       " ('119.html', 87),\n",
       " ('494.html', 88),\n",
       " ('212.html', 91),\n",
       " ('269.html', 91),\n",
       " ('395.html', 93),\n",
       " ('9.html', 93),\n",
       " ('440.html', 95),\n",
       " ('19.html', 98),\n",
       " ('432.html', 100),\n",
       " ('116.html', 101),\n",
       " ('298.html', 102),\n",
       " ('221.html', 105),\n",
       " ('354.html', 105),\n",
       " ('23.html', 106),\n",
       " ('601.html', 106),\n",
       " ('43.html', 108),\n",
       " ('42.html', 108),\n",
       " ('623.html', 115),\n",
       " ('500.html', 115),\n",
       " ('127.html', 118),\n",
       " ('184.html', 118),\n",
       " ('242.html', 119),\n",
       " ('521.html', 125),\n",
       " ('170.html', 125),\n",
       " ('155.html', 131),\n",
       " ('469.html', 134),\n",
       " ('351.html', 136),\n",
       " ('607.html', 136),\n",
       " ('56.html', 144),\n",
       " ('314.html', 145),\n",
       " ('304.html', 155),\n",
       " ('503.html', 156),\n",
       " ('175.html', 164),\n",
       " ('589.html', 166),\n",
       " ('457.html', 169),\n",
       " ('13.html', 177),\n",
       " ('361.html', 178),\n",
       " ('148.html', 185),\n",
       " ('245.html', 185),\n",
       " ('527.html', 199),\n",
       " ('509.html', 202),\n",
       " ('565.html', 203),\n",
       " ('610.html', 210),\n",
       " ('16.html', 210),\n",
       " ('356.html', 211),\n",
       " ('384.html', 214),\n",
       " ('167.html', 217),\n",
       " ('305.html', 217),\n",
       " ('481.html', 239),\n",
       " ('444.html', 239),\n",
       " ('165.html', 254),\n",
       " ('541.html', 256),\n",
       " ('326.html', 257),\n",
       " ('525.html', 258),\n",
       " ('260.html', 258),\n",
       " ('380.html', 260),\n",
       " ('263.html', 269),\n",
       " ('10.html', 275),\n",
       " ('51.html', 276),\n",
       " ('36.html', 285),\n",
       " ('529.html', 290),\n",
       " ('335.html', 290),\n",
       " ('136.html', 303),\n",
       " ('624.html', 313),\n",
       " ('353.html', 320),\n",
       " ('482.html', 321),\n",
       " ('270.html', 333),\n",
       " ('62.html', 344),\n",
       " ('571.html', 379),\n",
       " ('386.html', 383),\n",
       " ('592.html', 415),\n",
       " ('156.html', 426),\n",
       " ('124.html', 428),\n",
       " ('244.html', 440),\n",
       " ('462.html', 463),\n",
       " ('455.html', 490),\n",
       " ('572.html', 498),\n",
       " ('26.html', 507),\n",
       " ('219.html', 513),\n",
       " ('98.html', 549),\n",
       " ('536.html', 554),\n",
       " ('468.html', 568),\n",
       " ('65.html', 591),\n",
       " ('447.html', 592),\n",
       " ('537.html', 619),\n",
       " ('345.html', 648),\n",
       " ('514.html', 649),\n",
       " ('517.html', 649),\n",
       " ('460.html', 654),\n",
       " ('490.html', 666),\n",
       " ('89.html', 684),\n",
       " ('478.html', 764),\n",
       " ('449.html', 792),\n",
       " ('302.html', 835),\n",
       " ('303.html', 835),\n",
       " ('199.html', 856),\n",
       " ('539.html', 862),\n",
       " ('342.html', 961),\n",
       " ('206.html', 1006),\n",
       " ('625.html', 1113),\n",
       " ('568.html', 1248),\n",
       " ('37.html', 1280),\n",
       " ('126.html', 1304),\n",
       " ('300.html', 1319),\n",
       " ('348.html', 1371),\n",
       " ('55.html', 1692),\n",
       " ('75.html', 2038),\n",
       " ('531.html', 2453),\n",
       " ('358.html', 2541),\n",
       " ('547.html', 2666),\n",
       " ('399.html', 3088),\n",
       " ('67.html', 5142),\n",
       " ('120.html', 5743),\n",
       " ('543.html', 6880),\n",
       " ('31.html', 12228),\n",
       " ('425.html', 16028),\n",
       " ('545.html', 51309)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = sorted(lens.items(), key=lambda item: item[1])\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "885de7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "oecd = pd.read_csv(\"./oecd.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afaa8bc",
   "metadata": {},
   "source": [
    "## Porównanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "55b22810",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "545.html\n",
      "word diff 51309\n",
      "newspaper3k: 17528, dragnet 68837\n"
     ]
    }
   ],
   "source": [
    "filename = s[-1][0]\n",
    "print(filename)\n",
    "print(\"word diff %s\" % lens[filename])\n",
    "print(\"newspaper3k: %s, dragnet %s\" %(texts[filename][\"newspaper3k\"][\"len\"], texts[filename][\"dragnet\"][\"len\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9fdff5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fairness is a highly prized human value. Societies in which individuals can flourish need to be held together by practices and institutions that are regarded as fair. What it means to be fair has been much debated throughout history, rarely more so than in recent months. Issues such as the global Black Lives Matter movement, the “levelling up” of regional inequalities within the UK, and the many complex questions of fairness raised by the COVID-19 pandemic have kept fairness and equality at the centre of public debate.\n",
      "Inequality and unfairness have complex causes, but bias in the decisions that organisations make about individuals is often a key aspect. The impact of efforts to address unfair bias in decision-making have often either gone unmeasured or have been painfully slow to take effect. However, decision-making is currently going through a period of change. Use of data and automation has existed in some sectors for many years, but it is currently expanding rapidly due to an explosion in the volumes of available data, and the increasing sophistication and accessibility of machine learning algorithms. Data gives us a powerful weapon to see where bias is occurring and measure whether our efforts to combat it are effective; if an organisation has hard data about differences in how it treats people, it can build insight into what is driving those differences, and seek to address them.\n",
      "However, data can also make things worse. New forms of decision-making have surfaced numerous examples where algorithms have entrenched or amplified historic biases; or even created new forms of bias or unfairness. Active steps to anticipate risks and measure outcomes are required to avoid this.\n",
      "Concern about algorithmic bias was the starting point for this policy review. When we began the work this was an issue of concern to a growing, but relatively small, number of people. As we publish this report, the issue has exploded into mainstream attention in the context of exam results, with a strong narrative that algorithms are inherently problematic. This highlights the urgent need for the world to do better in using algorithms in the right way: to promote fairness, not undermine it. Algorithms, like all technology, should work for people, and not against them.\n",
      "This is true in all sectors, but especially key in the public sector. When the state is making life-affecting decisions about individuals, that individual often can’t go elsewhere. Society may reasonably conclude that justice requires decision-making processes to be designed so that human judgement can intervene where needed to achieve fair and reasonable outcomes for each person, informed by individual evidence.\n",
      "As our work has progressed it has become clear that we cannot separate the question of algorithmic bias from the question of biased decision-making more broadly. The approach we take to tackling biased algorithms in recruitment, for example, must form part of, and be consistent with, the way we understand and tackle discrimination in recruitment more generally.\n",
      "A core theme of this report is that we now have the opportunity to adopt a more rigorous and proactive approach to identifying and mitigating bias in key areas of life , such as policing, social services, finance and recruitment. Good use of data can enable organisations to shine a light on existing practices and identify what is driving bias. There is an ethical obligation to act wherever there is a risk that bias is causing harm and instead make fairer, better choices.\n",
      "The risk is growing as algorithms, and the datasets that feed them, become increasingly complex. Organisations often find it challenging to build the skills and capacity to understand bias, or to determine the most appropriate means of addressing it in a data-driven world. A cohort of people is needed with the skills to navigate between the analytical techniques that expose bias and the ethical and legal considerations that inform best responses. Some organisations may be able to create this internally, others will want to be able to call on external experts to advise them. Senior decision-makers in organisations need to engage with understanding the trade-offs inherent in introducing an algorithm. They should expect and demand sufficient explainability of how an algorithm works so that they can make informed decisions on how to balance risks and opportunities as they deploy it into a decision-making process.\n",
      "Regulators and industry bodies need to work together with wider society to agree best practice within their industry and establish appropriate regulatory standards. Bias and discrimination are harmful in any context. But the specific forms they take, and the precise mechanisms needed to root them out, vary greatly between contexts. We recommend that there should be clear standards for anticipating and monitoring bias, for auditing algorithms and for addressing problems. There are some overarching principles, but the details of these standards need to be determined within each sector and use case. We hope that CDEI can play a key role in supporting organisations, regulators and government in getting this right.\n",
      "Lastly, society as a whole will need to be engaged in this process. In the world before AI there were many different concepts of fairness. Once we introduce complex algorithms to decision-making systems, that range of definitions multiplies rapidly. These definitions are often contradictory with no formula for deciding which is correct. Technical expertise is needed to navigate these choices, but the fundamental decisions about what is fair cannot be left to data scientists alone. They are decisions that can only be truly legitimate if society agrees and accepts them. Our report sets out how organisations might tackle this challenge.\n",
      "Transparency is key to helping organisations build and maintain public trust. There is a clear, and understandable, nervousness about the use and consequences of algorithms, exacerbated by the events of this summer. Being open about how and why algorithms are being used, and the checks and balances in place, is the best way to deal with this. Organisational leaders need to be clear that they retain accountability for decisions made by their organisations, regardless of whether an algorithm or a team of humans is making those decisions on a day-to-day basis.\n",
      "In this report we set out some key next steps for the government and regulators to support organisations to get their use of algorithms right, whilst ensuring that the UK ecosystem is set up to support good ethical innovation. Our recommendations are designed to produce a step change in the behaviour of all organisations making life changing decisions on the basis of data, however limited, and regardless of whether they used complex algorithms or more traditional methods.\n",
      "Enabling data to be used to drive better, fairer, more trusted decision-making is a challenge that countries face around the world. By taking a lead in this area, the UK, with its strong legal traditions and its centres of expertise in AI, can help to address bias and inequalities not only within our own borders but also across the globe.\n",
      "The Board of the Centre for Data Ethics and Innovation\n",
      "Executive summary\n",
      "Unfair biases, whether conscious or unconscious, can be a problem in many decision-making processes. This review considers the impact that an increasing use of algorithmic tools is having on bias in decision-making, the steps that are required to manage risks, and the opportunities that better use of data offers to enhance fairness. We have focused on the use of algorithms in significant decisions about individuals, looking across four sectors (recruitment, financial services, policing and local government), and making cross-cutting recommendations that aim to help build the right systems so that algorithms improve, rather than worsen, decision-making.\n",
      "It is well established that there is a risk that algorithmic systems can lead to biased decisions , with perhaps the largest underlying cause being the encoding of existing human biases into algorithmic systems. But the evidence is far less clear on whether algorithmic decision-making tools carry more or less risk of bias than previous human decision-making processes. Indeed, there are reasons to think that better use of data can have a role in making decisions fairer, if done with appropriate care.\n",
      "When changing processes that make life-affecting decisions about individuals we should always proceed with caution. It is important to recognise that algorithms cannot do everything. There are some aspects of decision-making where human judgement, including the ability to be sensitive and flexible to the unique circumstances of an individual, will remain crucial.\n",
      "Using data and algorithms in innovative ways can enable organisations to understand inequalities and to reduce bias in some aspects of decision-making. But there are also circumstances where using algorithms to make life-affecting decisions can be seen as unfair by failing to consider an individual’s circumstances, or depriving them of personal agency. We do not directly focus on this kind of unfairness in this report, but note that this argument can also apply to human decision-making, if the individual who is subject to the decision does not have a role in contributing to the decision.\n",
      "History to date in the design and deployment of algorithmic tools has not been good enough. There are numerous examples worldwide of the introduction of algorithms persisting or amplifying historical biases, or introducing new ones. We must and can do better. Making fair and unbiased decisions is not only good for the individuals involved, but it is good for business and society. Successful and sustainable innovation is dependent on building and maintaining public trust. Polling undertaken for this review suggested that, prior to August’s controversy over exam results, 57% of people were aware of algorithmic systems being used to support decisions about them, with only 19% of those disagreeing in principle with the suggestion of a “fair and accurate” algorithm helping to make decisions about them. By October, we found that awareness had risen slightly (to 62%), as had disagreement in principle (to 23%). This doesn’t suggest a step change in public attitudes, but there is clearly still a long way to go to build trust in algorithmic systems. The obvious starting point for this is to ensure that algorithms are trustworthy.\n",
      "The use of algorithms in decision-making is a complex area, with widely varying approaches and levels of maturity across different organisations and sectors. Ultimately, many of the steps needed to challenge bias will be context specific. But from our work, we have identified a number of concrete steps for industry, regulators and government to take that can support ethical innovation across a wide range of use cases. This report is not a guidance manual, but considers what guidance, support, regulation and incentives are needed to create the right conditions for fair innovation to flourish.\n",
      "It is crucial to take a broad view of the whole decision-making process when considering the different ways bias can enter a system and how this might impact on fairness. The issue is not simply whether an algorithm is biased, but whether the overall decision-making processes are biased. Looking at algorithms in isolation cannot fully address this.\n",
      "It is important to consider bias in algorithmic decision-making in the context of all decision-making systems. Even in human decision-making, there are differing views about what is and isn’t fair. But society has developed a range of standards and common practices for how to manage these issues, and legal frameworks to support this. Organisations have a level of understanding on what constitutes an appropriate level of due care for fairness. The challenge is to make sure that we can translate this understanding across to the algorithmic world, and apply a consistent bar of fairness whether decisions are made by humans, algorithms or a combination of the two. We must ensure decisions can be scrutinised, explained and challenged so that our current laws and frameworks do not lose effectiveness, and indeed can be made more effective over time.\n",
      "Significant growth is happening both in data availability and use of algorithmic decision-making across many sectors; we have a window of opportunity to get this right and ensure that these changes serve to promote equality, not to entrench existing biases.\n",
      "Sector reviews\n",
      "The four sectors studied in Part II of this report are at different maturity levels in their use of algorithmic decision-making. Some of the issues they face are sector-specific, but we found common challenges that span these sectors and beyond.\n",
      "In recruitment we saw a sector that is experiencing rapid growth in the use of algorithmic tools at all stages of the recruitment process, but also one that is relatively mature in collecting data to monitor outcomes. Human bias in traditional recruitment is well evidenced and therefore there is potential for data-driven tools to improve matters by standardising processes and using data to inform areas of discretion where human biases can creep in.\n",
      "However, we also found that a clear and consistent understanding of how to do this well is lacking, leading to a risk that algorithmic technologies will entrench inequalities. More guidance is needed on how to ensure that these tools do not unintentionally discriminate against groups of people, particularly when trained on historic or current employment data. Organisations must be particularly mindful to ensure they are meeting the appropriate legislative responsibilities around automated decision-making and reasonable adjustments for candidates with disabilities.\n",
      "The innovation in this space has real potential for making recruitment fairer. However, given the potential risks, further scrutiny of how these tools work, how they are used and the impact they have on different groups, is required, along with higher and clearer standards of good governance to ensure that ethical and legal risks are anticipated and managed.\n",
      "In financial services, we saw a much more mature sector that has long used data to support decision-making. Finance relies on making accurate predictions about peoples’ behaviours, for example how likely they are to repay debts. However, specific groups are historically underrepresented in the financial system, and there is a risk that these historic biases could be entrenched further through algorithmic systems. We found financial service organisations ranged from being highly innovative to more risk averse in their use of new algorithmic approaches. They are keen to test their systems for bias, but there are mixed views and approaches regarding how this should be done. This was particularly evident around the collection and use of protected characteristic data, and therefore organisations’ ability to monitor outcomes.\n",
      "Our main focus within financial services was on credit scoring decisions made about individuals by traditional banks. Our work found the key obstacles to further innovation in the sector included data availability, quality and how to source data ethically, available techniques with sufficient explainability, risk averse culture, in some parts, given the impacts of the financial crisis and difficulty in gauging consumer and wider public acceptance. The regulatory picture is clearer in financial services than in the other sectors we have looked at. The Financial Conduct Authority (FCA) is the main regulator and is showing leadership in prioritising work to understand the impact and opportunities of innovative uses of data and AI in the sector.\n",
      "The use of data from non-traditional sources could enable population groups who have historically found it difficult to access credit, due to lower availability of data about them from traditional sources, to gain better access in future. At the same time, more data and more complex algorithms could increase the potential for the introduction of indirect bias via proxy as well as the ability to detect and mitigate it.\n",
      "Adoption of algorithmic decision-making in the public sector is generally at an early stage. In policing, we found very few tools currently in operation in the UK, with a varied picture across different police forces, both on usage and approaches to managing ethical risks.\n",
      "There have been notable government reviews into the issue of bias in policing, which is important context when considering the risks and opportunities around the use of technology in this sector. Again, we found potential for algorithms to support decision-making, but this introduces new issues around the balance between security, privacy and fairness, and there is a clear requirement for strong democratic oversight.\n",
      "Police forces have access to more digital material than ever before, and are expected to use this data to identify connections and manage future risks. The £63.7 million funding for police technology programmes announced in January 2020 demonstrates the government’s drive for innovation. But clearer national leadership is needed. Though there is strong momentum in data ethics in policing at a national level, the picture is fragmented with multiple governance and regulatory actors, and no single body fully empowered or resourced to take ownership. The use of data analytics tools in policing carries significant risk. Without sufficient care, processes can lead to outcomes that are biased against particular groups, or systematically unfair. In many scenarios where these tools are helpful, there is still an important balance to be struck between automated decision-making and the application of professional judgement and discretion. Given the sensitivities in this area it is not sufficient for care to be taken internally to consider these issues; it is also critical that police forces are transparent in how such tools are being used to maintain public trust.\n",
      "In local government, we found an increased use of data to inform decision-making across a wide range of services. Whilst most tools are still in the early phase of deployment, there is an increasing demand for sophisticated predictive technologies to support more efficient and targeted services.\n",
      "By bringing together multiple data sources, or representing existing data in new forms, data-driven technologies can guide decision-makers by providing a more contextualised picture of an individual’s needs. Beyond decisions about individuals, these tools can help predict and map future service demands to ensure there is sufficient and sustainable resourcing for delivering important services.\n",
      "However, these technologies also come with significant risks. Evidence has shown that certain people are more likely to be overrepresented in data held by local authorities and this can then lead to biases in predictions and interventions. A related problem occurs when the number of people within a subgroup is small. Data used to make generalisations can result in disproportionately high error rates amongst minority groups.\n",
      "Data-driven tools present genuine opportunities for local government. However, tools should not be considered a silver bullet for funding challenges and in some cases additional investment will be required to realise their potential. Moreover, we found that data infrastructure and data quality were significant barriers to developing and deploying data-driven tools effectively and responsibly. Investment in this area is needed before developing more advanced systems.\n",
      "Sector-specific recommendations to regulators and government\n",
      "Most of the recommendations in this report are cross-cutting, but we identified the following recommendations specific to individual sectors. More details are given in sector chapters below.\n",
      "Recommendation 1: The Equality and Human Rights Commission should update its guidance on the application of the Equality Act 2010 to recruitment, to reflect issues associated with the use of algorithms, in collaboration with consumer and industry bodies.\n",
      "Recommendation 2: The Information Commissioner’s Office should work with industry to understand why current guidance is not being consistently applied, and consider updates to guidance (e.g. in the Employment Practices Code), greater promotion of existing guidance, or other action as appropriate.\n",
      "Recommendation 3: The Home Office should define clear roles and responsibilities for national policing bodies with regards to data analytics and ensure they have access to appropriate expertise and are empowered to set guidance and standards. As a first step, the Home Office should ensure that work underway by the National Police Chiefs’ Council and other policing stakeholders to develop guidance and ensure ethical oversight of data analytics tools is appropriately supported.\n",
      "Local government\n",
      "Recommendation 4: Government should develop national guidance to support local authorities to legally and ethically procure or develop algorithmic decision-making tools in areas where significant decisions are made about individuals, and consider how compliance with this guidance should be monitored.\n",
      "Addressing the challenges\n",
      "We found underlying challenges across the four sectors, and indeed other sectors where algorithmic decision-making is happening. In Part III of this report, we focus on understanding these challenges, where the ecosystem has got to on addressing them, and the key next steps for organisations, regulators and government. The main areas considered are:\n",
      "The enablers needed by organisations building and deploying algorithmic decision-making tools to help them do this in a fair way, see Chapter 7.\n",
      "The regulatory levers , both formal and informal, needed to incentivise organisations to do this, and create a level playing field for ethical innovation see Chapter 8.\n",
      "How the public sector , as a major developer and user of data-driven technology, can show leadership in this area through transparency see Chapter 9.\n",
      "There are inherent links between these areas. Creating the right incentives can only succeed if the right enablers are in place to help organisations act fairly, but conversely, there is little incentive for organisations to invest in tools and approaches for fair decision-making if there is insufficient clarity on expected norms. We want a system that is fair and accountable; one that preserves, protects or improves fairness in decisions being made with the use of algorithms. We want to address the obstacles that organisations may face to innovate ethically, to ensure the same or increased levels of accountability for these decisions and how society can identify and respond to bias in algorithmic decision-making processes. We have considered the existing landscape of standards and laws in this area, and whether they are sufficient for our increasingly data-driven society.\n",
      "To realise this vision we need clear mechanisms for safe access to data to test for bias; organisations that are able to make judgements based on data about bias; a skilled industry of third parties who can provide support and assurance, and regulators equipped to oversee and support their sectors and remits through this change.\n",
      "Enabling fair innovation\n",
      "We found that many organisations are aware of the risks of algorithmic bias, but are unsure how to address bias in practice.\n",
      "There is no universal formulation or rule that can tell you an algorithm is fair. Organisations need to identify what fairness objectives they want to achieve and how they plan to do this. Sector bodies, regulators, standards bodies and the government have a key role in setting out clear guidelines on what is appropriate in different contexts; getting this right is essential not only for avoiding bad practice, but for giving the clarity that enables good innovation. However, all organisations need to be clear about their own accountability for getting it right. Whether an algorithm or a structured human process is being used to make a decision doesn’t change an organisation’s accountability.\n",
      "Improving diversity across a range of roles involved in the development and deployment of algorithmic decision-making tools is an important part of protecting against bias. Government and industry efforts to improve this must continue, and need to show results.\n",
      "Data is needed to monitor outcomes and identify bias, but data on protected characteristics is not available often enough. One reason for this is an incorrect belief that data protection law prevents collection or usage of this data. Indeed, there are a number of lawful bases in data protection legislation for using protected or special characteristic data when monitoring or addressing discrimination. But there are some other genuine challenges in collecting this data, and more innovative thinking is needed in this area; for example around the potential for trusted third party intermediaries.\n",
      "The machine learning community has developed multiple techniques to measure and mitigate algorithmic bias. Organisations should be encouraged to deploy methods that address bias and discrimination. However, there is little guidance on how to choose the right methods, or how to embed them into development and operational processes. Bias mitigation cannot be treated as a purely technical issue; it requires careful consideration of the wider policy, operational and legal contexts. There is insufficient legal clarity concerning novel techniques in this area. Many can be used legitimately, but care is needed to ensure that the application of some techniques does not cross into unlawful positive discrimination.\n",
      "Recommendations to government\n",
      "Recommendation 5: Government should continue to support and invest in programmes that facilitate greater diversity within the technology sector, building on its current programmes and developing new initiatives where there are gaps.\n",
      "Recommendation 6: Government should work with relevant regulators to provide clear guidance on the collection and use of protected characteristic data in outcome monitoring and decision-making processes. They should then encourage the use of that guidance and data to address current and historic bias in key sectors.\n",
      "Recommendation 7: Government and the Office for National Statistics (ONS) should open the Secure Research Service more broadly, to a wider variety of organisations, for use in evaluation of bias and inequality across a greater range of activities.\n",
      "Recommendation 8: Government should support the creation and development of data-focused public and private partnerships, especially those focused on the identification and reduction of biases and issues specific to under-represented groups. The Office for National Statistics (ONS) and Government Statistical Service should work with these partnerships and regulators to promote harmonised principles of data collection and use into the private sector, via shared data and standards development.\n",
      "Recommendations to regulators\n",
      "Recommendation 9: Sector regulators and industry bodies should help create oversight and technical guidance for responsible bias detection and mitigation in their individual secin individual sectors, adding context-specific detail to the existing cross-cutting guidance on data protection, and any new cross-cutting guidance on the Equality Act.\n",
      "Good, anticipatory governance is crucial here. Many of the high profile cases of algorithmic bias could have been anticipated with careful evaluation and mitigation of the potential risks. Organisations need to make sure that the right capabilities and structures are in place to ensure that this happens both before algorithms are introduced into decision-making processes, and through their life. Doing this well requires understanding of, and empathy for, the expectations of those who are affected by decisions, which can often only be achieved through the right engagement with groups. Given the complexity of this area, we expect to see a growing role for expert professional services supporting organisations. Although the ecosystem needs to develop further, there is already plenty that organisations can and should be doing to get this right. Data Protection Impact Assessments and Equality Impact Assessments can help with structuring thinking and documenting the steps taken.\n",
      "Guidance to organisation leaders and boards\n",
      "Those responsible for governance of organisations deploying or using algorithmic decision-making tools to support significant decisions about individuals should ensure that leaders are in place with accountability for: Understanding the capabilities and limits of those tools Considering carefully whether individuals will be fairly treated by the decision-making process that the tool forms part of Making a conscious decision on appropriate levels of human involvement in the decision-making process Putting structures in place to gather data and monitor outcomes for fairness Understanding their legal obligations and having carried out appropriate impact assessments\n",
      "This especially applies in the public sector when citizens often do not have a choice about whether to use a service, and decisions made about individuals can often be life-affecting.\n",
      "The regulatory environment\n",
      "Clear industry norms, and good, proportionate regulation, are key both for addressing risks of algorithmic bias, and for promoting a level playing field for ethical innovation to thrive.\n",
      "The increased use of algorithmic decision-making presents genuinely new challenges for regulation , and brings into question whether existing legislation and regulatory approaches can address these challenges sufficiently well. There is currently limited case law or statutory guidance directly addressing discrimination in algorithmic decision-making, and the ecosystems of guidance and support are at different maturity levels in different sectors.\n",
      "Though there is only a limited amount of case law, the recent judgement of the Court of Appeal in relation to the usage of live facial recognition technology by South Wales Police seems likely to be significant. One of the grounds for successful appeal was that South Wales Police failed to adequately consider whether their trial could have a discriminatory impact, and specifically that they did not take reasonable steps to establish whether their facial recognition software contained biases related to race or sex. In doing so, the court found that they did not meet their obligations under the Public Sector Equality Duty, even though there was no evidence that this specific algorithm was biased. This suggests a general duty for public sector organisations to take reasonable steps to consider any potential impact on equality upfront and to detect algorithmic bias on an ongoing basis. The current regulatory landscape for algorithmic decision-making consists of the Equality and Human Rights Commission (EHRC), the Information Commissioner’s Office (ICO) and sector regulators. At this stage, we do not believe that there is a need for a new specialised regulator or primary legislation to address algorithmic bias.\n",
      "However, algorithmic bias means the overlap between discrimination law, data protection law and sector regulations is becoming increasingly important. We see this overlap playing out in a number of contexts, including discussions around the use of protected characteristics data to measure and mitigate algorithmic bias, the lawful use of bias mitigation techniques, identifying new forms of bias beyond existing protected characteristics. The first step in resolving these challenges should be to clarify the interpretation of the law as it stands , particularly the Equality Act 2010, both to give certainty to organisations deploying algorithms and to ensure that existing individual rights are not eroded, and wider equality duties are met. However, as use of algorithmic decision-making grows further, we do foresee a future need to look again at the legislation itself , which should be kept under consideration as guidance is developed and case law evolves.\n",
      "Existing regulators need to adapt their enforcement to algorithmic decision-making , and provide guidance on how regulated bodies can maintain and demonstrate compliance in an algorithmic age. Some regulators require new capabilities to enable them to respond effectively to the challenges of algorithmic decision-making. While larger regulators with a greater digital remit may be able to grow these capabilities in-house, others will need external support. Many regulators are working hard to do this, and the ICO has shown leadership in this area both by starting to build a skills base to address these new challenges, and in convening other regulators to consider issues arising from AI. Deeper collaboration across the regulatory ecosystem is likely to be needed in future.\n",
      "Outside of the formal regulatory environment, there is increasing awareness within the private sector of the demand for a broader ecosystem of industry standards and professional services to help organisations address algorithmic bias. There are a number of reasons for this: it is a highly specialised skill that not all organisations will be able to support, it will be important to have consistency in how the problem is addressed, and because regulatory standards in some sectors may require independent audit of systems. Elements of such an ecosystem might be licenced auditors or qualification standards for individuals with the necessary skills. Audit of bias is likely to form part of a broader approach to audit that might also cover issues such as robustness and explainability. Government, regulators, industry bodies and private industry will all play important roles in growing this ecosystem so that organisations are better equipped to make fair decisions.\n",
      "Recommendations to government\n",
      "Recommendation 10: Government should issue guidance that clarifies the Equality Act responsibilities of organisations using algorithmic decision-making. This should include guidance on the collection of protected characteristics data to measure bias and the lawfulness of bias mitigation techniques.\n",
      "Recommendation 11: Though the development of this guidance and its implementation, government should assess whether it provides both sufficient clarity for organisations on meeting their obligations, and leaves sufficient scope for organisations to take actions to mitigate algorithmic bias. If not, government should consider new regulations or amendments to the Equality Act to address this.\n",
      "Recommendations to regulators\n",
      "Recommendation 12: The EHRC should ensure that it has the capacity and capability to investigate algorithmic discrimination. This may include EHRC reprioritising resources to this area, EHRC supporting other regulators to address algorithmic discrimination in their sector, and additional technical support to the EHRC.\n",
      "Recommendation 13: Regulators should consider algorithmic discrimination in their supervision and enforcement activities, as part of their responsibilities under the Public Sector Equality Duty.\n",
      "Recommendation 14: Regulators should develop compliance and enforcement tools to address algorithmic bias, such as impact assessments, audit standards, certification and/or regulatory sandboxes.\n",
      "Recommendation 15: Regulators should coordinate their compliance and enforcement efforts to address algorithmic bias, aligning standards and tools where possible. This could include jointly issued guidance, collaboration in regulatory sandboxes, and joint investigations.\n",
      "Public sector transparency\n",
      "Making decisions about individuals is a core responsibility of many parts of the public sector, and there is increasing recognition of the opportunities offered through the use of data and algorithms in decision-making. The use of technology should never reduce real or perceived accountability of public institutions to citizens. In fact, it offers opportunities to improve accountability and transparency, especially where algorithms have significant effects on significant decisions about individuals.\n",
      "A range of transparency measures already exist around current public sector decision-making processes; both proactive sharing of information about how decisions are made, and reactive rights for citizens to request information on how decisions were made about them. The UK government has shown leadership in setting out guidance on AI usage in the public sector, including a focus on techniques for explainability and transparency. However, more is needed to make transparency about public sector use of algorithmic decision-making the norm. There is a window of opportunity to ensure that we get this right as adoption starts to increase, but it is sometimes hard for individual government departments or other public sector organisations to be first in being transparent; a strong central drive for this is needed.\n",
      "The development and delivery of an algorithmic decision-making tool will often include one or more suppliers, whether acting as technology suppliers or business process outsourcing providers. While the ultimate accountability for fair decision-making always sits with the public body, there is limited maturity or consistency in contractual mechanisms to place responsibilities in the right place in the supply chain. Procurement processes should be updated in line with wider transparency commitments to ensure standards are not lost along the supply chain.\n",
      "Recommendations to government\n",
      "Recommendation 16: Government should place a mandatory transparency obligation on all public sector organisations using algorithms that have a significant influence on significant decisions affecting individuals. Government should conduct a project to scope this obligation more precisely, and to pilot an approach to implement it, but it should require the proactive publication of information on how the decision to use an algorithm was made, the type of algorithm, how it is used in the overall decision-making process, and steps taken to ensure fair treatment of individuals.\n",
      "Recommendation 17: Cabinet Office and the Crown Commercial Service should update model contracts and framework agreements for public sector procurement to incorporate a set of minimum standards around ethical use of AI, with particular focus on expected levels of transparency and explainability, and ongoing testing for fairness.\n",
      "Next steps and future challenges\n",
      "This review has considered a complex and rapidly evolving field. There is plenty to do across industry, regulators and government to manage the risks and maximise the benefits of algorithmic decision-making. Some of the next steps fall within CDEI’s remit, and we are happy to support industry, regulators and government in taking forward the practical delivery work to address the issues we have identified and future challenges which may arise. Outside of specific activities, and noting the complexity and range of the work needed across multiple sectors, we see a key need for national leadership and coordination to ensure continued focus and pace in addressing these challenges across sectors. This is a rapidly moving area. A level of coordination and monitoring will be needed to assess how organisations building and using algorithmic decision-making tools are responding to the challenges highlighted in this report, and to the proposed new guidance from regulators and government. Government should be clear on where it wants this coordination to sit; for example in central government directly, in a specific regulator or in CDEI.\n",
      "In this review we have concluded that there is significant scope to address the risks posed by bias in algorithmic decision-making within the law as it stands, but if this does not succeed then there is a clear possibility that future legislation may be required. We encourage organisations to respond to this challenge; to innovate responsibly and think through the implications for individuals and society at large as they do so.\n",
      "Part I: Introduction\n",
      "1. Background and scope\n",
      "1.1 About CDEI\n",
      "The adoption of data-driven technology affects every aspect of our society and its use is creating opportunities as well as new ethical challenges.\n",
      "The Centre for Data Ethics and Innovation (CDEI) is an independent expert committee, led by a board of specialists, set up and tasked by the UK government to investigate and advise on how we maximise the benefits of these technologies.\n",
      "Our goal is to create the conditions in which ethical innovation can thrive: an environment in which the public are confident their values are reflected in the way data-driven technology is developed and deployed; where we can trust that decisions informed by algorithms are fair; and where risks posed by innovation are identified and addressed.\n",
      "More information about CDEI can be found at www.gov.uk/cdei .\n",
      "1.2 About this review\n",
      "In the October 2018 Budget, the Chancellor announced that we would investigate the potential bias in decisions made by algorithms. This review formed a key part of our 2019/2020 work programme, though completion was delayed by the onset of COVID-19. This is the final report of CDEI’s review and includes a set of formal recommendations to the government.\n",
      "Government tasked us to draw on expertise and perspectives from stakeholders across society to provide recommendations on how they should address this issue. We also provide advice for regulators and industry, aiming to support responsible innovation and help build a strong, trustworthy system of governance. The government has committed to consider and respond publicly to our recommendations.\n",
      "1.3 Our focus\n",
      "The use of algorithms in decision-making is increasing across multiple sectors of our society. Bias in algorithmic decision-making is a broad topic, so in this review, we have prioritised the types of decisions where potential bias seems to represent a significant and imminent ethical risk.\n",
      "This has led us to focus on: Areas where algorithms have the potential to make or inform a decision that directly affects an individual human being (as opposed to other entities, such as companies). The significance of decisions of course varies, and we have typically focused on areas where individual decisions could have a considerable impact on a person’s life, i.e. decisions that are significant in the sense of the Data Protection Act 2018. The extent to which algorithmic decision-making is being used now, or is likely to be soon, in different sectors. Decisions made or supported by algorithms, and not wider ethical issues in the use of artificial intelligence. The changes in ethical risk in an algorithmic world as compared to an analogue world. Circumstances where decisions are biased (see Chapter 2 for a discussion of what this means), rather than other forms of unfairness such as arbitrariness or unreasonableness.\n",
      "This scope is broad, but it doesn’t cover all possible areas where algorithmic bias can be an issue. For example, the CDEI Review of online targeting , published earlier this year, highlighted the risk of harm through bias in targeting within online platforms. These are decisions which are individually very small, for example on targeting an advert or recommending content to a user, but the overall impact of bias across many small decisions can still be problematic. This review did touch on these issues, but they fell outside of our core focus on significant decisions about individuals.\n",
      "It is worth highlighting that the main work of this review was carried out before a number of highly relevant events in mid 2020; the COVID-19 pandemic, Black Lives Matter, the awarding of exam results without exams, and (with less widespread attention, but very specific relevance) the judgement of the Court of Appeal in Bridges v South Wales Police. We have considered links to these issues in our review, but have not been able to treat them in full depth. [footnote 1]\n",
      "1.4 Our approach\n",
      "Sector approach\n",
      "The ethical questions in relation to bias in algorithmic decision-making vary depending on the context and sector. We chose four initial areas of focus to illustrate the range of issues. These were recruitment, financial services, policing and local government. Our rationale for choosing these sectors is set out in the introduction to Part II .\n",
      "Cross-sector themes\n",
      "From the work we carried out on the four sectors, as well as our engagement across government, civil society, academia and interested parties in other sectors, we were able to identify themes, issues and opportunities that went beyond the individual sectors.\n",
      "We set out three key cross-cutting questions in our interim report , which we have sought to address on a cross-sector basis:\n",
      "1. Data: Do organisations and regulators have access to the data they require to adequately identify and mitigate bias?\n",
      "2. Tools and techniques: What statistical and technical solutions are available now or will be required in future to identify and mitigate bias and which represent best practice?\n",
      "3. Governance: Who should be responsible for governing, auditing and assuring these algorithmic decision-making systems?\n",
      "These questions have guided the review. While we have made sector-specific recommendations where appropriate, our recommendations focus more heavily on opportunities to address these questions (and others) across multiple sectors.\n",
      "Our evidence base for this final report is informed by a variety of work including:\n",
      "A landscape summary led by Professor Michael Rovatsos of the University of Edinburgh, which assessed the current academic and policy literature.\n",
      "An open call for evidence which received responses from a wide cross section of academic institutions and individuals, civil society, industry and the public sector.\n",
      "A series of semi-structured interviews with companies in the financial services and recruitment sectors developing and using algorithmic tools.\n",
      "Work with the Behavioural Insights Team on attitudes to the use of algorithms in personal banking. [footnote 2] Commissioned research from the Royal United Services Institute (RUSI) on data analytics in policing in England and Wales. [footnote 3] Contracted work by Faculty on technical bias mitigation techniques. [footnote 4]\n",
      "Representative polling on public attitudes to a number of the issues raised in this report, conducted by Deltapoll as part of CDEI’s ongoing public engagement work.\n",
      "Meetings with a variety of stakeholders including regulators, industry groups, civil society organisations, academics and government departments, as well as desk-based research to understand the existing technical and policy landscape.\n",
      "2. The issue\n",
      "Algorithms are structured processes, which have long been used to aid human decision-making. Recent developments in machine learning techniques and exponential growth in data has allowed for more sophisticated and complex algorithmic decisions, and there has been corresponding growth in usage of algorithm supported decision-making across many areas of society.\n",
      "This growth has been accompanied by significant concerns about bias ; that the use of algorithms can cause a systematic skew in decision-making that results in unfair outcomes. There is clear evidence that algorithmic bias can occur, whether through entrenching previous human biases or introducing new ones.\n",
      "Some forms of bias constitute discrimination under the Equality Act 2010, namely when bias leads to unfair treatment based on certain protected characteristics. There are also other kinds of algorithmic bias that are non-discriminatory, but still lead to unfair outcomes.\n",
      "There are multiple concepts of fairness , some of which are incompatible and many of which are ambiguous. In human decisions we can often accept this ambiguity and allow for human judgement to consider complex reasons for a decision. In contrast, algorithms are unambiguous.\n",
      "Fairness is about much more than the absence of bias : fair decisions need to also be non-arbitrary, reasonable, consider equality implications and respect the circumstances and personal agency of the individuals concerned.\n",
      "Despite concerns about ‘black box’ algorithms, in some ways algorithms can be more transparent than human decisions; unlike a human it is possible to reliably test how an algorithm responds to changes in parts of the input. There are opportunities to deploy algorithmic decision-making transparently, and enable the identification and mitigation of systematic bias in ways that are challenging with humans. Human developers and users of algorithms must decide the concepts of fairness that apply to their context, and ensure that algorithms deliver fair outcomes.\n",
      "Fairness through unawareness is often not enough to prevent bias : ignoring protected characteristics is insufficient to prevent algorithmic bias and it can prevent organisations from identifying and addressing bias.\n",
      "The need to address algorithmic bias goes beyond regulatory requirements under equality and data protection law. It is also critical for innovation that algorithms are used in a way that is both fair, and seen by the public to be fair.\n",
      "2.1 Introduction\n",
      "Human decision-making has always been flawed, shaped by individual or societal biases that are often unconscious. Over the years, society has identified ways of improving it, often by building processes and structures that encourage us to make decisions in a fairer and more objective way, from agreed social norms to equality legislation. However, new technology is introducing new complexities. The growing use of algorithms in decision-making has raised concerns around bias and fairness.\n",
      "Even in this data-driven context, the challenges are not new. In 1988, the UK Commission for Racial Equality found a British medical school guilty of algorithmic discrimination when inviting applicants to interview. [footnote 5] The computer program they had used was determined to be biased against both women and applicants with non-European names.\n",
      "The growth in this area has been driven by the availability and volume of (often personal) data that can be used to train machine learning models, or as inputs into decisions, as well as cheaper and easier availability of computing power, and innovations in tools and techniques. As usage of algorithmic tools grows, so does their complexity. Understanding the risks is therefore crucial to ensure that these tools have a positive impact and improve decision-making.\n",
      "Algorithms have different but related vulnerabilities to human decision-making processes. They can be more able to explain themselves statistically, but less able to explain themselves in human terms. They are more consistent than humans but are less able to take nuanced contextual factors into account. They can be highly scalable and efficient, but consequently capable of consistently applying errors to very large populations. They can also act to obscure the accountabilities and liabilities that individual people or organisations have for making fair decisions.\n",
      "2.2 The use of algorithms in decision-making\n",
      "In simple terms, an algorithm is a structured process. Using structured processes to aid human decision-making is much older than computation. Over time, the tools and approaches available to deploy such decision-making have become more sophisticated. Many organisations responsible for making large numbers of structured decisions (for example, whether an individual qualifies for a welfare benefits payment, or whether a bank should offer a customer a loan), make these processes scalable and consistent by giving their staff well-structured processes and rules to follow. Initial computerisation of such decisions took a similar path, with humans designing structured processes (or algorithms) to be followed by a computer handling an application.\n",
      "However, technology has reached a point where the specifics of those decision-making processes are not always explicitly manually designed. Machine learning tools often seek to find patterns in data without requiring the developer to specify which factors to use or how exactly to link them, before formalising relationships or extracting information that could be useful to make decisions. The results of these tools can be simple and intuitive for humans to understand and interpret, but they can also be highly complex.\n",
      "Some sectors, such as credit scoring and insurance, have a long history of using statistical techniques to inform the design of automated processes based on historical data. An ecosystem has evolved that helps to manage some of the potential risks, for example credit reference agencies offer customers the ability to see their own credit history, and offer guidance on the factors that can affect credit scoring. In these cases, there are a range of UK regulations that govern the factors that can and cannot be used.\n",
      "We are now seeing the application of data-driven decision-making in a much wider range of scenarios. There are a number of drivers for this increase, including: The exponential growth in the amount of data held by organisations, which makes more decision-making processes amenable to data-driven approaches. Improvements in the availability and cost of computing power and skills. Increased focus on cost saving, driven by fiscal constraints in the public sector, and competition from disruptive new entrants in many private sector markets. Advances in machine learning techniques, especially deep neural networks, that have rapidly brought many problems previously inaccessible to computers into routine everyday use (e.g. image and speech recognition). In simple terms, an algorithm is a set of instructions designed to perform a specific task. In algorithmic decision-making, the word is applied in two different contexts: A machine learning algorithm takes data as an input to create a model. This can be a one-off process, or something that happens continually as new data is gathered. Algorithm can also be used to describe a structured process for making a decision , whether followed by a human or computer, and possibly incorporating a machine learning model.\n",
      "The usage is usually clear from context. In this review we are focused mainly on decision-making processes involving machine learning algorithms, although some of the content is also relevant to other structured decision-making processes. Note that there is no hard definition of exactly which statistical techniques and algorithms constitute novel machine learning. We have observed that many recent developments are associated with applying existing statistical techniques more widely in new sectors, not about novel techniques.\n",
      "We interpret algorithmic decision-making to include any decision-making process where an algorithm makes, or meaningfully assists, the decision. This includes what is sometimes referred to as algorithmically-assisted decision-making. In this review we are focused mainly on decisions about individual people.\n",
      "Figure 1 below shows an example of how a machine learning algorithm can be used within a decision-making process, such as a bank making a decision on whether to offer a loan to an individual.\n",
      "Machine-learning algorithms can be used within decision-making processes. First, a set of data is gathered, for example a collection of input data from historical applications for a service (e.g. a loan) along with the decisions reached, and any data on whether those outcomes were the right ones (e.g. was the loan repaid). A human decides what data to make available to the model. Second, A machine learning algorithm is chosen, and uses historical data (e.g. a set of past input data (e.g. a set of past input data, the decisions reached) to build a model, optimising against a set of criteria specified by a human. The model can take a number of different forms in different machine learning techniques, but might be a weighted average of a number of a number of input data fields, or a complex structured decision tree. Third, the resulting model is then used repeatedly as part of the decision-making process, either to make an automated decision, or to offer guidance to a human making the final decision. A human can be involved at this stage to vet the machine-learning model’s outputs and make judgements about how to incorporate this information into a final decision. Fourth, new input data and associated decisions can be fed back into the data set to enable the model to be updated (either periodically or continuously).\n",
      "Figure 1: How data and algorithms come together to support decision-making\n",
      "It is important to emphasise that algorithms often do not represent the complete decision-making process. There may be elements of human judgement, exceptions treated outside of the usual process and opportunities for appeal or reconsideration. In fact, for significant decisions, an appropriate provision for human review will usually be required to comply with data protection law. Even before an algorithm is deployed into a decision-making process, it is humans that decide on the objectives it is trying to meet, the data available to it, and how the output is used.\n",
      "It is therefore critical to consider not only the algorithmic aspect, but the whole decision-making process that sits around it. Human intervention in these processes will vary, and in some cases may be absent entirely in fully automated systems. Ultimately the aim is not just to avoid bias in algorithmic aspects of a process, but that the process as a whole achieves fair decision-making.\n",
      "2.3 Bias\n",
      "As algorithmic decision-making grows in scale, increasing concerns are being raised around the risks of bias. Bias has a precise meaning in statistics, referring to a systematic skew in results, that is an output that is not correct on average with respect to the overall population being sampled.\n",
      "However in general usage, and in this review, bias is used to refer to an output that is not only skewed, but skewed in a way that is unfair (see below for a discussion on what unfair might mean in this context).\n",
      "Bias can enter algorithmic decision-making systems in a number of ways, including:\n",
      "Historical bias: The data that the model is built, tested and operated on could introduce bias. This may be because of previously biased human decision-making or due to societal or historical inequalities. For example, if a company’s current workforce is predominantly male then the algorithm may reinforce this, whether the imbalance was originally caused by biased recruitment processes or other historical factors. If your criminal record is in part a result of how likely you are to be arrested (as compared to someone else with the same history of behaviour, but not arrests), an algorithm constructed to assess risk of reoffending is at risk of not reflecting the true likelihood of reoffending, but instead reflects the more biased likelihood of being caught reoffending.\n",
      "Data selection bias: How the data is collected and selected could mean it is not representative. For example, over or under recording of particular groups could mean the algorithm was less accurate for some people, or gave a skewed picture of particular groups. This has been the main cause of some of the widely reported problems with accuracy of some facial recognition algorithms across different ethnic groups, with attempts to address this focusing on ensuring a better balance in training data. [footnote 6]\n",
      "Algorithmic design bias: It may also be that the design of the algorithm leads to introduction of bias. For example, CDEI’s Review of online targeting noted examples of algorithms placing job advertisements online designed to optimise for engagement at a given cost, leading to such adverts being more frequently targeted at men because women are more costly to advertise to.\n",
      "Human oversight is widely considered to be a good thing when algorithms are making decisions, and mitigates the risk that purely algorithmic processes cannot apply human judgement to deal with unfamiliar situations. However, depending on how humans interpret or use the outputs of an algorithm, there is also a risk that bias re-enters the process as the human applies their own conscious or unconscious biases to the final decision.\n",
      "There is also risk that bias can be amplified over time by feedback loops, as models are incrementally re-trained on new data generated, either fully or partly, via use of earlier versions of the model in decision-making. For example, if a model predicting crime rates based on historical arrest data is used to prioritise police resources, then arrests in high risk areas could increase further, reinforcing the imbalance. CDEI’s Landscape summary discusses this issue in more detail.\n",
      "2.4 Discrimination and equality\n",
      "In this report we use the word discrimination in the sense defined in the Equality Act 2010, meaning unfavourable treatment on the basis of a protected characteristic. [footnote 7]\n",
      "The Equality Act 2010 [footnote 8] makes it unlawful to discriminate against someone on the basis of certain protected characteristics (for example age, race, sex, disability) in public functions, employment and the provision of goods and services.\n",
      "The choice of these characteristics is a recognition that they have been used to treat people unfairly in the past and that, as a society, we have deemed this unfairness unacceptable. Many, albeit not all, of the concerns about algorithmic bias relate to situations where that bias may lead to discrimination in the sense set out in the Equality Act 2010.\n",
      "The Equality Act 2010 [footnote 9] defines two main categories of discrimination: [footnote 10]\n",
      "Direct Discrimination: When a person is treated less favourably than another because of a protected characteristic.\n",
      "Indirect Discrimination: When a wider policy or practice, even if it applies to everyone, disadvantages a group of people who share a protected characteristic (and there is not a legitimate reason for doing so).\n",
      "Where this discrimination is direct, the interpretation of the law in an algorithmic decision-making process seems relatively clear. If an algorithmic model explicitly leads to someone being treated less favourably on the basis of a protected characteristic that would be unlawful. There are some very specific exceptions to this in the case of direct discrimination on the basis of age (where such discrimination could be lawful if a proportionate means to a proportionate aim, e.g. services targeted at a particular age range) or limited positive actions in favour of those with disabilities.\n",
      "However, the increased use of data-driven technology has created new possibilities for indirect discrimination. For example, a model might consider an individual’s postcode. This is not a protected characteristic, but there is some correlation between postcode and race. Such a model, used in a decision-making process (perhaps in financial services or policing) could in principle cause indirect racial discrimination. Whether that is the case or not depends on a judgement about the extent to which such selection methods are a proportionate means of achieving a legitimate aim. [footnote 11] For example, an insurer might be able to provide good reasons why postcode is a relevant risk factor in a type of insurance. The level of clarity about what is and is not acceptable practice varies by sector, reflecting in part the maturity in using data in complex ways. As algorithmic decision-making spreads into more use cases and sectors, clear context-specific norms will need to be established. Indeed as the ability of algorithms to deduce protected characteristics with certainty from proxies continues to improve, it could even be argued that some examples could potentially cross into direct discrimination.\n",
      "Unfair bias beyond discrimination\n",
      "Discrimination is a narrower concept than bias. Protected characteristics have been included in law due to historical evidence of systematic unfair treatment, but individuals can also experience unfair treatment on the basis of other characteristics that are not protected.\n",
      "There will always be grey areas where individuals experience systematic and unfair bias on the basis of characteristics that are not protected, for example accent, hairstyle, education or socio-economic status. [footnote 12] In some cases, these may be considered as indirect discrimination if they are connected with protected characteristics, but in other cases they may reflect unfair biases that are not protected by discrimination law.\n",
      "However the increased use of algorithms may exacerbate this difficulty. The introduction of algorithms can encode existing biases into algorithms, if they are trained from existing decisions. This can reinforce and amplify existing unfair bias, whether on the basis of protected characteristics or not.\n",
      "Algorithmic decision-making can also go beyond amplifying existing biases, to creating new biases that may be unfair, though difficult to address through discrimination law. This is because machine learning algorithms find new statistical relationships, without necessarily considering whether the basis for those relationships is fair, and then apply this systematically in large numbers of individual decisions.\n",
      "2.5 Fairness\n",
      "We defined bias as including an element of unfairness. This highlights challenges in defining what we mean by fairness, which is a complex and long debated topic. Notions of fairness are neither universal nor unambiguous, and they are often inconsistent with one another.\n",
      "In human decision-making systems, it is possible to leave a degree of ambiguity about how fairness is defined. Humans may make decisions for complex reasons, and are not always able to articulate their full reasoning for making a decision, even to themselves. There are pros and cons to this. It allows for good fair-minded decision-makers to consider the specific individual circumstances, and human understanding of the reasons for why these circumstances might not conform to typical patterns. This is especially important in some of the most critical life-affecting decisions, such as those in policing or social services, where decisions often need to be made on the basis of limited or uncertain information; or where wider circumstances, beyond the scope of the specific decision, need to be taken into account. It is hard to imagine that automated decisions could ever fully replace human judgement in such cases. But human decisions are also open to the conscious or unconscious biases of the decision-makers, as well as variations in their competence, concentration levels or mood when specific decisions are made.\n",
      "Algorithms, by contrast, are unambiguous. If we want a model to comply with a definition of fairness, we must tell it explicitly what that definition is. How significant a challenge that is depends on context. Sometimes the meaning of fairness is very clearly defined; to take an extreme example, a chess playing AI achieves fairness by following the rules of the game. Often though, existing rules or processes require a human decision-maker to exercise discretion or judgement, or to account for data that is difficult to include in a model (e.g. context around the decision that cannot be readily quantified). Existing decision-making processes must be fully understood in context in order to decide whether algorithmic decision-making is likely to be appropriate. For example, police officers are charged with enforcing the criminal law, but it is often necessary for officers to apply discretion on whether a breach of the letter of the law warrants action. This is broadly a good thing, but such discretion also allows an individual’s personal biases, whether conscious or unconscious, to affect decisions.\n",
      "Even in cases where fairness can be more precisely defined, it can still be challenging to capture all relevant aspects of fairness in a mathematical definition. In fact, the trade-offs between mathematical definitions demonstrate that a model cannot conform to all possible fairness definitions at the same time. Humans must choose which notions of fairness are appropriate for a particular algorithm, and they need to be willing to do so upfront when a model is built and a process is designed.\n",
      "The General Data Protection Regulation (GDPR) and Data Protection Act 2018 contain a requirement that organisations should use personal data in a way that is fair. The legislation does not elaborate further on the meaning of fairness, but the ICO guides organisations that “In general, fairness means that you should only handle personal data in ways that people would reasonably expect and not use it in ways that have unjustified adverse effects on them.” [footnote 13] Note that the discussion in this section is wider than the notion in GDPR, and does not attempt to define how the word fair should be interpreted in that context.\n",
      "Notions of fairness\n",
      "Notions of fair decision-making (whether human or algorithmic) are typically gathered into two broad categories:\n",
      "procedural fairness is concerned with ‘fair treatment’ of people, i.e. equal treatment within the process of how a decision is made. It might include, for example, defining an objective set of criteria for decisions, and enabling individuals to understand and challenge decisions about them.\n",
      "outcome fairness is concerned with what decisions are made i.e. measuring average outcomes of a decision-making process and assessing how they compare to an expected baseline. The concept of what a fair outcome means is of course highly subjective; there are multiple different definitions of outcome fairness.\n",
      "Some of these definitions are complementary to each other, and none alone can capture all notions of fairness. A ‘fair’ process may still produce ‘unfair’ results, and vice versa, depending on your perspective. Even within outcome fairness there are many mutually incompatible definitions for a fair outcome. Consider for example a bank making a decision on whether an applicant should be eligible for a given loan, and the role of an applicant’s sex in this decision. Two possible definitions of outcome fairness in this example are:\n",
      "A. The probability of getting a loan should be the same for men and women.\n",
      "B. The probability of getting a loan should be the same for men and women who earn the same income. Taken individually, either of these might seem like an acceptable definition of fair. But they are incompatible. In the real world sex and income are not independent of each other; the UK has a gender pay gap meaning that, on average, men earn more than women. [footnote 14] Given that gap, it is mathematically impossible to achieve both A and B simultaneously.\n",
      "This example is by no means exhaustive in highlighting the possible conflicting definitions that can be made, with a large collection of possible definitions identified in the machine learning literature. [footnote 15]\n",
      "In human decision-making we can often accept ambiguity around this type of issue, but when determining if an algorithmic decision-making process is fair, we have to be able to explicitly determine what notion of fairness we are trying to optimise for. It is a human judgement call whether the variable (in this case salary) acting as a proxy for a protected characteristic (in this case sex) is seen as reasonable and proportionate in the context. We investigated public reactions to a similar example to this in work with the Behavioural Insights Team (see further detail in Chapter 4.\n",
      "Addressing fairness\n",
      "Even when we can agree what constitutes fairness, it is not always clear how to respond. Conflicting views about the value of fairness definitions arise when the application of a process intended to be fair produces outcomes regarded as unfair. This can be explained in several ways, for example: Differences in outcomes are evidence that the process is not fair. If in principle, there is no good reason why there should be differences on average in the ability of men and women to do a particular job, differences in the outcomes between male and female applicants may be evidence that a process is biased and failing to accurately identify those most able. By correcting this, the process is both fairer and more efficient. Differences in outcomes are the consequence of past injustices. For example, a particular set of previous experience might be regarded as a necessary requirement for a role, but might be more common among certain socio-economic backgrounds due to past differences in access to employment and educational opportunities. Sometimes it might be appropriate for an employer to be more flexible on requirements to enable them to get the benefits of a more diverse workforce (perhaps bearing a cost of additional training); but sometimes this may not be possible for an individual employer to resolve in their recruitment, especially for highly specialist roles.\n",
      "The first argument implies greater outcome fairness is consistent with more accurate and fair decision-making. The second argues that different groups ought to be treated differently to correct for historical wrongs and is the argument associated with quota regimes. It is not possible to reach a general opinion on which argument is correct, this is highly dependent on the context (and there are also other possible explanations).\n",
      "In decision-making processes based on human judgement it is rarely possible to fully separate the causes of differences in outcomes. Human recruiters may believe they are accurately assessing capabilities, but if the outcomes seem skewed it is not always possible to determine the extent to which this in fact reflects bias in methods of assessing capabilities.\n",
      "How do we handle this in the human world? There are a variety of techniques, for example steps to ensure fairness in an interview-based recruitment process might include: Training interviewers to recognise and challenge their own individual unconscious biases. Policies on the composition of interview panels. Designing assessment processes that score candidates against objective criteria. Applying formal or informal quotas (though a quota based on protected characteristics would usually be unlawful in the UK).\n",
      "Why algorithms are different\n",
      "The increased use of more complex algorithmic approaches in decision-making introduces a number of new challenges and opportunities.\n",
      "The need for conscious decisions about fairness: In data-driven systems, organisations need to address more of these issues at the point a model is built, rather than relying on human decision-makers to interpret guidance appropriately (an algorithm can’t apply “common sense” on a case-by-case basis). Humans are able to balance things implicitly, machines will optimise without any balance if asked to do so.\n",
      "Explainability: Data-driven systems allow for a degree of explainability about the factors causing variation in the outcomes of decision-making systems between different groups and to assess whether or not this is regarded as fair. For example, it is possible to examine more directly the degree to which relevant characteristics are acting as a proxy for other characteristics, and causing differences in outcomes between different groups. If a recruitment process included requirements for length of service and qualification, it would be possible to see whether, for example, length of service was generally lower for women due to career breaks and that this was causing an imbalance.\n",
      "The extent to which this is possible depends on the complexity of the algorithm used. Dynamic algorithms drawing on large datasets may not allow for a precise attribution of the extent to which the outcome of the process for an individual woman was attributable to a particular characteristic and its association with gender. However, it is possible to assess the degree to which over a time period, different characteristics are influencing recruitment decisions and how they correlate with characteristics during that time.\n",
      "The term ‘black box’ is often used to describe situations where, for a variety of different reasons, an explanation for a decision is unobtainable. This can include commercial issues (e.g. the decision-making organisation does not understand the details of the algorithm which their supplier considers their own intellectual property) or technical reasons (e.g. machine learning techniques that are less accessible for easy human explanation of individual decisions). The Information Commissioner’s Office and the Alan Turing Institute have recently published detailed joint advice on how organisations can overcome some of these challenges and provide a level of explanation of decisions. [footnote 16]\n",
      "Scale of impact: The potential breadth of impact of an algorithm links to the market dynamics. Many algorithmic software tools are developed as platforms and sold across many companies. It is therefore possible, for example, that individuals applying to multiple jobs could be rejected at sift by the same algorithm (perhaps sold to a large number of companies recruiting for the same skill sets in the same industry). If the algorithm does this for reasons irrelevant to their actual performance, but on the basis of a set of characteristics that are not protected, then this feels very much like systematic discrimination against a group of individuals, but the Equality Act provides no obvious protection against this.\n",
      "Algorithmic decision-making will inevitably increase over time; the aim should be to ensure that this happens in a way that acts to challenge bias, increase fairness and promote equality, rather than entrenching existing problems. The recommendations of this review are targeted at making this happen.\n",
      "Case study: Exam results in August 2020\n",
      "Due to COVID-19, governments across the UK decided to cancel school examinations in summer 2020, and find an alternative approach to awarding grades. All four nations of the UK attempted to implement similar processes to deliver this; combining teacher assessments with a statistical moderation process that attempted to achieve a similar distribution of grades to previous years. The approaches were changed in response to public concerns, and significant criticism about both individual fairness and concerns that grades were biased.\n",
      "How should fairness have been interpreted in this case? There were a number of different notions of fairness to consider, including: [footnote 17]\n",
      "Fairness between year groups: Achieve a similar distribution of grades to previous and future year groups.\n",
      "Group fairness between different schools: Attempt to standardise teacher assessed grades, given the different levels of strictness/optimism in grading between different schools to be fair to individual students from different schools.\n",
      "Group fairness and discrimination: Avoid exacerbating differences in outcomes correlated with protected characteristics; particularly sex and race. This did not include addressing any systematic bias in results based on inequality of opportunity; this was seen as outside the mandate of an exam body.\n",
      "Avoid any bias based on socio-economic status.\n",
      "A fair process for allocating grades to individual students, i.e. allocating them a grade that was seen to be a fair representation of their own individual capabilities and efforts.\n",
      "The main work of this review was complete prior to the release of summer 2020 exam results, but there are some clear links between the issues raised and the contents of this review, including issues of public trust, transparency and governance.\n",
      "For a detailed example, Ofqual, the exam regulator in England, set out the details of their approach and also provided a statement to parliament with their reflections on the process.  ↩\n",
      "2.6 Applying ethical principles\n",
      "The way decisions are made, the potential biases which they are subject to, and the impact these decisions have on individuals, are highly context dependent. It is unlikely that all forms of bias can be entirely eliminated. This is also true in human decision-making; it is important to understand the status quo prior to the introduction of data-driven technology in any given scenario. Decisions may need to be made about what kinds and degrees of bias are tolerable in certain contexts and the ethical questions will vary depending on the sector.\n",
      "We want to help create the conditions where ethical innovation using data-driven technology can thrive. It is therefore essential to ensure our approach is grounded in robust ethical principles.\n",
      "The UK government, along with 41 other countries, has signed up to the OECD Principles on Artificial Intelligence [footnote 18] . They provide a good starting point for considering our approach to dealing with bias, as follows: [footnote 19]\n",
      "1: AI should benefit people and the planet by driving inclusive growth, sustainable development and well-being.\n",
      "There are many potential advantages of algorithmic decision-making tools when used appropriately, such as the potential efficiency and accuracy of predictions. There is also the opportunity for these tools to support good decision-making by reducing human error and combating existing bias. When designed correctly, they can offer a more objective alternative (or supplement) to human subjective interpretation. It is core to this review, and the wider purpose of CDEI, to identify how we can collectively ensure that these opportunities outweigh the risks.\n",
      "2: AI systems should be designed in a way that respects the rule of law, human rights, democratic values and diversity, and they should include appropriate safeguards – for example, enabling human intervention where necessary – to ensure a fair and just society.\n",
      "This principle sets out some core terms for what we mean by fairness in an algorithmic decision-making process. We cover a number of aspects of it throughout the review.\n",
      "Our focus in this review on significant decisions means that we have been largely considering decisions where the algorithm forms only part of an overall decision-making process, and hence there is a level of direct human oversight of individual decisions. However, consideration is always needed on whether the role of the human remains meaningful; does the human understand the algorithm (and its limitations) sufficiently well to exercise that oversight effectively? Does the organisational environment that they are working within empower them to do so? Is there a risk that human biases could be reintroduced through this oversight?\n",
      "In Chapter 8 below we consider the ability of existing UK legal and regulatory structures to ensure fairness in this area, especially data protection and equality legislation, and how they will need to evolve to adapt to an algorithmic world.\n",
      "3: There should be transparency and responsible disclosure around AI systems to ensure that people understand AI-based outcomes and can challenge them.\n",
      "Our sector-led work has identified variable levels of transparency on the usage of algorithms. A variety of other recent reviews have called for increased levels of transparency across the public sector.\n",
      "It is clear that more work is needed to achieve this level of transparency in a consistent way across the economy, and especially in the public sector where many of the highest stakes decisions are made. We discuss how this can be achieved in Chapter 9.\n",
      "4: AI systems must function in a robust, secure and safe way throughout their life cycles and potential risks should be continually assessed and managed.\n",
      "In Chapter 7 we identify approaches taken to mitigate the risk of bias through the development lifecycle of an algorithmic decision-making system, and suggest action that the government can take to support development teams in taking a fair approach.\n",
      "5: Organisations and individuals developing, deploying or operating AI systems should be held accountable for their proper functioning in line with the above principles.\n",
      "The use of algorithmic decision-making tools within decisions can have a significant impact on individuals or society, raising a requirement for clear lines of accountability in their use and impact.\n",
      "When decisions are made by humans in large organisations, we don’t generally consider it possible to get it right every time. Instead, we expect organisations to have appropriate structures, policies and procedures to anticipate and address potential bias, offer redress when it occurs, and set clear governance processes and lines of accountability for decisions.\n",
      "Organisations that are introducing algorithms into decisions that were previously purely made by humans should be looking to achieve at least equivalent standards of fairness, accountability and transparency, and in many cases should look to do better. Defining equivalence is not always easy of course, there may be occasions where these standards have to be achieved in a different way in an algorithmic world. We discuss this issue in more detail in Part III of the report.\n",
      "For all of these issues, it is important to remember that we are not just interested in the output of an algorithm, but the overall decision-making process that sits around it. Organisations have existing accountability processes and standards, and the use of algorithms in decision-making needs to sit within existing accountability processes to ensure that they are used intentionally and effectively, and therefore that the organisation is as accountable for the outcome as they are for traditional human decision-making.\n",
      "We must decide how far to mitigate bias and how we should govern our approach to doing so. These decisions require value judgements and trade-offs between competing values. Humans are often trusted to make these trade-offs without having to explicitly state how much weight they have put on different considerations. Algorithms are different. They are programmed to make trade-offs according to rules and their decisions can be interrogated and made explicit.\n",
      "2.7 The opportunity\n",
      "The OECD principles are clearly high level, and only take us so far when making difficult ethical balances for individual decision-making systems. The work in this review suggests that as algorithmic decision-making continues to grow in scale, we should be ambitious in aiming not only to avoid new bias, but to use this as an opportunity to address historical unfairness.\n",
      "Organisations responsible for using algorithms require more specific guidance on how principles apply in their circumstances. The principles are often context specific and are discussed in more detail in the sector sections below. However, we can start to outline some rules of thumb that can guide all organisations using algorithms to support significant decision-making processes: History shows that most decision-making processes are biased, often unintentionally. If you want to make fairer decisions, then using data to measure this is the best approach; certainly assuming the non-existence of bias in a process is a highly unreliable approach. If your data shows historical patterns of bias, this does not mean that algorithms should not be considered. The bias should be addressed, and the evidence from the data should help inform that approach. Algorithms designed to mitigate bias may be part of the solution. If an algorithm is introduced to replace a human decision system, the bias mitigation strategy should be designed to result in fairer outcomes and a reduction in unwarranted differences between groups. While it is important to test the outputs of algorithms and assess their fairness, the key measure of the fairness of an algorithm is the impact it has on the whole decision process. In some cases, resolving fairness issues may only be possible outside of the actual decision-making process, e.g. by addressing wider systemic issues in society. Putting a ‘human in the loop’ is a way of addressing concern about the ‘unforgiving nature’ of algorithms (as they can bring perspectives or contextual information not available to the algorithm) but can also introduce human bias into the system. Humans ‘over the loop’ monitoring the fairness of the whole decision process are also needed, with responsibility for the whole process. Humans over the loop need to understand how the machine learning model works, and the limitations and trade-offs that it is making, to a great enough extent to make informed judgements on whether it is performing effectively and fairly.\n",
      "Part II: Sector reviews\n",
      "The ethical questions in relation to bias in algorithmic decision-making vary depending on the context and sector. We therefore chose four initial areas of focus to illustrate the range of issues. These were recruitment, financial services, policing and local government.\n",
      "All of these sectors have the following in common: They involve making decisions at scale about individuals which involve significant potential impacts on those individuals’ lives. There is a growing interest in the use of algorithmic decision-making tools in these sectors, including those involving machine learning in particular. There is evidence of historic bias in decision-making within these sectors, leading to risks of this being perpetuated by the introduction of algorithms.\n",
      "There are of course other sectors that we could have considered; these were chosen as a representative sample across the public and private sector, not because we have judged that the risk of bias is most acute in these specific cases.\n",
      "In this part of the review, we focus on the sector-specific issues, and reach a number of recommendations specific to individual sectors. The sector studies then inform the cross-cutting findings and recommendations in Part III below.\n",
      "Overview of findings:\n",
      "The use of algorithms in recruitment has increased in recent years, in all stages of the recruitment process. Trends suggest these tools will become more widespread, meaning that clear guidance and a robust regulatory framework are essential.\n",
      "When developed responsibly, data-driven tools have the potential to improve recruitment by standardising processes and removing discretion where human biases can creep in, however if using historical data, these human biases are highly likely to be replicated.\n",
      "Rigorous testing of new technologies is necessary to ensure platforms do not unintentionally discriminate against groups of people, and the only way to do this is to collect demographic data on applicants and use this data to monitor how the model performs. Currently, there is little standardised guidance for how to do this testing, meaning companies are largely self-regulated.\n",
      "Algorithmic decision-making in recruitment is currently governed primarily by the Equality Act 2010 and the Data Protection Act 2018, however we found in both cases there is confusion regarding how organisations should enact their legislative responsibilities.\n",
      "Recommendations to regulators:\n",
      "Recommendation 1: The Equality and Human Rights Commission should update its guidance on the application of the Equality Act 2010 to recruitment, to reflect issues associated with the use of algorithms, in collaboration with consumer and industry bodies.\n",
      "Recommendation 2: The Information Commissioner’s Office should work with industry to understand why current guidance is not being consistently applied, and consider updates to guidance (e.g. in the Employment Practices Code), greater promotion of existing guidance, or other action as appropriate.\n",
      "Advice to industry Organisations should carry out Equality Impact Assessments to understand how their models perform for candidates with different protected characteristics, including intersectional analysis for those with multiple protected characteristics.\n",
      "Future CDEI work CDEI will consider how it can work with relevant organisations to assist with developing guidance on applying the Equality Act 2010 to algorithms in recruitment.\n",
      "3.1 Background\n",
      "Decisions about who to shortlist, interview and employ have significant effects on the lives of individuals and society. When certain groups are disadvantaged either directly or indirectly from the recruitment process, social inequalities are broadened and embedded.\n",
      "The existence of human bias in traditional recruitment is well-evidenced. [footnote 20] A famous study found that when orchestral players were kept behind a screen for their audition, there was a significant increase in the number of women who were successful. [footnote 21] Research in the UK found that candidates with ethnic minority backgrounds have to send as many of 60% more applications than white candidates to receive a positive response. [footnote 22] Even more concerning is the fact that there has been very little historical improvement in these figures over the last few decades. [footnote 23] Recruitment is also considered a barrier to employment for people with disabilities. [footnote 24] A range of factors from affinity biases, where recruiters tend to prefer people similar to them, to informal processes that recruit candidates already known to the organisation all amplify these biases, and some people believe technology could play a role in helping to standardise processes and make them fairer. [footnote 25]\n",
      "The internet has also meant that candidates are able to apply for a much larger number of jobs, thus creating a new problem for organisations needing to review hundreds, sometimes thousands, of applications. These factors have led to an increase in new data-driven tools, promising greater efficiency, standardisation and objectivity. There is a consistent upwards trend in adoption, with around 40% of HR functions in international companies now using AI. [footnote 26] It is however important to distinguish between new technologies and algorithmic decision-making. Whilst new technology is increasingly being applied across the board in recruitment, our research was focused on tools that utilise algorithmic decision-making systems, training on data to predict a candidate’s future success.\n",
      "There are concerns about the potential negative impacts of algorithmic decision-making in recruitment . There are also concerns about the effectiveness of technologies to be able to predict good job performance given the relative inflexibility of systems and the challenge of conducting a thorough assessment using automated processes at scale. For the purpose of this report, our focus is on bias rather than effectiveness.\n",
      "How we approached our work\n",
      "Our work on recruitment as a sector began with a call for evidence and the landscape summary. This evidence gathering provided a broad overview of the challenges and opportunities presented by using algorithmic tools in hiring.\n",
      "In addition to desk-based research, we conducted a series of semi-structured interviews with a broad range of software providers and recruiters. In these conversations we focused on how providers currently test and mitigate bias in their tools. We also spoke with a range of other relevant organisations and individuals including think tanks, academics, government departments, regulators and civil society groups.\n",
      "3.2 Findings\n",
      "Tools are being created and used for every stage of the recruitment process\n",
      "There are many stages in a recruitment process and algorithms are increasingly being used throughout. [footnote 27] Starting with the sourcing of applicants via targeting online advertisements [footnote 28] through to CV screening, then interview and selection phases. Data-driven tools are sold as a more efficient, accurate and objective way of assisting with recruiting decisions.\n",
      "Figure 2: Examples of algorithmic tools used through the sourcing, screening, interview and selection stages of the recruitment process\n",
      "In recruitment, AI tools are available to support all stages of the hiring process: sourcing, screening, interview and selection. At the sourcing stage, tools are available to review job descriptions, target job advertising to potential candidates, power recruiting chatbots and ‘headhunt’ for high-performing candidates. At the screening stage, AI tools can be used to screen qualifications, match CVs to specific job roles, run psychometric and game-based tests to assess cognitive skills, and rank applications. At the interview stage, voice and face recognition technology can be used in video interviewing. At the selection stage, AI software can be used to perform background checks and predict offers.\n",
      "Organisations may use different providers for the stages of the recruitment process and there are increasing options to integrate different types of tools.\n",
      "Algorithms trained on historic data carry significant risks for bias\n",
      "There are many ways bias can be introduced into the recruiting process when using data-driven technology. Decisions such as how data is collected, which variables to collect, how the variables are weighted, and the data the algorithm is trained on all have an impact and will vary depending on the context. However one theme that arises consistently is the risk of training algorithms on biased historical data.\n",
      "High profile cases of biased recruiting algorithms include those trained using historical data on current and past employees within an organisation, which is then used to try and predict the performance of future candidates. [footnote 29] Similar systems are used for video interviewing software where existing employees or prospective applicants undertake the assessment and this is assessed and correlated in line with a performance benchmark. [footnote 30] The model is then trained on this data to understand the traits of people who are considered high performers.\n",
      "Without rigorous testing, these kinds of predictive systems can pull out characteristics that have no relevance to job performance but are rather descriptive features that correlate with current employees. For example, one company developed a predictive model trained on their company data that found having the name “Jared” was a key indicator of a successful applicant. [footnote 31] This is an example where a machine learning process has picked up a very explicit bias, others are often more subtle but can be still as damaging. In the high profile case of Amazon, an application system trained on existing employees never made it past the development phase when testing showed that women’s CVs were consistently rated worse. [footnote 32] Pattern detection of this type is likely to identify various factors that correspond with protected characteristics if development goes unchecked, so it is essential that organisations interrogate their models to identify proxies or risk indirectly discriminating against protected groups.\n",
      "Another way bias can arise is through having a dataset that is limited in respect to candidates with certain characteristics. For example, if the training set was from a company that had never hired a woman, the algorithm would be far less accurate in respect to female candidates. This type of bias arises from imbalance, and can easily be replicated across other demographic groups. Industry should therefore be careful about the datasets used to develop these systems both with respect to biases arising through historical prejudice, but also from unbalanced data.\n",
      "Whilst most companies we spoke to evaluated their models to check that the patterns being detected did not correlate with protected characteristics, there is very little guidance or standards companies have to meet so it is difficult to evaluate the robustness of these processes. [footnote 33] Further detail can be found in Section 7.4 on the challenges and limitations of bias mitigation approaches.\n",
      "Recruiting tool providers are largely self-regulated but tend to follow international standards\n",
      "Currently guidance on discrimination within recruitment sits with the Equality and Human Rights Commission who oversee compliance with the Equality Act (2010) through the Employment Statutory Code of Practice setting out what fair recruitment looks like under the Equality Act. They also provide detailed guidance to employers on how to interpret and apply the Equality Act. [footnote 34] However there is not currently any guidance on how the Equality Act extends to algorithmic recruitment. This means providers of recruiting tools are largely self-regulating, and often base their systems on equality law in other jurisdictions, especially the US (where there have been some high profile legal cases in this area). [footnote 35]\n",
      "Our research found that most companies test their tools internally and only some independently validate results. This has led to researchers and civil society groups calling for greater transparency around bias testing in recruiting algorithms as a way of assuring the public that appropriate steps have been taken to minimise the risk of bias. [footnote 36] We are now seeing some companies publish information on how their tools are validated and tested for bias. [footnote 37] However, researchers and civil society groups believe this has not gone far enough, calling for recruiting algorithms to be independently audited. [footnote 38] Further discussion of the regulatory landscape and auditing can be found in Chapter 8.\n",
      "Recommendations to regulators\n",
      "Recommendation 1: The Equality and Human Rights Commission should update its guidance on the application of the Equality Act 2010 to recruitment, to reflect issues associated with the use of algorithms, in collaboration with relevant industry and consumer bodies.\n",
      "CDEI is happy to support this work if this would be helpful.\n",
      "Collecting demographic data for monitoring purposes is increasingly widespread and helps to test models for biases and proxies\n",
      "The only way to be sure a model is not directly or indirectly discriminating against a protected group is to check, and doing so requires having the necessary data. The practice of collecting data on protected characteristics is becoming increasingly common in recruitment as part of the wider drive to monitor and improve recruiting for underrepresented groups. This then allows vendors or recruiting organisations to test their models for proxies and monitor the drop-out rate of groups across the recruitment process. Compared to the other sectors we studied, recruitment is more advanced in the practice of collecting equality data for monitoring purposes. We found in our interviews that it is now standard practice to collect this data and provide applicants with disclaimers highlighting that the data will not be used as part of the process.\n",
      "One challenge that was raised in our interviews was that some applicants may not want to provide this data as part of a job application, which is within their rights to withhold. We consider this issue in detail in Section 7.3 below, and conclude that clearer national guidance is needed to support organisations in doing this. Organisations should also be encouraged to monitor the overlap for people with multiple protected characteristics, as this may not be picked up through monitoring that only reviews data through a one-dimensional lens. This form of intersectional analysis is essential for ensuring people are not missed as a result of having multiple protected characteristics. [footnote 39]\n",
      "Advice to employers and industry: Organisations should carry out equality impact assessments to understand how their models perform for candidates with different protected characteristics, including intersectional analysis for those with multiple protected characteristics.\n",
      "In the US there is specific guidance setting the minimum level of drop-off allowed for applicants from protected groups before a recruitment process could be considered discriminatory. This is known as the “four-fifths rule” and was introduced as a mechanism to adjudicate on whether a recruitment process was considered to have had a disparate impact on certain groups of people. [footnote 40] We found in our research that many third-party software providers use these standards and some tools offer this feature as part of their platforms to assess the proportion of applicants that are moving through the process. However, the four-fifths rule is not part of UK law, and not a meaningful test of whether a system might lead to discrimination under UK law. It is therefore important for the EHRC to provide guidance on how the Equality Act 2010 applies (see Chapter 7 and Chapter 8 below for further discussion in this area).\n",
      "Many tools are developed and used with fairness in mind\n",
      "Although the most frequently cited reason for adopting data-driven technologies is efficiency, we found a genuine desire to use tools to make processes fairer. Where historically decisions about who to hire were made through referrals or unconscious biases. Recruiters also often do not have the relevant demographic data on applicants to know whether they are being fair in the criteria they are applying when looking for candidates. Many companies developing these tools want to provide less biased assessments of candidates by standardising processes and using more accurate assessment for candidates’ potential to succeed in a job. For example, one provider offers machine learning software that redacts parts of CVs associated with protected characteristics so those assessing the application can make a fairer judgement. Others try to equalise the playing field by developing games that assess core skills rather than relying on CVs which place weight on socio-demographic markers like educational institutions.\n",
      "The innovation in this space has real potential for making recruitment less biased if developed and deployed responsibly. [footnote 41] However, the risks if they go wrong are significant because the tools are incorporating and replicating biases on a larger scale. Given the potential risks, there is a need for scrutiny in how these tools work, how they are used and the impact they have on different groups.\n",
      "More needs to be done to ensure that data-driven tools can support reasonable adjustments for those who need them, or that alternative routes are available\n",
      "One area where there is particular concern is how certain tools may work for those with disabilities. AI often identifies patterns related to a defined norm, however those with disabilities often require more bespoke arrangements because their requirements will likely differ from the majority, which may lead to indirect discrimination. [footnote 42] For example, someone with a speech impediment may be at a disadvantage in an AI assessed video interview, or someone with a particular cognitive disability may not perform as well in a gamified recruitment exercise. In the same way that reasonable adjustments are made for in-person interviews, companies should consider how any algorithmic recruitment process takes these factors into account, meeting their obligations under the Equality Act 2010.\n",
      "Organisations should start by building inclusive design into their processes and include explicit steps for considering how certain tools may impact those with disabilities. This may include increasing the number of people with disabilities hired in development and design teams or offering candidates with disabilities the option of a human-assessed alternative route where appropriate. It is worth noting that some reports have found that AI recruitment could improve the experience for disabled applicants by reducing biases, however this will likely vary depending on the tools and the wide spectrum of barriers to progression faced by candidates with disabilities. A one size fits all approach is unlikely to be successful.\n",
      "Automated rejections are governed by data protection legislation but compliance with guidance seems mixed\n",
      "Most algorithmic tools in recruitment are designed to assist people with decision-making, however some fully automate elements of the process. This appears particularly common around automated rejections for candidates at application stage that do not meet certain requirements. Fully automated decision-making is regulated under Article 22 of the General Data Protection Regulation (GDPR), overseen by the Information Commissioner’s Office (ICO). The ICO have set out how this requirement should be operationalised for automated decision-making, with guidance that states organisations should be: giving individuals information about the processing; introducing simple ways for them to request human intervention or challenge a decision; carrying out regular checks to make sure that your systems are working as intended [footnote 43]\n",
      "It is not clear how organisations screening many thousands of candidates should make provisions for the second of these suggestions, and indeed this is often not common practice for large scale sifts carried out by either algorithmic or non-algorithmic methods. Our research suggested the guidance was rarely applied in the way outlined above, particularly on introducing ways to request human intervention or review. We therefore think there would be value in the ICO working with employers to understand how this guidance (and the more detailed guidance set out in the Employment Practices Code ) is being interpreted and applied, and consider how to ensure greater consistency in the application of the law so individuals are sufficiently able to exercise their rights under data protection.\n",
      "Recommendations to regulators:\n",
      "Recommendation 2: The Information Commissioner’s Office should work with industry to understand why current guidance is not being consistently applied, and consider updates to guidance (e.g. in the Employment Practices Code), greater promotion of existing guidance, or other action as appropriate.\n",
      "Clearly it would be most helpful for the EHRC and ICO to coordinate their work to ensure that organisations have clarity on how data protection and equality law requirements interact; they may even wish to consider joint guidance addressing recommendations 1 and 2. Topics where there may be relevant overlaps include levels of transparency, auditing and recording recommended to improve standards of practice and ensure legal compliance. CDEI is happy to support this collaboration.\n",
      "4. Financial services\n",
      "Overview of findings:\n",
      "Financial services organisations have long used data to support their decision-making. They range from being highly innovative to more risk averse in their use of new algorithmic approaches. For example, when it comes to credit scoring decisions, most banks are using logistic regression models rather than more advanced machine learning algorithms.\n",
      "There are mixed views and approaches amongst financial organisations on the collection and use of protected characteristics data and this affects the ability of organisations to check for bias.\n",
      "Explainability of models used in financial services, in particular in customer-facing decisions, is key for organisations and regulators to identify and mitigate discriminatory outcomes and for fostering customer trust in the use of algorithms.\n",
      "The regulatory picture is clearer in financial services than in the other sectors we have looked at. The Financial Conduct Authority (FCA) is the lead regulator and is conducting work to understand the impact and opportunities of innovative uses of data and AI in the sector.\n",
      "Future CDEI work: CDEI will be an observer on the Financial Conduct Authority and Bank of England’s AI Public Private Forum which will explore means to support the safe adoption of machine learning and artificial intelligence within financial services.\n",
      "4.1 Background\n",
      "Financial services organisations make decisions which have a significant impact on our lives, such as the amount of credit we can be offered or the price our insurance premium is set at. Algorithms have long been used in this sector but more recent technological advances have seen the application of machine learning techniques to inform these decisions. [footnote 4] Given the historical use of algorithms, the financial services industry is well-placed to embrace the most advanced data-driven technology to make better decisions about which products to offer to which customers.\n",
      "However, these decisions are being made in the context of a socio-economic environment where financial resources are not spread evenly between different groups. For example, there is established evidence documenting the inequalities experienced by ethnic minorities and women in accessing credit, either as business owners or individuals, though these are generally attributed to wider societal and structural inequalities, rather than to the direct actions of lenders. [footnote 45] If financial organisations rely on making accurate predictions about peoples’ behaviours, for example how likely they are to repay debts like mortgages, and specific individuals or groups are historically underrepresented in the financial system, there is a risk that these historic biases could be entrenched further through algorithmic systems. [footnote 46]\n",
      "In theory, using more data and better algorithms could help make these predictions more accurate. For example, incorporating non-traditional data sources could enable groups who have historically found it difficult to access credit, because of a paucity of data about them from traditional sources, to gain better access in future. At the same time, more complex algorithms could increase the potential of indirect bias via proxy as we become less able to understand how an algorithm is reaching its output and what assumptions it is making about an individual in doing so.\n",
      "Difficulty in assessing credit discrimination by gender [footnote 47]\n",
      "New York’s Department of Financial Services’ investigated Goldman Sachs for potential credit discrimination by gender. This came from the web entrepreneur David Heinemeier Hansson who tweeted that the Apple Card, which Goldman manages, had given him a credit limit 20 times that extended to his wife, though the two filed joint tax returns and she had a better credit score. Goldman Sachs’ response was that it did not consider gender when determining creditworthiness, as this would be illegal in the US, and therefore there was no way they could discriminate on the basis of gender. The full facts around this case are not yet public, as the regulatory investigation is ongoing. Nonetheless, there is evidence that considering gender could help mitigate gender bias or at least test the algorithm to better understand whether it is biased. This example brings up a key challenge for financial organisations in terms of testing for bias which we will explore later in this chapter.\n",
      "Current landscape\n",
      "Despite plenty of anecdotal evidence, there has previously been a lack of structured evidence about the adoption of machine learning (ML) in UK financial services. In 2018, a Financial Times global survey of banks provided evidence of a cautious approach being taken by firms. [footnote 48] However, in 2019, the Bank of England and FCA conducted a joint survey into the use of ML in financial services, which was the first systematic survey of its kind. The survey found that ML algorithms were increasingly being used in UK financial services, with two thirds of respondents [footnote 49] reporting its use in some form and the average firm using ML applications in two business areas. [footnote 50] It also found that development is entering the more mature stages of deployment, in particular in the banking and insurance sectors.\n",
      "The survey focused on ML algorithms in financial services, rather than rules-based algorithms. The key difference being that a human does not explicitly programme an ML algorithm, but instead computer programmes fit a model or recognise patterns from data. Many ML algorithms constitute an incremental, rather than fundamental, change in statistical methods used in financial services. They also provide more flexibility as they are not constrained by the linear relationships often imposed in traditional economic and financial analysis and can often make better predictions than traditional models or find patterns in large amounts of data from increasingly diverse sources.\n",
      "The key uses of ML algorithms in financial services are to inform back-office functions, such as risk management and compliance. This can include identifying third parties who are trying to damage customers, or the bank itself, through fraud, identity theft and money laundering. This is the area where ML algorithms find the highest extent of application due to their ability to connect large datasets and pattern detection. However, ML algorithms are also increasingly being applied to front-office areas, such as credit scoring, where ML applications are used in granting access to credit products such as credit cards, loans and mortgages.\n",
      "The Financial Conduct Authority and the Bank of England conducted a survey about the maturity of machine learning tools, by business area within financial services. The survey results are organised by the following levels of deployment: “initial experiments”, “development phase”, “small-scale deployment”, “medium-scale deployment” and “full deployment.” Roughly 30% to 50% of respondents reported “risk management”, “customer engagement” and “other” use cases were in “initial experiments”, “small-scale deployment” and “full deployment”. Approximately 20% to 30% of respondents reported that “credit” and “sales and trading” were in “small-scale deployment”. Around 10% to 20% of respondents reported that “customer engagement”, “other”, “credit”, “sales and trading”, “investment banking (M&A, ECM, DCM)” and “miscellaneous” were being deployed at all levels. Between 0% and 10% of respondents stated that “asset management’, “general insurance”, “life insurance” and “treasury” uses were present at all levels of deployment.\n",
      "Figure 3: Machine learning maturity of different business areas in financial services, as surveyed by the FCA and Bank of England [footnote 51]\n",
      "The underlying methodology behind these different applications varies from more traditional methods such as logistic regression and random forest models, to more advanced machine learning and natural language processing. There are varying reports on how widely the most advanced tools are being used. For example, the FCA and Bank of England report highlights how many cases of ML development have entered into the more advanced stages of deployment, in particular in the banking and insurance sectors. [footnote 52]\n",
      "We have seen a real acceleration in the last five years with machine learning and deep learning being more widely adopted in financial services. We do not see this slowing down. - CDEI interview with leading credit reference agency\n",
      "The Bank of England has identified the following potential benefits of increased use of algorithmic decision-making in financial services: improved customer choice, services and more accurate pricing; increased access to credit for households and SMEs; substantially lower cross border transaction costs; and improved diversity and resilience of the system.[^53] However, there are obstacles to the adoption of algorithmic decision-making in financial services. Organisations report these to be mainly internal to firms themselves, rather than stemming from regulation, and range from lack of data accessibility, legacy systems, and challenges integrating ML into existing business processes. [footnote 54]\n",
      "The FCA is the lead sector regulator for financial services and regulates 59,000 financial services firms and financial markets in the UK and is the prudential regulator for over 18,000 of those firms. The FCA and Bank of England recently jointly announced that they would be establishing the Financial Services Artificial Intelligence Public-Private Forum (AIPPF). [footnote 55] The Forum was set up in recognition that work is needed to better understand how the pursuit of algorithmic decision-making and increasing data availability are driving change in financial markets and consumer engagement, and a wide range of views need to be gathered on the potential areas where principles, guidance or good practice examples could support the safe adoption of these technologies.\n",
      "4.2 Findings\n",
      "Our main focus within financial services has been on credit scoring decisions made about individuals by traditional banks. We did not look in detail at how algorithms are being used by fintech companies and in the insurance industry, but do incorporate key trends and findings from these areas in our Review. We also separately conducted a short piece of research on AI in personal insurance. [footnote 56] In order to understand the key opportunities and risks with regards to the use of algorithms in the financial sector we conducted semi-structured interviews with financial services organisations, predominantly traditional banks and credit reference agencies. We also ran an online experiment with the Behavioural Insights Team to understand people’s perceptions of the use of algorithms in credit scoring and how fair they view the use of data which could act as a proxy for sex or ethnicity, particularly newer forms of data, such as social media, in informing these algorithms.\n",
      "On the whole, the financial organisations we interviewed range from being very innovative to more risk averse with regards to the models they are building and the data sources they are drawing on. However, they agreed that the key obstacles to further innovation in the sector were as follows: Data availability, quality and how to source data ethically Available techniques with sufficient explainability A risk averse culture, in some parts, given the impacts of the financial crisis Difficulty in gauging consumer and wider public acceptance\n",
      "Algorithms are mainly trained using historical data, with financial organisations being hesitant to incorporate newer, non-traditional, data-sets\n",
      "In our interviews, organisations argued that financial data would be biased due to the fact that, in the past, mainly men have participated in the financial system. One could also see another data-related risk in the fact that there are fewer training datasets for minority communities might result in the reduced performance of investment advice algorithms for these communities.\n",
      "A key challenge is posed by data… the output of a model is only as good as the quality of data fed into it – the so-called “rubbish in, rubbish out” [footnote 57] problem… AI/ML is underpinned by the huge expansion in the availability and sources of data: as the amount of data used grows, so the scale of managing this problem will increase. [footnote 58] - James Proudman, Bank of England\n",
      "On the whole, financial organisations train their algorithms on historical data. The amount of data that a bank or credit reference agency has at its disposal varies. We know from our interview with one of the major banks that they use data on the location of transactions made, along with data they share with other companies to identify existing credit relationships between banks and consumers. In the case of a credit reference agency we spoke to, models are built on historical data, but are trained on a variety of public sources including applications made on the credit market, the electoral registry, public data, such as filing for bankruptcy, data provided by the clients themselves, and behavioural information such as: turnover, returned items, rental data.\n",
      "In terms of using non-traditional forms of data, the phenomenon of credit-worthiness by association [footnote 59] describes the move from credit scoring algorithms just using data from an individual’s credit history, to drawing on additional data about an individual for example their rent repayment history or their wider social network. Of the companies we spoke to, most were not using social media data and were sceptical of its value. For example, a credit reference agency and major bank we interviewed had explored using social media data a few years ago, but decided against it as they did not believe it would sufficiently improve the accuracy of the algorithm to justify its use.\n",
      "The use of more data from non-traditional sources could enable population groups who have historically found it difficult to access credit, due to there being less data about them from traditional sources, to gain better access in future. For example in our interview with a credit reference agency they spoke of customers who are referred to as “thin files”, as there is little data available about them, which could be a source of financial exclusion. Their approach with these customers is to ensure decisions about them are subjected to manual review. In order to address the problem of “thin files”, Experian added rent repayments to the credit reports of more than 1.2 million tenants in the UK with the intention of making it easier for renters to access finance deals. [footnote 60]\n",
      "While having more data could improve inclusiveness and improve the representativeness of datasets, more data and more complex algorithms could also increase the potential for the introduction of indirect bias via proxy as well as the ability to detect and mitigate it.\n",
      "Although there is a general standard not to collect protected characteristics data, financial organisations are developing approaches to testing their algorithms for bias\n",
      "It is common practice to avoid using data on protected characteristics, or proxies for those characteristics, as inputs into decision-making algorithms, as to do so is likely to be unlawful or discriminatory. However, understanding the distribution of protected characteristics among the individuals affected by a decision is necessary to identify biased outcomes. For example, it is difficult to establish the existence of a gender pay gap at a company without knowing whether each employee is a man or woman. This tension between the need to create algorithms which are blind to protected characteristics while also checking for bias against those same characteristics creates a challenge for organisations seeking to use data responsibly. This means that whilst organisations will go to lengths to ensure they are not breaking the law or being discriminatory, their ability to test how the outcomes of their decisions affect different population groups is limited by the lack of demographic data. Instead organisations test their model’s accuracy through validation techniques [footnote 61] and ensuring sufficient human oversight of the process as a way of managing bias in the development of the model.\n",
      "Case study - London fintech company\n",
      "We spoke to a London fintech company which uses supervised ML in order to predict whether people are able to repay personal loans and to detect fraud. In line with legislation, they do not include protected characteristics in their models, but to check for bias they adopt a ‘fairness through unawareness’ approach [footnote 62] involving ongoing monitoring and human judgement. The ongoing monitoring includes checking for sufficiency across the model performance, business optimisation and building test models to counteract the model. The human judgement involves interpreting the direction in which their models are going and if a variable does not fit the pattern rejecting or transforming it. This approach requires significant oversight to ensure fair operation and to effectively mitigate bias.\n",
      "We consider the problems inherent in “fairness through unawareness” approaches in Chapter 7.  ↩\n",
      "Some organisations do hold some protected characteristic data, which they do not use in their models. For example, a major bank we interviewed has access to sex, age and postcode data on their customers, and can test for bias on the basis of sex and age. Moreover, banks advise that parameters that they consider to strongly correlate with protected characteristics are usually removed from the models. Given there is no defined threshold for bias imposed by the FCA or any standards body, organisations manage risks around algorithmic bias using their own judgement and by managing data quality. A small proportion of companies analyse model predictions on test data, such as representative synthetic data or anonymised public data.\n",
      "The extent to which a problem of algorithmic bias exists in financial services is still relatively unclear given that decisions around finance and credit are often highly opaque for reasons of commercial sensitivity and competitiveness. Even where it is apparent that there are differences in outcomes for different demographic groups, without extensive access to the models used by companies in their assessments of individuals, and access to protected characteristic data, it is very difficult to determine whether these differences are due to biased algorithms or to underlying societal, economic or structural causes.\n",
      "Insights from our work in financial services have fed into our wider recommendation around collecting protected characteristics data which is set out in Chapter 7.\n",
      "Case study: Bias in insurance algorithms\n",
      "A Propublica investigation [footnote 63] in the US found that people in minority neighbourhoods on average paid higher car insurance premiums than residents of majority-white neighbourhoods, despite having similar accident costs. While the journalists could not confirm the cause of these differences, they suggest biased algorithms may be to blame.\n",
      "Like any organisation using algorithms to make significant decisions, insurers must be mindful of the risks of bias in their AI systems and take steps to mitigate unwarranted discrimination. However, there may be some instances where using proxy data may be justified. For example, while car engine size may be a proxy for sex, it is also a material factor in determining damage costs, giving insurers more cause to collect and process information related to it. Another complication is that insurers often lack the data to identify where proxies exist. Proxies can in theory be located by checking for correlations between different data points and the protected characteristic in question (e.g. between the colour of a car and ethnicity). Yet insurers are reluctant to collect this sensitive information for fear of customers believing the data will be used to directly discriminate against them.\n",
      "The Financial Conduct Authority conducted research [footnote 64] in 2018 on the pricing practices of household insurance firms. One of the key findings was the risk that firms could discriminate against consumers by using rating factors in pricing based either directly or indirectly on data relating to or derived from protected characteristics. The FCA has since done further work, including a market study and initiating a public debate, on fair pricing and related possible harms in the insurance industry.\n",
      "Ensuring explainability of all models used in financial services is key\n",
      "Explainability refers to the ability to understand and summarise the inner workings of a model, including the factors that have gone into the model. As set out in Section 2.5, explainability is key to understanding the factors causing variation in outcomes of decision-making systems between different groups and to assess whether or not this is regarded as fair. In polling undertaken for this review, of the possible safeguards which could be put in place to ensure an algorithmic decision-making process was as fair as possible, an easy to understand explanation came in second in a list of six options, after only human oversight.\n",
      "In the context of financial services, the explainability of an algorithm is important for regulators, banks and customers. For banks, when developing their own algorithms, explainability should be a key requirement in order to have better oversight of what their systems do and why, so they can identify and mitigate discriminatory outcomes. For example when giving loans, using an explainable algorithm makes it possible to examine more directly the degree to which relevant characteristics are acting as a proxy for other characteristics, and causing differences in outcomes between different groups. This means that where there may be valid reasons for loans to be disproportionately given to one group over another this can be properly understood and explained.\n",
      "For customers, explainability is crucial so that they can understand the role the model has played in the decision being made about them. For regulators, understanding how an algorithmically-assisted decision was reached is vital to knowing whether an organisation has met legal requirements and treated people fairly in the process. Indeed, the expert panel we convened for our AI Barometer discussions, viewed a lack of explainability for regulators as a significantly greater risk than a lack of transparency for consumers of algorithmic decision-making in financial services. [footnote 65]\n",
      "The lack of explainability of machine learning models was highlighted as one of the top risks by respondents to the Bank of England and FCA’s survey. The survey highlighted that in use cases such as credit scoring where explainability was a priority, banks were opting for logistic regression techniques, with ML elements, to ensure decisions could be explained to customers where required. However, research by the ICO [footnote 66] has shown that while some organisations in banking and insurance are continuing to select interpretable models in their customer-facing AI decision-support applications, they are increasingly using more opaque ‘challenger’ models alongside these, for the purposes of feature engineering or selection, comparison, and insight.\n",
      "In our interviews with banks they reported using tree-based models, such as ‘random forests’, as they claim they generate the most accurate predictions. However, they acknowledged that the size and complexity of the models made it difficult to explain exactly how they work and the key variables that drive predictions. As a result, logistic regression techniques with ML elements continue to be popular in this type of use case, and provide a higher degree of apparent explainability.\n",
      "There are approaches to breaking down the procedures of neural networks in order to justify why a decision is made about a particular customer or transaction. In our interview with a credit reference agency they described an example in which their US team had calculated the impact that every input parameter had on the final score and then used this information to return the factors that had the biggest impact, in a format that was customer-specific but still general enough to work across the entire population. This then means a low credit score could be explained with a simple statement such as “rent not paid on time”. Nonetheless, even if there are approaches to explain models at a system level and understand why credit has been denied, these are not always directly available as individual level explanations to customers and it may be difficult to assign it to one factor, rather than a combination.\n",
      "In other cases, firms are required to provide information about individual decisions. This includes under the GDPR (Articles 13, 14, 15 and 22 in particular), under FCA rules for lenders and under industry standards such as the Standards of Lending Practice.\n",
      "The risks to explainability may not always come from the type of model being used, but from other considerations for example commercial sensitivities or concerns that people may game or exploit a model if they know too much about how it works. Interestingly, public attitudes research conducted by the RSA [footnote 67] suggested that customers could consider some circumstances in which commercial interests could supersede individuals’ rights, for example when making financial decisions, in recognition that providing a detailed explanation could backfire by helping fraudsters outwit the system, and where such interests should be overruled. The firms we interviewed reported mostly designing and developing tools in-house, apart from sometimes procuring from third-parties for the underlying platforms and infrastructure such as cloud computing, which should mean that intellectual property considerations do not impinge on explainability standards. Nonetheless, where there may be commercial sensitivities, concerns around gaming, or other risks these should be clearly documented from the outset and justified in the necessary documentation.\n",
      "There are clear benefits to organisations, individuals and society in explaining algorithmic decision-making in financial services. Providing explanations to individuals affected by a decision can help organisations ensure more fairness in the outcomes for different groups across society. Moreover, for organisations it makes business sense as a way of building trust with your customers by empowering them to understand the process and providing them the opportunity to challenge where needed.\n",
      "ICO and the Alan Turing Institute’s Explainability Guidance\n",
      "In May 2020, the ICO and the Alan Turing Institute published their guidance [footnote 68] to organisations on how to explain decisions made with AI, to the individuals affected by them. The guidance sets out key concepts and different types of explanations, along with more tailored support to senior management teams on policies and procedures organisations should put in place to ensure they provide meaningful explanations to affected individuals. The FCA has fed into this guidance to ensure it takes into account the opportunities and challenges facing banks in explaining AI-assisted decisions to customers.\n",
      "Public acceptability of the use of algorithms in financial services is higher than in other sectors, but can be difficult to gauge\n",
      "In polling undertaken for this review, when asked how aware they were of algorithms being used to support decisions in the context of the four sectors we have looked at in this report, financial services was the only option selected by a majority of people (around 54-57%). This was in contrast to only 29-30% of people being aware of their use in local government.\n",
      "In our interviews with financial companies, it was evident they were making efforts to understand public acceptability, mainly in the context of their customers. For example, financial organisations we interviewed had conducted consumer polling and focus groups to understand how the public felt about the use of payment data. In another interview, we learnt that a bank gauged public acceptability with a focus on customer vulnerability, by conducting surveys and interviews, but also by considering the impact of a new product on customers through their risk management framework. Moreover, each product goes through an annual review, which takes into account if there have been any problems, for example customer complaints.\n",
      "In order to better understand public attitudes we conducted a public engagement exercise with the Behavioural Insights Team (BIT) [footnote 69] through their online platform, Predictiv. We measured participants’ perceptions of fairness of banks’ use of algorithms in loan decisions. In particular we wanted to understand how people’s fairness perceptions of banking practices varied depending on the type of information an algorithm used in a loan decision, for example the use of a variable which could serve as a proxy for a protected characteristic such as sex or ethnicity.\n",
      "We found that, on average, people moved twice as much money away from banks that use algorithms in loan application decisions, when told that the algorithms draw on proxy data for protected characteristics or social media data. Not surprisingly, those historically most at risk of being discriminated against in society feel most strongly that it is unfair for a bank to use proxy information for protected characteristics. For example, directionally, women punish the bank that used information which could act as a proxy for sex more strongly than men. However, some people thought it was fair to use the proxy variable if it produced a more accurate result. This brings into question whether there are legitimate proxies, for example salary, which although could function as proxies for sex and ethnicity, may also accurately assist a bank in making decisions around loan eligibility. The experiment also found that people are less concerned about the use of social media data than about data that relates to sex and ethnicity. However, the frequency with which an individual uses social media does not have an impact on how concerned they are about its use in informing loan decisions.\n",
      "This experiment highlighted the challenges in framing questions about a bank’s use of algorithms in an unbiased and nuanced way. More research is needed into the use of proxies and non-traditional forms of data in financial services to give financial organisations the confidence that they are innovating in a way that is deemed acceptable by the public.\n",
      "Regulation on bias and fairness in financial services is currently not seen as an unjustified barrier to innovation, but additional guidance and support would be beneficial\n",
      "The majority (75%) of respondents to the FCA and the Bank of England’s survey, said that they did not consider Prudential Regulation Authority [footnote 70] / FCA regulations an unjustified barrier to deploying ML algorithms. This view was supported by organisations we interviewed. This may be because the FCA has responded constructively to the increased use of ML algorithms and is proactively finding ways to support ethical innovation. However, there were respondents in the survey who noted challenges of meeting regulatory requirements to explain decision-making when using more advanced, complex algorithms. Moreover, firms also highlighted that they would benefit from additional guidance from regulators on how to apply existing regulations to ML.\n",
      "Whilst it is positive that the FCA is seen as a constructive, innovation-enabling regulator, future regulation may need to be adapted or adjusted to account for developments in ML algorithms in order to protect consumers. The AIPPF will be well-placed to identify where this may be the case whilst also identifying good practice examples.\n",
      "Future CDEI work: CDEI will be an observer on the Financial Conduct Authority and Bank of England’s AI Public Private Forum which will explore means to support the safe adoption of machine learning and artificial intelligence within financial services.\n",
      "5. Policing\n",
      "Overview of findings:\n",
      "Adoption of algorithmic decision-making is at an early stage, with very few tools currently in operation in the UK. There is a varied picture across different police forces, both on levels of usage and approaches to managing ethical risks.\n",
      "Police forces have identified opportunities to use data analytics and AI at scale to better allocate resources, but there is a significant risk that without sufficient care systematically unfair outcomes could occur.\n",
      "The use of algorithms to support decision-making introduces new issues around the balance between security, privacy and fairness. There is a clear requirement for strong democratic oversight of this and meaningful engagement with the public is needed on which uses of police technology are acceptable.\n",
      "Clearer national leadership is needed around the ethical use of data analytics in policing. Though there is strong momentum in data ethics in policing at a national level, the picture is fragmented with multiple governance and regulatory actors and no one body fully empowered or resourced to take ownership. A clearer steer is required from the Home Office.\n",
      "Recommendation to government: Recommendation 3: The Home Office should define clear roles and responsibilities for national policing bodies with regards to data analytics and ensure they have access to appropriate expertise and are empowered to set guidance and standards. As a first step, the Home Office should ensure that work underway by the National Police Chiefs’ Council and other policing stakeholders to develop guidance and ensure ethical oversight of data analytics tools is appropriately supported.\n",
      "Advice to police forces/ suppliers:\n",
      "Police forces should conduct an integrated impact assessment before investing in new data analytics software as a full operational capability, to establish a clear legal basis and operational guidelines for use of the tool. Further details of what the integrated impact assessment should include are set out in the report we commissioned from RUSI.\n",
      "Police forces should classify the output of statistical algorithms as a form of police intelligence, alongside a confidence rating indicating the level of uncertainty associated with the prediction.\n",
      "Police forces should ensure that they have appropriate rights of access to algorithmic software and national regulators should be able to audit the underlying statistical models if needed (for instance, to assess risk of bias and error rates). Intellectual property rights must not be a restriction on this scrutiny.\n",
      "Future CDEI work: CDEI will be applying and testing its draft ethics framework for police use of data analytics with police partners on real-life projects and developing underlying governance structures to make the framework operational.\n",
      "5.1 Background\n",
      "There have been notable government reviews into the issue of bias in policing which are important when considering the risks and opportunities around the use of technology in policing. For example, the 2017 Lammy Review [footnote 71] found that BAME individuals faced bias, including overt discrimination, in parts of the justice system. And prior to the Lammy Review, the 1999 public inquiry into the fatal stabbing of Black teenager Stephen Lawrence branded the Metropolitan Police force “institutionally racist” [footnote 72] .More recently, the 2017 Race Disparity Audit [footnote 73] highlighted important disparities in treatment and outcomes for BAME communities along with lower levels of confidence in the police among younger Black adults. With these findings in mind, it is vital to consider historic and current disparities and inequalities when looking at how algorithms are incorporated into decision-making in policing. Whilst there is no current evidence of police algorithms in the UK being racially biased, one can certainly see the risks of algorithms entrenching and amplifying widely documented human biases and prejudices, in particular against BAME individuals, in the criminal justice system.\n",
      "The police have long been under significant pressure and scrutiny to predict, prevent and reduce crime. But as Martin Hewitt QPM, Chair of the National Police Chiefs’ Council (NPCC) and other senior police leaders, have highlighted that “the policing environment has changed profoundly in many ways and the policing mission has expanded in both volume and complexity. This has taken place against a backdrop of diminishing resources”. [footnote 74]\n",
      "Prime Minister Boris Johnson’s announcement to recruit 20,000 new police officers, as one of his headline policy pledges [footnote 75] , signals a government commitment to respond to mounting public unease about local visibility of police officers. But the decentralised nature of policing in England and Wales means that each force is developing their own plan for how to respond to these new pressures and challenges.\n",
      "Police forces have access to more digital material than ever before [footnote 76] , and are expected to use this data to identify connections and manage future risks. Indeed, the £63.7 million ministerial funding announcement [footnote 77] in January 2020 for police technology programmes, amongst other infrastructure and national priorities, demonstrates the government’s commitment to police innovation.\n",
      "In response to these incentives to innovate, some police forces are looking to data analytics tools to derive insight, inform resource allocation and generate predictions. But drawing insights and predictions from data requires careful consideration, independent oversight and the right expertise to ensure it is done legally, ethically and in line with existing policing codes [footnote 78] . Despite multiple legal frameworks and codes setting out clear duties, the police are facing new challenges in adhering to the law and following these codes in their development and use of data analytics.\n",
      "Case study: Innovation in Avon and Somerset Constabulary\n",
      "Avon and Somerset Constabulary have been successful in building in-house data science expertise through their Office for Data Analytics. One of their tools is Qlik Sense, a software product that connects the force’s own internal databases and other local authority datasets. It applies predictive modelling to produce individual risk-assessment and intelligence profiles, to assist the force in triaging offenders according to their perceived level of risk.\n",
      "Although Avon and Somerset Constabulary do not operate a data ethics committee model, like West Midlands Police, they do have governance and oversight processes in place. Moreover, their predictive models are subject to ongoing empirical validation, which involves revisiting models on a quarterly basis to ensure they are accurate and adding value.\n",
      "In theory, tools which help spot patterns of activity and potential crime, should lead to more effective prioritisation and allocation of scarce police resources. A range of data driven tools are being developed and deployed by police forces including tools which help police better integrate and visualise their data, tools which help guide resource allocation decisions and those that inform decisions about individuals such as someone’s likelihood to reoffend. However, there is a limited evidence base regarding the claimed benefits, scientific validity or cost effectiveness of police use of algorithms [footnote 79] . For example, there is empirical evidence around the effectiveness of actuarial tools to predict reoffending. However, experts disagree over the statistical and theoretical validity of individual risk-assessment tools. More needs to be done to establish benefits of this technology. In order to do this the technology must be tested in a controlled, proportionate manner, following national guidelines.\n",
      "The use of data-driven tools in policing also carries significant risk. The Met Police’s Gangs Matrix [footnote 80] is an example of a highly controversial intelligence and prioritisation tool in use since 2011. The tool intends to identify those at risk of committing, or being a victim of, gang-related violence in London. Amnesty International raised serious concerns with the Gangs Matrix in 2018, in particular that it featured a disproportionate number of Black boys and young men and people were being kept on the database despite a lack of evidence and a reliance on out-of-date information [footnote 81] . In addition, the Gangs Matrix was found by the Information Commissioner’s Office to have breached data protection laws and an enforcement notice was issued to the Met Police [footnote 82] . Since, the Mayor of London, Sadiq Khan announced an overhaul [footnote 83] of the Gangs Matrix highlighting that the number of people of a Black African Caribbean background added to the database had dropped from 82.8 per cent in 2018 to 66 per cent in 2019. The Gangs Matrix is likely to continue to be closely scrutinised by civil society, regulators and policymakers.\n",
      "It is evident that without sufficient care, the use of intelligence and prioritisation tools in policing can lead to outcomes that are biased against particular groups, or systematically unfair in other regards. In many scenarios where these tools are helpful, there is still an important balance to be struck between automated decision-making and the application of professional judgement and discretion. Where appropriate care has been taken internally to consider these issues fully, it is critical for public trust in policing that police forces are transparent in how such tools are being used.\n",
      "Our approach\n",
      "Given the breadth of applications and areas where technology is being used in law enforcement we chose to focus on the use of data analytics in policing to derive insights, inform operational decision-making or make predictions. This does not include biometric identification, automated facial recognition [footnote 84] , digital forensics or intrusive surveillance. However, some of the opportunities, risks and potential approaches that we discuss remain relevant to other data-driven technology issues in policing.\n",
      "To build on and strengthen existing research and publications on these issues [footnote 85] we commissioned new, independent research from the Royal United Services Institute (RUSI). [footnote 86] The aim of this research was to identify the key ethical concerns, in particular on the issue of bias, and propose future policy to address these issues. We incorporated the findings in RUSI’s Report [footnote 87] into this chapter and, where relevant, throughout this report.\n",
      "We also issued a call for evidence on the use of algorithmic tools, efforts to mitigate bias, engagement with the public on these issues, and governance and regulation gaps across the four sectors addressed in this report, including policing, receiving a diverse range of responses.\n",
      "We have conducted extensive stakeholder engagement over the last year to understand the key challenges and concerns about the development and use of data analytics tools in this sector. For example, we have spoken to local police forces, including Avon and Somerset, Durham, Essex, Hampshire, Police Scotland and South Wales.\n",
      "Working with West Midlands Police\n",
      "West Midlands Police are one of the leading forces in England and Wales in the development of data analytics. They have an in-house data analytics lab and are the lead force on the National Data Analytics Solution. Their PCC has also set up an Ethics Committee [footnote 88] to review data science projects developed by the lab and advise the PCC and Chief Constable on whether the proposed project has sufficiently addressed legal and ethical considerations. We have met with representatives at West Midlands Police and the PCC’s Office multiple times throughout this project and we were invited to observe a meeting of their ethics committee. They were also interviewed for and contributed to the RUSI report and development of our policing framework. We are interested in seeing, going forward, to what extent other forces follow the West Midlands PCC Ethics Committee model and hope to continue working closely with West Midlands on future policing work.\n",
      "West Midlands Police and Crime Commissioner, ‘Ethics Committee’   ↩\n",
      "We established a partnership with the Cabinet Office’s Race Disparity Unit (RDU), a UK Government unit which collates, analyses and publishes government data on the experiences of people from different ethnic backgrounds in order to drive policy change where disparities are found. We have drawn on their expertise to better understand how algorithmic decision-making could disproportionately impact ethnic minorities. Our partnership has included jointly meeting with police forces and local authorities, along with the RDU and their Advisory Group contributing to our roundtables with RUSI and reviewing our report and recommendations.\n",
      "We have met with senior representatives from policing bodies including the National Police Chiefs’ Council (NPCC), Her Majesty’s Inspectorate of Constabulary and Fire Rescue Services (HMICFRS), the Police ICT Company, the College of Policing, the Association of Police and Crime Commissioners (APCC), and regulators with an interest in this sector, including the Information Commissioner’s Office. We have also engaged with teams across the Home Office, with an interest in police technology.\n",
      "A draft framework to support police to develop data analytics ethically\n",
      "CDEI has been developing a Draft Framework to support police in innovating ethically with data. It is intended for police project teams developing or planning to develop data analytics tools. It should also help senior decision-makers in the police identify the problems best addressed using data analytics along with those not suited to a technological solution. The Framework is structured around the agile delivery cycle and sets out the key questions that should be asked at each stage. We have tested the Framework with a small group of stakeholders from police forces, academics and civil society and plan to release it more widely following the publication of this review. The feedback we have received to date has also highlighted that a well-informed public debate around AI in policing is missing. These are complex issues where current public commentary is polarised. But without building a common consensus on where and how it is acceptable for police to use AI, the police risk moving ahead without public buy-in. CDEI will be exploring options for facilitating that public conversation going forward and testing the Framework with police forces.\n",
      "Future CDEI work: CDEI will be applying and testing its draft ethics framework for police use of data analytics with police partners on real-life projects and developing underlying governance structures to make the framework operational.\n",
      "5.2 Findings\n",
      "Algorithms are in development and use across some police forces in England and Wales but the picture is varied\n",
      "From the responses we received to our call for evidence and wider research, we know there are challenges in defining what is meant by an algorithmic tool and consequently understanding the extent and scale of adoption. In line with this, it is difficult to say with certainty how many police forces in England and Wales are currently developing, trialling or using algorithms due in part to different definitions and also a lack of information sharing between forces.\n",
      "The RUSI research surfaced different terms being used to refer to data analytics tools used by police forces. For example, several interviewees considered the term ‘predictive policing’ problematic. Given that many advanced analytics tools are used to ‘classify’ and ‘categorise’ entities into different groups, it would be more accurate to describe them as tools for ‘prioritisation’ rather than ‘prediction’. For instance, ‘risk scoring’ offenders according to their perceived likelihood of reoffending by comparing selected characteristics within a specified group does not necessarily imply that an individual is expected to commit a crime. Rather, it suggests that a higher level of risk management is required than the level assigned to other individuals within the same cohort.\n",
      "RUSI sorted the application of data analytics tools being developed by the police into the following categories: Predictive mapping : the use of statistical forecasting applied to crime data to identify locations where crime may be most likely to happen in the near future. Recent data suggests that 12 of 43 police forces in England and Wales are currently using or developing such systems. [footnote 89] Individual risk assessment : the use of algorithms applied to individual-level personal data to assess risk of future offending. For example, the Offender Assessment System (OASys) and the Offender Group Reconviction Scale (OGRS), routinely used by HM Prison and Probation Service (HMPPS) to measure individuals’ likelihood of reoffending and to develop individual risk management plans. [footnote 90]\n",
      "Data scoring tools : the use of advanced machine learning algorithms applied to police data to generate ‘risk’ scores of known offenders.\n",
      "Other : complex algorithms used to forecast demand in control centres, or triage crimes for investigation according to their predicted ‘solvability’.\n",
      "Examples of the data scoring tools include:\n",
      "A Harm Assessment Risk Tool (HART), developed and being deployed by Durham police. It uses supervised machine learning to classify individuals in terms of their likelihood of committing a violent or non-violent offence within the next two years.\n",
      "Use of Qlik Sense (a COTS analytics platform) by Avon and Somerset to link data from separate police databases to generate new insights into crime patterns.\n",
      "The Integrated Offender Management Model, in development but not currently deployed by West Midlands Police. It makes predictions as to the probability that an individual will move from committing low / middling levels of harm, via criminal activity, to perpetrating the most harmful offending.\n",
      "There have also been reports of individual forces buying similar technology, for example the Origins software which is reportedly currently being used by the Metropolitan Police Service and has previously been used by several forces including Norfolk, Suffolk, West Midlands and Bedfordshire police forces [footnote 91] . The software intends to help identify whether different ethnic groups “specialise” in particular types of crime and has come under strong critique from equality and race relations campaigners who argue that it is a clear example of police forces racial profiling at a particularly fraught time between police and the Black community.\n",
      "In England and Wales, police forces are currently taking a variety of different approaches to their development of algorithmic systems, ethical safeguards, community engagement and data science expertise.\n",
      "Mitigating bias and ensuring fairness requires looking at the entire decision-making process\n",
      "As set out earlier in the report, we think it is crucial to take a broad view of the whole decision-making process when considering the different ways bias can enter a system and how this might impact on fairness. In the context of policing, this means not only looking at the development of an algorithm, but also the context in which it is deployed operationally.\n",
      "At the design and testing stage, there is a significant risk of bias entering the system due to the nature of the police data which the algorithms are trained on. Police data can be biased due to it either being unrepresentative of how crime is distributed or in more serious cases reflecting unlawful policing practices. It is well-documented [footnote 92] that certain communities are over or under-policed and certain crimes are over or under-reported. For example, a police officer interviewed in our RUSI research, highlighted that ‘young Black men are more likely to be stopped and searched than young white men, and that’s purely down to human bias’. Indeed this is backed by Home Office data released last year stating that those who identify as Black or Black British are 9.7 times more likely to be stopped and searched by an officer than a white person [footnote 93] . Another way police data can provide a misrepresentative picture is that individuals from disadvantaged socio demographic backgrounds are likely to engage with public services more frequently, which means that more data is held on them. Algorithms could then risk calculating groups with more data held on them by the police as posing a greater risk. Further empirical research is needed to assess the level of bias in police data and the impact of that potential bias.\n",
      "A further challenge to be considered at this stage is the use of sensitive personal data to develop data analytics tools. Whilst models may not include a variable for race, in some areas postcode can function as a proxy variable for race or community deprivation, thereby having an indirect and undue influence on the outcome prediction. If these biases in the data are not understood and managed early on this could lead to the creation of a feedback loop whereby future policing, not crime, is predicted. It could also influence how high or low risk certain crimes or areas are deemed by a data analytics tool and potentially perpetuate or exacerbate biased criminal justice outcomes for certain groups or individuals.\n",
      "At the deployment stage, bias may occur in the way the human decision-maker receiving the output of the algorithm responds. One possibility is that the decision-maker over-relies on the automated output, without applying their professional judgement to the information. The opposite is also possible, where the human decision-maker feels inherently uncomfortable with taking insights from an algorithm to the point where they are nervous to use it at all [footnote 94] , or simply ignores it in cases where their own human bias suggests a different risk level. A balance is important to ensure due regard is paid to the insights derived, whilst ensuring the professional applies their expertise and understanding of the wider context and relevant factors. It has been argued, for example by Dame Cressida Dick in her keynote address at the launch event of the CDEI/RUSI report on data analytics in policing, that police officers may be better equipped than many professionals to apply a suitable level of scepticism to the outcome of an algorithm, given that weighing the reliability of evidence is so fundamental to their general professional practice.\n",
      "Without sufficient care of the multiple ways bias can enter the system, outcomes can be systematically unfair and lead to bias and discrimination against individuals or those within particular groups.\n",
      "There is a need for strong national leadership on the ethical use of data analytics tools\n",
      "The key finding from the RUSI research was a “widespread concern across the UK law enforcement community regarding the lack of any official national guidance for the use of algorithms in policing, with respondents suggesting that this gap should be addressed as a matter of urgency.” [footnote 95] Without any national guidance, initiatives are being developed to different standards and to varying levels of oversight and scrutiny.\n",
      "For example, while all police forces in England and Wales have established local ethics committees, these are not currently resourced to look at digital projects. Instead, some Police and Crime Commissioners, like West Midlands, have established data ethics committees to provide independent ethical oversight to police data analytics projects. However, given the absence of guidance it is unclear whether each force should be setting up data ethics committees, upskilling existing ones, or whether regional or a centralised national structure should be set up to provide digital ethical oversight to all police forces. A review of existing police ethics committees would be useful in order to develop proposals for ethical oversight of data analytics projects.\n",
      "Similarly, the lack of national coordination and oversight means that data initiatives are developed at a local level. This can lead to pockets of innovation and experimentation. However, it also risks meaning that efforts are duplicated, knowledge and lessons are not transferred across forces and systems are not made interoperable. As described by a senior officer interviewed as part of the RUSI project, “it’s a patchwork quilt, uncoordinated, and delivered to different standards in different settings and for different outcomes”. [footnote 96]\n",
      "There is work underway at a national level, led by the National Police Chiefs’ Council, in order to develop a coordinated approach to data analytics in policing. This is reflected in the National Digital Policing Strategy [footnote 97] , which sets out an intention to develop a National Data Ethics Governance model, and to provide clear lines of accountability on data and algorithm use at the top of all policing organisations. This should continue to be supported to ensure a more consistent approach across police forces. Moreover, HMICFRS should be included in national work in this area for example by establishing an External Reference Group for police use of data analytics, with a view to incorporating use of data analytics and its effectiveness into future crime data integrity inspections.\n",
      "The RUSI report sets out in detail what a policy framework for data analytics in policing should involve and at CDEI we have been developing a draft Framework to support police project teams in addressing the legal and ethical considerations when developing data analytics tools. Without clear, consistent national guidance, coordination and oversight, we strongly believe that the potential benefits of these tools may not be fully realised, and the risks will materialise.\n",
      "Recommendations to government:\n",
      "Recommendation 3 : The Home Office should define clear roles and responsibilities for national policing bodies with regards to data analytics and ensure they have access to appropriate expertise and are empowered to set guidance and standards. As a first step, the Home Office should ensure that work underway by the National Police Chiefs’ Council and other policing stakeholders to develop guidance and ensure ethical oversight of data analytics tools is appropriately supported.\n",
      "Significant investment is needed in police project teams to address new challenges\n",
      "Whilst it is crucial that a national policy framework is developed, without significant investment in skills and expertise in police forces, no framework will be implemented effectively. If police forces are expected to be accountable for these systems,engage with developers and make ethical decisions including trade-off considerations, significant investment is needed.\n",
      "The announcement to recruit 20,000 new police officers provides an opportunity to bring in a diverse set of skills, however work is needed to ensure existing police officers are equipped to use data analytics tools. We also welcome the announcement in January 2020 of a Police Chief Scientific Advisor and dedicated funding for investment in Science, Technology and Research [footnote 98] as first steps in addressing this skills gap.\n",
      "Based on the RUSI research and our engagement with police stakeholders, we know that a wide range of skills are required, ranging from, and not limited to legal, data science, and evaluation. In particular, our research with RUSI highlighted insufficient expert legal consultation at the development phase of data analytics projects, leading to a problematic delay in adhering to legal requirements. Developing a mechanism by which specialist expertise, such as legal, can be accessed by forces would help ensure this expertise is incorporated from the outset of developing a tool.\n",
      "Moreover, there have been examples where the police force’s Data Protection Officer was not involved in discussions at the beginning of the project and has not been able to highlight where the project may interact with GDPR and support with the completion of a Data Protection Impact Assessment. Similarly, upskilling is needed of police ethics committees if they are to provide ethical oversight of data analytics projects.\n",
      "Public deliberation on police use of data-driven technology is urgently needed\n",
      "The decisions police make on a daily basis about which neighbourhoods or individuals to prioritise monitoring affect us all. The data and techniques used to inform these decisions are of great interest and significance to society at large. Moreover, due to wider public sector funding cuts, police are increasingly required to respond to non-crime problems [footnote 99] . For example, evidence suggests that police are spending less time dealing with theft and burglary and more time investigating sexual crime and responding to mental health incidents. [footnote 100]\n",
      "The longstanding Peelian Principles, which define the British approach of policing by consent , are central to how a police force should behave and their legitimacy in the eyes of the public. And the values at the core of the Peelian Principles, integrity, transparency and accountability, continue to be as relevant today, in particular in light of the ethical considerations brought up by new technologies.\n",
      "Research by the RSA and DeepMind [footnote 101] highlights that people feel more strongly against the use of automated decision systems in the criminal justice system (60 percent of people oppose or strongly oppose its use in this domain) than other sectors such as financial services. Moreover, people are least familiar with the use of automated decision-making systems in the criminal justice system; 83 percent were either not very familiar or not at all familiar with its use. These findings suggest a risk that if police forces move too quickly in developing these tools, without engaging meaningfully with the public, there could be significant public backlash and a loss of trust in the police’s use of data. A failure to engage effectively with the public is therefore not only an ethical risk, but a risk to the speed of innovation.\n",
      "Police have many existing ways of engaging with the public through relationships with community groups and through Police and Crime Commissioners (PCC). West Midlands PCC have introduced a community representative role to their Ethics Committee to increase accountability for their use of data analytics tools. However, a civil society representative interviewed by RUSI highlighted that ethics committees could “act as a fig leaf over wider discussions” which the police should be having with the public.\n",
      "We should take the steady increase in public trust in police to tell the truth since the 1980s [footnote 102] as a promising overarching trend. This signals an opportunity for police, policymakers, technologists, and regulators, to ensure data analytics tools in policing are designed and used in a way that builds legitimacy and is trustworthy in the eyes of the public.\n",
      "6. Local government\n",
      "Overview of findings:\n",
      "Local authorities are increasingly using data to inform decision-making across a wide range of services. Whilst most tools are still in the early phase of deployment, there is an increasing demand for sophisticated predictive technologies to support more efficient and targeted services.\n",
      "Data-driven tools present genuine opportunities for local government when used to support decisions. However, tools should not be considered a silver bullet for funding challenges and in some cases will require significant additional investment to fulfil their potential and possible increase in demand for services.\n",
      "Data infrastructure and data quality are significant barriers to developing and deploying data-driven tools; investing in these is necessary before developing more advanced systems.\n",
      "National guidance is needed as a priority to support local authorities in developing and using data-driven tools ethically, with specific guidance addressing how to identify and mitigate biases. There is also a need for wider sharing of best practice between local authorities.\n",
      "Recommendation to government: Recommendation 4 : Government should develop national guidance to support local authorities to legally and ethically procure or develop algorithmic decision-making tools in areas where significant decisions are made about individuals, and consider how compliance with this guidance should be monitored.\n",
      "Future CDEI work: CDEI is exploring how best to support local authorities to responsibly and ethically develop data-driven technologies, including possible partnerships with both central and local government.\n",
      "6.1 Background\n",
      "Local authorities are responsible for making significant decisions about individuals on a daily basis. The individuals making these decisions are required to draw on complex sources of evidence, as well as their professional judgement. There is also increasing pressure to target resources and services effectively following a reduction of £16 billion in local authority funding over the last decade. [footnote 103] These competing pressures have created an environment where local authorities are looking to digital transformation as a way to improve efficiency and service quality. [footnote 104]\n",
      "Whilst most research has found machine learning approaches and predictive technologies in local government to be in a nascent stage, there is growing interest in AI as a way to maximise service delivery and target early intervention as a way of saving resources further down the line when a citizen’s needs are more complex. [footnote 105] By bringing together multiple data sources, or representing existing data in new forms, data-driven technologies can guide decision-makers by providing a more contextualised picture of an individual’s needs. Beyond decisions about individuals, these tools can help predict and map future service demands to ensure there is sufficient and sustainable resourcing for delivering important services.\n",
      "However, these technologies also come with significant risks. Evidence has shown that certain people are more likely to be overrepresented in data held by local authorities and this can then lead to biases in predictions and interventions. [footnote 106] A related problem is when the number of people within a subgroup is small, data used to make generalisations can result in disproportionately high error rates amongst minority groups. In many applications of predictive technologies, false positives may have limited impact on the individual. However in particularly sensitive areas, such as when deciding if and how to intervene in a case where a child may be at risk, false negatives and positives both carry significant consequences, and biases may mean certain people are more likely to experience these negative effects. Because the risks are more acute when using these technologies to support individual decision-making in areas such as adult and children’s services, it is for this reason that we have focused predominantly on these use cases. [footnote 107]\n",
      "Where is data science in local government most being used?\n",
      "Welfare and social care\n",
      "Source: Data Science in Local Government, Oxford Internet Institute, Bright et al, 2019  ↩\n",
      "6.2 Findings\n",
      "Our work on local government as a sector began with desk based research facilitated through our call for evidence and the landscape summary we commissioned. This evidence gathering provided a broad overview of the challenges and opportunities presented by predictive tools in local government.\n",
      "We wanted to ensure our research was informed by those with first-hand accounts of the challenges of implementing and using these technologies, so we met and spoke with a broad range of people and organisations. This included researchers based in academic and policy organisations, third-party tool providers, local authorities, industry bodies and associations, and relevant government departments.\n",
      "It is difficult to map how widespread algorithmic decision-making is in local government\n",
      "There have been multiple attempts to map the usage of algorithmic decision-making tools across local authorities but many researchers have found this challenging. [footnote 109] An investigation by The Guardian found that, at a minimum, 140 councils out of 408 have invested in software contracts that cover identifying benefit fraud, identifying children at risk and allocating school places. [footnote 110] However this did not include additional use cases found in a report by the Data Justice Lab, a research group based in Cardiff University. The Data Justice Lab used Freedom of Information requests to learn which tools are being used and how frequently. However, there were many challenges with this approach with one fifth of requests being delayed or never receiving a response. [footnote 111] On the part of the local authorities, we have heard that there is often a significant challenge presented by the inconsistent terminology being used to describe algorithmic decision-making systems leading to varied reporting across local authorities using similar technologies. It is also sometimes difficult to coordinate activities across the whole authority because service delivery areas can operate relatively independently.\n",
      "Given the rising interest in the use of predictive tools in local government, local authorities are keen to emphasise that their algorithms support rather than replace decision-makers, particularly in sensitive areas such as children’s social services. Our interviews found that local authorities were concerned that the current narrative focused heavily on automation rather than their focus which is more towards using data to make more evidence based decisions.\n",
      "There is a risk that concerns around public reaction or media reporting on this topic will disincentivize transparency in the short-term. However, this is likely to cause further suspicion if data-driven technologies in local authorities appear opaque to the public. This may go on to harm trust if citizens do not have a way to understand how their data is being used to deliver public services. We believe that introducing requirements to promote transparency across the public sector will help standardise reporting, support researchers and build public trust (see Chapter 9 below for further discussion).\n",
      "Comparison: Bias in local government and policing\n",
      "There are many overlaps in the risks and challenges of data-driven technologies in both policing and local government. In both cases, public sector organisations are developing tools with data that may not be high quality and where certain populations are more likely to be represented, which could lead to unintentional discrimination. Both sectors often rely on procuring third-party software and may not have the necessary capacity and capability to question providers over risks around bias and discrimination.\n",
      "There is scope for greater sharing and learning between these two sectors, and the wider public sector, around how to tackle these challenges, as well as considering adopting practices that have worked well in other places. For example, local authorities may want to look to certain police forces that have set up ethics committees as a way of providing external oversight of their data projects. Similarly, initiatives to develop integrated impact assessments, taking into account both data protection and equality legislation, may be applicable in both contexts.\n",
      "Tool development approaches\n",
      "Some local authorities have developed algorithmic decision-making tools in-house, others have tools procured from third-parties.\n",
      "A. In-house approaches\n",
      "Some local authorities have developed their own tools in-house, such as the Integrated Analytical Hub used by Bristol City Council. Bristol developed the hub in response to the Government’s Troubled Families programme, which provided financial incentives to local authorities who could successfully identify and support families at risk. [footnote 112] The hub brings together 35 datasets covering a wide range of topics including school attendance, crime statistics, children’s care data, domestic abuse records and health problem data such as adult involvement in alcohol and drug programmes. The datasets are then used to develop predictive modelling with targeted interventions then offered to families who are identified as most at risk. [footnote 113]\n",
      "One of the benefits of using in-house approaches is that they offer local authorities greater control over the data being used. They also require a fuller understanding of the organisation’s data quality and infrastructure, which is useful when monitoring the system. However, building tools in-house often require significant investment in internal expertise, which may not be feasible for many local authorities. They also carry significant risks if an in-house project ultimately does not work.\n",
      "B. Third-party software\n",
      "There is an increasing number of third-party providers offering predictive analytics and data analysis software to support decision-making. Software to support detecting fraudulent benefit claims which is reportedly used by around 70 local councils. [footnote 114]\n",
      "Other third-party providers offer predictive software that brings together different data sources and uses them to develop models to identify and target services. The use cases are varied and include identifying children at risk, adults requiring social care, or those at risk of homelessness. Software that helps with earlier interventions has the potential to bring costs down in the longer-term, however this relies on the tools being accurate and precise and so far there has been limited evaluation on the efficacy of these interventions.\n",
      "Third-party providers offer specialist data science expertise that is likely not available to most local authorities and are likely to have valuable experience from previous work with other local authorities. However there are also risks around the costs of procuring the technologies. Transparency and accountability are also particularly important when procuring third-party tools, because commercial sensitivities may prevent providers wanting to share information to explain how a model is developed. Local authorities have a responsibility to understand how decisions are made regardless of whether they are using a third-party or developing an in-house approach, and third-parties should not be seen as a way to outsource these complex decisions. Local authorities should also consider how they will manage risks around bias, that may be outside the scope of the provider’s service (see Section 9.3 for further discussion of public sector procurement and transparency).\n",
      "Local authorities struggle with data quality and data sharing when implementing data-driven tools\n",
      "There are multiple challenges local authorities face when introducing new data-driven technologies. Due to legacy systems, local authorities often struggle with maintaining their data infrastructure and developing standardised processes for data sharing. Many of the conversations we had with companies who partnered with local authorities found that the set-up phase took a lot longer than expected due to these challenges which led to costly delays and a need to reprioritise resources. Local authorities should be wary of introducing data-driven tools as a quick-fix, particularly in cases where data infrastructure requires significant investment. For many local authorities, investing in more basic data requirements is likely to reap higher rewards than introducing more advanced technologies at this stage.\n",
      "There is also an associated risk that legacy systems will have poor data quality. Poor data quality creates significant challenges because without a good quality, representative dataset, the algorithm will face the challenge of “rubbish in, rubbish out”, where poor quality training data results in a poor quality algorithm. One of the challenges identified by data scientists is that because data was being pulled from different sources, data scientists did not always have the necessary access to correct data errors. [footnote 115] As algorithms are only as good as their training data, interrogating the data quality of all data sources being used to develop a new predictive tool should be a top priority prior to procuring any new software.\n",
      "CDEI’s work on data sharing\n",
      "One of the challenges most frequently mentioned by local authorities wanting to explore the opportunities presented by data-driven technologies are concerns around data sharing. Often decision-support systems require bringing together different datasets, but physical barriers, such as poor infrastructure, and cultural barriers, such as insufficient knowledge of how and when to share data in line with data protection legislation, often mean that innovation is slow, even in cases where there are clear benefits.\n",
      "For example, we often hear that in children’s social services, social workers do not always have access to the data they need to assess whether a child is at risk. Whilst the data may be held within the local authority and there is a clear legal basis for social workers to have access, local authorities experience various challenges in facilitating sharing. For data sharing to be effective, there also needs to be consideration of how to share data whilst retaining trust between individuals and organisations. Our recent report on data sharing explores these challenges and potential solutions in more detail.\n",
      "National guidance is needed to govern the use of algorithms in the delivery of public services\n",
      "There is currently little guidance for local authorities wanting to use algorithms to assist decision-making. We found that whilst many local authorities are confident in understanding the data protection risks, they are less clear on how legislation such as the Equality Act 2010 and Human Rights Act 1998 should be applied. There is a risk that without understanding and applying these frameworks, some tools may be in breach of the law.\n",
      "The What Works Centre for Children’s Social Care recently commissioned a review of the ethics of machine learning approaches to children’s social care, conducted by the Alan Turing Institute and University of Oxford’s Rees Centre. They also found that national guidance should be a priority to ensure the ethical development and deployment of new data-driven approaches. [footnote 116] The review concludes that a “cautious, thoughtful and inclusive approach to using machine learning in children’s social care” is needed, but that this will only be facilitated through a series of recommendations, including nationally mandated standards. The research echoed what we have found in our work, that stakeholders felt strongly that national guidance was needed to protect vulnerable groups against the misuse of their data, including reducing the risk of unintentional biases.\n",
      "Whilst most research has looked at the need for guidance in children’s social care, similar challenges are likely to arise across a range of services within local government that make important decisions about individuals, such as housing, adult social care, education, public health. We therefore think that guidance should be applicable across a range of areas, recognising there are likely to be places where supplementary detailed guidance is necessary, particularly where regulatory frameworks differ.\n",
      "Taken together, there is a strong case for national guidelines setting out how to responsibly develop and introduce decision-supporting algorithms in local government. Government departments such as the Department for Education, the Ministry of Housing, Communities, and Local Government (MHCLG) and Department of Health and Social Care are best placed to support and coordinate the development of national guidance. The Local Government Association has also started a project bringing local authorities together to understand the challenges and opportunities with the aim of bringing this expertise together to develop guidance. National guidelines should look to build upon this work.\n",
      "Recommendations to government\n",
      "Recommendation 4 : Government should develop national guidance to support local authorities to legally and ethically procure or develop algorithmic decision-making tools in areas where significant decisions are made about individuals, and consider how compliance with this guidance should be monitored.\n",
      "Introducing data-driven technologies to save money may result in significant challenges\n",
      "Local authorities have a variety of motivations for introducing algorithmic tools, with many focused on wanting to improve decision-making. However, given the significant reduction in income over the last decade, there is a drive towards using technology to improve efficiencies in service delivery within local government.\n",
      "In their research exploring the uptake of AI across local government, the Oxford Internet Institute found that deploying tools with cost-saving as a primary motivation was unlikely to yield the results as expected. They state: “The case for many such projects is often built around the idea that they will save money. In the current climate of intense financial difficulty this is understandable. But we also believe this is fundamentally the wrong way to conceive data science in a government context: many useful projects will not, in the short term at least, save money.” [footnote 117] The focus of predictive tools is often grounded in the idea of early intervention. If there is a way to identify someone who is at risk and put assistive measures in place early, then the situation is managed prior to escalation, thus reducing overall resources. This longer-term way of thinking may result in less demand overall, however in the short-term it is likely to lead to increased workload and investment in preventative services.\n",
      "There is a challenging ethical issue around the follow-up required once someone is identified. We heard examples of local authorities who held off adopting new tools because it would cost too much to follow up on the intelligence provided. Due to the duty of care placed on local authorities, there is also a concern that staff may be blamed for not following up leads if a case later develops. Therefore councils need to carefully plan how they will deploy resources in response to a potential increase in demands for services and should be wary of viewing these tools as a silver bullet for solving resourcing needs.\n",
      "There should be greater support for sharing lessons, best practice and joint working between local authorities\n",
      "Local authorities often experience similar challenges, but the networks to share lessons learned are often ad hoc and informal and rely on local authorities knowing which authorities have used similar tools. The Local Government Association’s work has started bringing this knowledge and experience together which is an important first step. There should also be opportunities for central government to learn from the work undertaken within local government, so as not to miss out on the innovation taking place and the lessons learned from challenges that are similar in both sectors.\n",
      "The Local Digital Collaboration Unit within the Ministry of Housing, Communities and Local Government has also been set up to provide support and training to local authorities undertaking digital innovation projects. The Local Digital Collaboration Unit oversees the Local Digital Fund that provides financial support for digital innovation projects in local government. Greater support for this fund, particularly for projects looking at case studies for identifying and mitigating bias in local government algorithms, evaluating the effectiveness of algorithmic tools, public engagement and sharing best practice, would all add significant value. Our research found local authorities thought this fund was a very helpful initiative, however felt that greater investment would improve access to the benefits and be more cost effective over the long term.\n",
      "Part III: Addressing the challenges\n",
      "In Part I we surveyed the issue of bias in algorithmic decision-making, and in Part II we studied the current state in more detail across four sectors. Here, we move on to identify how some of the challenges we identified can be addressed, the progress made so far, and what needs to happen next.\n",
      "There are three main areas to consider:\n",
      "The enablers needed by organisations building and deploying algorithmic decision-making tools to help them do so in a fair way (see Chapter 7).\n",
      "The regulatory levers, both formal and informal, needed to incentivise organisations to do this, and create a level playing field for ethical innovation (see Chapter 8).\n",
      "How the public sector , as a major developer and user of data-driven technology, can show leadership through transparency (see Chapter 9).\n",
      "There are inherent links between these areas. Creating the right incentives can only succeed if the right enablers are in place to help organisations act fairly, but conversely there is little incentive for organisations to invest in tools and approaches for fair decision-making if there is insufficient clarity on the expected norms.\n",
      "Lots of good work is happening to try to make decision-making fair, but there remains a long way to go. We see the status quo as follows:\n",
      "Impact on bias: Algorithms could help to address bias but Building algorithms that replicate existing biased mechanisms will embed or even exacerbate existing inequalities. Measurement of bias: More data available than ever before to help organisations understand the impacts of decision-making. but Collection of protected characteristic data is very patchy, with significant perceived uncertainty about ethics, legality, and the willingness of individuals to provide data (see Section 7.3). Uncertainties concerning the legality and ethics of inferring protected characteristics. Most decision processes (whether using algorithms or not) exhibit bias in some form and will fail certain tests of fairness. The law offers limited guidance to organisations on adequate ways to address this. Mitigating bias: Lots of academic study and open source tooling available to support bias mitigation. but Relatively limited understanding of how to use these tools in practice to support fair, end-to-end, decision-making. A US-centric ecosystem where many tools do not align with UK equality law. Uncertainty about usage of tools, and issues on legality of some approaches under UK law. Perceived trade-offs with accuracy (though often this may suggest an incomplete notion of accuracy) (see Section 7.4). Expert support: A range of consultancy services are available to help with these issues. but An immature ecosystem, with no clear industry norms around these services, the relevant professional skills, or important legal clarity (see Section 8.5). Workforce diversity: Strong stated commitment from government and industry to improving diversity. but Still far too little diversity in the tech sector (see Section 7.2). Leadership and governance: Many organisations understand the strategic drivers to act fairly and proactively in complying with data protection obligations and anticipating ethical risks. but Recent focus on data protection (due to the arrival of GDPR), and especially privacy and security aspects of this, risks de-prioritisation of fairness and equality issues (even though these are also required in GDPR). Identifying historical or current bias in decision-making is not a comfortable thing for organisations to do. There is a risk that public opinion will penalise those who proactively identify and address bias. Governance needs to be more than compliance with current regulations; it needs to consider the possible wider implications of the introduction of algorithms, and anticipate future ethical problems that may emerge (see Section 7.5). Transparency: Transparency about the use and impact of algorithmic decision-making would help to drive greater consistency. but There are insufficient incentives for organisations to be more transparent and risks to going alone. There is a danger of creating requirements that create public perception risks for organisations even if they would help reduce risks of biased decision-making. The UK public sector has identified this issue, but could do more to lead through its own development and use of algorithmic decision-making (see Chapter 9). Regulation: Good regulation can support ethical innovation. but Not all regulators are currently equipped to deal with the challenges posed by algorithms. There is continued nervousness in industry around the implications of GDPR. The ICO has worked hard to address this, and recent guidance will help, but there remains a way to go to build confidence on how to interpret GDPR in this context (see Chapter 8).\n",
      "Governance is a key theme throughout this part of the review; how should organisations and regulators ensure that risks of bias are being anticipated and managed effectively? This is not trivial to get right, but there is clear scope for organisations to do better in considering potential impacts of algorithmic decision-making tools, and anticipating risks in advance.\n",
      "The terms anticipatory governance and anticipatory regulation are sometimes used to describe this approach; though arguably anticipatory governance or regulation is simply part of any good governance or regulation. In Chapter 7 we consider how organisations should approach this, in Chapter 8 the role of regulators and the law in doing so, and in Chapter 9 how a habit of increased transparency in the public sector’s use of such tools could encourage this.\n",
      "7. Enabling fair innovation\n",
      "Many organisations are unsure how to address bias in practice. Support is needed to help them consider, measure, and mitigate unfairness.\n",
      "Improving diversity across a range of roles involved in technology development is an important part of protecting against certain forms of bias. Government and industry efforts to improve this must continue, and need to show results.\n",
      "Data is needed to monitor outcomes and identify bias, but data on protected characteristics is not available often enough. One cause is an incorrect belief that data protection law prevents collection or usage; but there are a number of lawful bases in data protection legislation for using protected or special characteristic data for monitoring or addressing discrimination. There are some other genuine challenges in collecting this data, and more innovative thinking is needed in this area; for example, the potential for trusted third party intermediaries.\n",
      "The machine learning community has developed multiple techniques to measure and mitigate algorithmic bias. Organisations should be encouraged to deploy methods that address bias and discrimination. However, there is little guidance on how to choose the right methods, or how to embed them into development and operational processes. Bias mitigation cannot be treated as a purely technical issue ; it requires careful consideration of the wider policy, operational and legal context. There is insufficient legal clarity concerning novel techniques in this area; some may not be compatible with equality law.\n",
      "Recommendations to government:\n",
      "Recommendation 5 : Government should continue to support and invest in programmes that facilitate greater diversity within the technology sector, building on its current programmes and developing new initiatives where there are gaps.\n",
      "Recommendation 6 : Government should work with relevant regulators to provide clear guidance on the collection and use of protected characteristic data in outcome monitoring and decision-making processes. They should then encourage the use of that guidance and data to address current and historic bias in key sectors.\n",
      "Recommendation 7 : Government and the ONS should open the Secure Research Service more broadly, to a wider variety of organisations, for use in evaluation of bias and inequality across a greater range of activities.\n",
      "Recommendation 8 : Government should support the creation and development of data-focused public and private partnerships, especially those focused on the identification and reduction of biases and issues specific to under-represented groups. The ONS and Government Statistical Service should work with these partnerships and regulators to promote harmonised principles of data collection and use into the private sector, via shared data and standards development.\n",
      "Recommendations to regulators: Recommendation 9 : Sector regulators and industry bodies should help create oversight and technical guidance for responsible bias detection and mitigation in their individual sectors, adding context-specific detail to the existing cross-cutting guidance on data protection, and any new cross-cutting guidance on the Equality Act.\n",
      "Advice to industry:\n",
      "Organisations building and deploying algorithmic decision-making tools should make increased diversity in their workforce a priority. This applies not just to data science roles, but also to wider operational, management and oversight roles. Proactive gathering and use of data in the industry is required to identify and challenge barriers for increased diversity in recruitment and progression, including into senior leadership roles.\n",
      "Where organisations operating within the UK deploy bias detection or mitigation tools developed in the US, they must be mindful that relevant equality law (along with that across much of Europe) is different.\n",
      "Where organisations face historical issues, attract significant societal concern, or otherwise believe bias is a risk, they will need to measure outcomes by relevant protected characteristics to detect biases in their decision-making, algorithmic or otherwise. They must then address any uncovered direct discrimination, indirect discrimination, or outcome differences by protected characteristics that lack objective justification.\n",
      "In doing so, organisations should ensure that their mitigation efforts do not produce new forms of bias or discrimination. Many bias mitigation techniques, especially those focused on representation and inclusion, can legitimately and lawfully address algorithmic bias when used responsibly. However, some risk introducing positive discrimination, which is illegal under the Equality Act. Organisations should consider the legal implications of their mitigation tools, drawing on industry guidance and legal advice.\n",
      "Guidance to organisation leaders and boards:\n",
      "Understanding the capabilities and limits of those tools.\n",
      "Considering carefully whether individuals will be fairly treated by the decision-making process that the tool forms part of.\n",
      "Making a conscious decision on appropriate levels of human involvement in the decision-making process.\n",
      "Putting structures in place to gather data and monitor outcomes for fairness.\n",
      "Understanding their legal obligations and having carried out appropriate impact assessments.\n",
      "This especially applies in the public sector when citizens often do not have a choice about whether to use a service, and decisions made about individuals can often be life-affecting.\n",
      "7.1 Introduction\n",
      "There is clear evidence, both from wider public commentary and our research, that many organisations are aware of potential bias issues and are keen to take steps to address them.\n",
      "However, the picture is variable across different sectors and organisations, and many do not feel that they have the right enablers in place to take action. Some organisations are uncertain of how they should approach issues of fairness, including associated reputational, legal and commercial issues. To improve fairness in decision-making, it needs to be as easy as possible for organisations to identify and address bias. A number of factors are required to help build algorithmic decision-making tools and machine learning models with fairness in mind:\n",
      "Sufficient diversity in the workforce to understand potential issues of bias and the problems they cause.\n",
      "Availability of the right data to understand bias in data and models.\n",
      "Access to the right tools and approaches to help identify and mitigate bias.\n",
      "An ecosystem of expert individuals and organisations able to support them.\n",
      "Governance structures that anticipate risks, and build in opportunities to consider the wider impact of an algorithmic tool with those affected.\n",
      "Confidence that efforts to behave ethically (by challenging bias) and lawfully (by eliminating discrimination) will attract the support of organisational leadership and the relevant regulatory bodies.\n",
      "Some of these strategies can only be achieved by individual organisations, but the wider ecosystem needs to enable them to act in a way that is both effective and commercially viable.\n",
      "It is always better to acknowledge biases, understand underlying causes, and address them as far as possible, but the ‘correct’ approach for ensuring fairness in an algorithmic decision-making tool will depend strongly on use case and context. The real-world notion of what is considered ‘fair’ is as much a legal, ethical or philosophical idea as a mathematical one, which can never be as holistic, or as applicable across cases. What good practice should a team then follow when seeking to ensure fairness in an algorithmic decision-making tool? We investigate the issue further in this chapter.\n",
      "7.2 Workforce diversity\n",
      "There is increasing recognition that it is not algorithms that cause bias alone, but rather that technology may encode and amplify human biases. One of the strongest themes in responses to our Call for Evidence , and our wider research and engagement, was the need to have a diverse technology workforce; better able to interrogate biases that may arise throughout the process of developing, deploying and operating an algorithmic decision-making tool. By having more diverse teams, biases are more likely to be identified and less likely to be replicated in these systems.\n",
      "There is a lot to do to make the technology sector more diverse. A report from Tech Nation found that only 19% of tech workers are women. [footnote 117] What is perhaps more worrying is how little this has changed over the last 10 years, compared with sectors such as engineering, which have seen a significant increase in the proportion of women becoming engineers. This gender gap is similarly represented at senior levels of tech companies.\n",
      "Although the representation of people with BAME backgrounds is proportionate to the UK population (15%), when this is broken down by ethnicity we see that Black people are underrepresented by some margin. It should be a priority to improve this representation. Organisations should also undertake research to understand how ethnicity intersects with other characteristics, as well as whether this representation is mirrored at more senior levels. [footnote 118]\n",
      "There is less data on other forms of diversity, which has spurred calls for greater focus on disability inclusion within the tech sector. [footnote 119] Similarly, more work needs to be done in terms of age, socio-economic background, and geographic spread across the UK. It is important to note that the technology sector is doing well in some areas. For example, the tech workforce is much more international than many others. [footnote 120] The workforce relevant to algorithmic decision-making is, of course, not limited to technology professionals; a diverse range of skills is necessary within teams and organisations to properly experience the benefits of diversity and equality. Beyond training and recruitment, technology companies need to support workers by building inclusive workplaces, which are key to retaining, as well as attracting, talented staff from different backgrounds.\n",
      "There are a lot of activities aimed at improving the current landscape. The Government is providing financial support to a variety of initiatives including the Tech Talent Charter [footnote 121] , founded by a group of organisations wanting to work together to create meaningful change for diversity in tech. Currently, the charter has around 500 signatories ranging from small start-ups to big businesses and is intending to grow to 600 by the end of 2020. In 2018 the Government also launched a £1 million “Digital Skills Innovation Fund”, specifically for helping underrepresented groups develop skills to move into digital jobs. The Government’s Office for AI and AI Council is conducting a wide range of work in this area, including helping to drive diversity in the tech workforce, as well as recently securing £10 million in funding for students from underrepresented backgrounds to study AI related courses. [footnote 122]\n",
      "Recommendations to government\n",
      "Recommendation 5 : Government should continue to support and invest in programmes that facilitate greater diversity within the technology sector, building on its current programmes and developing new initiatives where there are gaps.\n",
      "There are also a huge number of industry initiatives and nonprofits aimed at encouraging and supporting underrepresented groups in the technology sector. [footnote 123] They are wide-ranging in both their approaches and the people they are supporting. These efforts are already helping to raise the profile of tech’s diversity problem, as well as supporting people who want to either move into the tech sector or develop further within it. The more government and industry can do to support this work the better.\n",
      "Advice to industry\n",
      "Organisations building and deploying algorithmic decision-making tools should make increased diversity in their workforce a priority. This applies not just to data science roles, but also to wider operational, management and oversight roles. Proactive gathering and use of data in the industry is required to identify and challenge barriers for increased diversity in recruitment and progression, including into senior leadership roles.\n",
      "Given the increasing momentum around a wide-range of initiatives springing up both within government and from grassroots campaigns, we hope to soon see a measurable improvement in data on diversity in tech.\n",
      "7.3 Protected characteristic data and monitoring outcomes\n",
      "The issue\n",
      "A key part of understanding whether a decision-making process is achieving fair outcomes is measurement. Organisations may need to compare outcomes across different demographic groups to assess whether they match expectations. To do this, organisations must have some data on the demographic characteristics of groups they are making decisions about. In recruitment, especially in the public sector, the collection of some ‘protected characteristic’ data (defined under the Equality Act, 2010) for monitoring purposes has become common-place, but this is less common in other sectors.\n",
      "Removing or not collecting protected characteristic data does not by itself ensure fair data-driven outcomes. Although this removes the possibility of direct discrimination, it may make it impossible to evaluate whether indirect discrimination is taking place. This highlights an important tension: to avoid direct discrimination as part of the decision-making process, protected characteristic attributes should not be considered by an algorithm. But, in order to assess the overall outcome (and hence assess the risk of indirect discrimination), data on protected characteristics is required. [footnote 124]\n",
      "There have been calls for wider data collection, reflecting an acceptance that doing so helps promote fairness and equality in areas where bias could occur. [footnote 125] CDEI supports these calls; we think that greater collection of protected characteristic data would allow for fairer algorithmic decision-making in many circumstances . In this section we explore why that is, and the issues that need to be overcome to make this happen more often.\n",
      "The need to monitor outcomes is important even when no algorithm is involved in a decision, but the introduction of algorithms makes this more pressing. Machine learning detects patterns and can find relationships in data that humans may not see or be able to fully understand. Although machine learning models optimise against objectives they have been given by a human, if data being analysed reflects historical or subconscious bias, then imposed blindness will not prevent models from finding other, perhaps more obscure, relationships. These could then lead to similarly biased outcomes, encoding them into future decisions in a repeatable way. This is therefore the right time to investigate organisational biases and take the actions required to address them.\n",
      "There are a number of reasons why organisations are not currently collecting protected characteristic data, including concerns or perceptions that:\n",
      "Collecting protected characteristic data is not permitted by data protection law. This is incorrect in the UK, but seems to be a common perception (see below for further discussion).\n",
      "It may be difficult to justify collection in data protection law, and then store and use that data in an appropriate way (i.e. separate to the main decision-making process).\n",
      "Service users and customers will not want to share the data, and may be concerned about why they are being asked for it. Our own survey work suggests that this is not necessarily true for recruitment [footnote 126] , although it may be elsewhere. Data could provide an evidence base that organisational outcomes were biased; whether in a new algorithmic decision-making process, or historically.\n",
      "In this section, we consider what is needed to overcome these barriers, so that organisations in the public and private sector can collect and use data more often, in a responsible way. Not all organisations will need to collect or use protected characteristic data. Services may not require it, or an assessment may find that its inclusion does more harm than good. However, many more should engage in collection than do so at present.\n",
      "Data protection concerns\n",
      "Our research suggests a degree of confusion about how data protection law affects the collection, retention and use of protected characteristic data.\n",
      "Data protection law sets additional conditions for processing special category data. This includes many of the protected characteristics in the Equality Act 2010 (discussed in this chapter), as well as other forms of sensitive data that are not currently protected characteristics, such as biometric data.\n",
      "There are overlaps between the protected characteristics covered by the Equality Act, and the special categories of personal data under data protection law. Protected characteristics solely covered under the Equality Act are “sex”, “marriage & civil partnership” and “age”. Special categories of personal data under Data Protection law include “genetic data”, “biometric data (where used for identification”, “health data”, “sex life”, “political opinion”, “trade union membership” and “criminal convictions”. The overlapping characteristics are: “race, colour, ethnic origin & nationality”, “religion or philosophical belief”, “sexual orientation”, “disability”, “pregnancy & maternity” and “gender reassignment.”\n",
      "Figure 4: Overlap between the Protected Characteristics of equality law and Special Categories of personal data under data protection law\n",
      "Several organisations we spoke to believed that data protection requirements prevent the collection, processing or use of special category data to test for algorithmic bias and discrimination. This is not the case: data protection law sets out specific conditions and safeguards for the processing of special category data, but explicitly includes use for monitoring equality.\n",
      "The collection, processing and use of special category data is allowed if it is for “substantial public interest”, among other specific purposes set out in data protection law. In Schedule 1, the Data Protection Act sets out specific public interest conditions that meet this requirement, including ‘equality of opportunity or treatment’ where the Act allows processing of special category data where it “is necessary for the purposes of identifying or keeping under review the existence or absence of equality of opportunity or treatment between groups of people specified in relation to that category with a view to enabling such equality to be promoted or maintained,” (Schedule 1, 8.1(b)). Notably, this provision also specifically mentions equality rather than discrimination, which allows for this data to address broader fairness and equality considerations rather than just discrimination as defined by equality or human rights law.\n",
      "However, this provision of the Data Protection Act clarifies that it does not allow for using special category data for individual decisions (Schedule 1, 8.3), or if it causes substantial damage or distress to an individual (Schedule 1, 8.4). In addition to collection, data retention also needs some thought. Organisations may want to monitor outcomes for historically disadvantaged groups over time, which would require longer data retention periods than needed for individual use cases. This may lead to a tension between monitoring equality and data protection in practice, but these restrictions are much less onerous than sometimes described by organisations. The recently published ICO guidance on AI and data protection [footnote 127] sets out some approaches to assessing these issues, including guidance on special category data.\n",
      "Section 8.3 of this report sets out further details of how equality and data protection law apply to algorithmic decision-making.\n",
      "The need for guidance\n",
      "As with diversity monitoring in recruitment, pockets of the public sector are increasingly viewing the collection of data on protected characteristics as essential to the monitoring of unintentional discrimination in their services. The Open Data Institute recently explored how public sector organisations should consider collecting protected characteristic data to help fulfil their responsibilities under the Public Sector Equality Duty [footnote 128] . They recognise that:\n",
      "there is no accepted practice for collecting and publishing data about who uses digital services, which makes it hard to tell whether they discriminate or not. [footnote 129]\n",
      "The EHRC provides guidance [footnote 130] on how to deal with data protection issues when collecting data in support of obligations in the Public Sector Equality Duty, but is yet to update it for the significant changes to data protection law through GDPR and the Data Protection Act 2018, or to consider the implications of algorithmic decision-making on data collection. This needs to be addressed. There should also be consistent guidance for public sector organisations wanting to collect protected characteristic data specifically for equality monitoring purposes, which should become standard practice. Such practice is essential for testing algorithmic discrimination against protected groups. Organisations need to be assured that by following guidance, they are not just making their systems more fair and reducing their legal risk, but also minimising any unintended consequences of personal data collection and use, and thus helping to maintain public trust.\n",
      "The picture is more complicated in the private sector because organisations do not have the same legal responsibility under the Public Sector Equality Duty [footnote 131] . Equalities law requires that all organisations avoid discrimination, but there is little guidance on how they should practically identify it in algorithmic contexts. Without guidance or the PSED, private sector organisations have to manage different expectations from customers, employees, investors and the public about how to measure and manage the risks of algorithmic bias.\n",
      "There are also concerns about balancing the trade-off between fairness and privacy. In our interviews with financial institutions, many focused on principles such as data minimisation within data protection legislation. In some cases it was felt that collecting this data at all may be inappropriate, even if the data does not touch upon decision-making tools and models. In insurance, for example, there are concerns around public trust in whether providing this information could affect an individual’s insurance premium. Organisations should think carefully about how to introduce processes that secure trust, such as being as transparent as possible about the data being collected, why and how it is used and stored, and how people can access and control their data. Building public trust is difficult, especially when looking to assess historic practices which may hide potential liabilities. Organisations may fear that by collecting data, they identify and expose patterns of historic bad practice. However, data provides a key means of addressing issues of bias and discrimination, and therefore reducing risk in the long term.\n",
      "Although public services normally sit within a single national jurisdiction, private organisations may be international. Different jurisdictions have different requirements for the collection of protected characteristic data, which may even be prohibited. The French Constitutional Council, for example, prohibits data collection or processing regarding race or religion. International organisations may need help to satisfy UK specific or nationally devolved regulation.\n",
      "There will be exceptions to the general principle that collection of protected or special characteristic data is a good thing. In cases where action is not needed, or urgently required, due to context or entirely obvious pre-existing biases, collecting protected characteristic data will be unnecessary. In others it may be seen as disproportionately difficult to gather the relevant data to identify bias. In others still, it may be impossible to provide privacy for very small groups, where only a very small number of service users or customers have a particular characteristic. Overcoming and navigating such barriers and concerns will require a combination of effective guidance, strong promotion of new norms from a centralised authority, or even regulatory compulsion.\n",
      "Recommendations to government\n",
      "Recommendation 6 : Government should work with relevant regulators to provide clear guidance on the collection and use of protected characteristic data in outcome monitoring and decision-making processes. They should then encourage the use of that guidance and data to address current and historic bias in key sectors.\n",
      "Alternative approaches\n",
      "Guidance is a first step, but more innovative thinking may be needed on new models for collecting, protecting or inferring protected characteristic data.\n",
      "Such models include a safe public third-party, collecting protected characteristic data on behalf of organisations, and securely testing their algorithms and decision-making processes without ever providing data to companies themselves. [footnote 132] This could be a responsibility of the relevant sector regulator or a government organisation such as the Office for National Statistics. There are also models where a private sector company could collect and store data securely, offering individuals guarantees on privacy and purpose, but then carrying out testing on behalf of other companies as a third party service.\n",
      "Where organisations do not collect protected characteristic data explicitly, they can sometimes infer it from other data; for example by extracting the likely ethnicity of an individual from their name and postcode. If used within an actual decision-making process, such proxies present some of the key bias risks, and using this information in relation to any individual presents substantial issues for transparency, accuracy, appropriateness and agency. In cases where collecting protected characteristic data is unfeasible, identifying proxies for protected characteristics purely for monitoring purposes may be a better option than keeping processes blind. However, there are clear risks around the potential for this type of monitoring to undermine trust, so organisations need to think carefully about how to proceed ethically, legally and responsibly. Inferred personal data (under data protection law) is still, legally, personal data, and thus subject to the relevant laws and issues described above. [footnote 133] A right to reasonable inference is under current academic discussion. [footnote 134]\n",
      "Further development is needed around all of these concepts, and few models exist of how they would work in practice. As above, legal clarity is a necessary first step, followed by economic viability, technical capability, security, and public trust. However, there are some models of success to work from, such as the ONS Secure Research Service, described below, and the NHS Data Safe Havens [footnote 135] , as well as ongoing research projects in the field [footnote 136] . If a successful model could be developed, private sector companies would be able to audit their algorithms for bias without individuals being required to hand over their sensitive data to multiple organisations. We believe further research is needed to develop a longer-term proposal for the role of third-parties in such auditing , and will consider future CDEI work in this area.\n",
      "Access to baseline data\n",
      "Where organisations determine that collecting protected characteristics is appropriate for assessing bias, they will often need to collect information about their service users or customers, and compare it with relevant wider (often national) demographic data. It is hard to tell if a decision is having a negative effect on a group without some sense of what should be considered normal. A lack of relevant and representative wider data can make it difficult for both public and private organisations to tell if their processes are biased, and then to develop responsible algorithmic tools in response.\n",
      "Relevant data is already made available publicly, including UK Census and survey data published by the ONS. The devolved administrations have also made significant volumes of data widely accessible (through StatsWales , Statistics.gov.scot , and NISRA ), as have a number of Government departments and programmes . Sector specific datasets and portals add to this landscape in policing , finance , and others.\n",
      "More detailed population data can be accessed through the ONS’ Secure Research Service which provides a wide variety of national scale information, including pre-existing survey and administrative data resources.\n",
      "Usage of this service is managed through its “ 5 Safes ” (safe projects, people, settings, data and outputs) framework, and restricted to the purposes of research, evaluation and analysis. This often restricts access to academic research groups, but there may be opportunities to widen the service to support evaluation of diversity outcomes by regulators and delivery organisations.\n",
      "Regulators can help by promoting key public datasets of specific value to their sector, along with guidance material accessible for their industry. Wider availability of aggregate demographic information for business use would also allow for better data gathering, or better synthetic data generation. Publicly available, synthetically augmented, and plausible versions of more surveys (beyond the Labour Force Survey [footnote 137] ) would help more users find and develop use cases.\n",
      "Government announcements in 2020 included £6.8 million (over three years) to help the ONS share more, higher-quality data across government, and to link and combine datasets in new ways (for example, to inform policy or evaluate interventions).\n",
      "Recommendations to government\n",
      "Recommendation 7 : Government and the ONS should open the Secure Research Service more broadly, to a wider variety of organisations, for use in evaluation of bias and inequality across a greater range of activities.\n",
      "In the short term, organisations who find publicly held data insufficient will need to engage in partnership with their peers, or bodies that hold additional representative or demographic information, to create new resources. In the private sphere these approaches include industry specific data sharing initiatives (Open Banking in finance, Presumed Open in energy, and more under discussion by the Better Regulation Executive), and trusted sector-specific data intermediaries.\n",
      "Recommendations to government\n",
      "Recommendation 8 : Government should support the creation and development of data-focused public and private partnerships, especially those focused on the identification and reduction of biases and issues specific to under-represented groups. The Office for National Statistics and Government Statistical Service should work with these partnerships and regulators to promote harmonised principles of data collection and use into the private sector, via shared data and standards development.\n",
      "Case Study: Open Banking\n",
      "In 2016 the Competition and Markets Authority (CMA) intervened in the UK ecosystem to require that nine of the largest UK banks grant direct, transaction level, data access to licensed startups. Although compliance and enforcement sits with the CMA, Open Banking represents a regulatory partnership as much as a data partnership, with the FCA providing financial oversight, and the ICO providing data protection. Open Banking has led to over 200 regulated companies providing new services, including financial management and credit scoring. As a result access to credit, debt advice and financial advice is likely to widen, which in turn is expected to allow for better service provision for under-represented groups. This provides an opportunity to address unfairness and systemic biases, but new forms of (digital) exclusion and bias may yet appear. [footnote 138]\n",
      "Examples of wider partnerships include projects within the Administrative Data Research UK programme (bringing together government, academia and public bodies) and the increasing number of developmental sandboxes aimed at industry or government support (see Section 8.5). Where new data ecosystems are created around service-user data, organisations like the new Global Open Finance Centre of Excellence can then provide coordination and research support. Empowering organisations to share their own data with trusted bodies will enable industry wide implementation of simple but specific common data regimes. Relatively quick wins are achievable in sectors that have open data standards in active development such as Open Banking and Open Energy.\n",
      "Case Study: Monitoring for bias in digital transformation of the courts\n",
      "Accessing protected characteristic data to monitor outcomes is not only necessary when introducing algorithmic decision-making, but also when making other major changes to significant decision-making processes.\n",
      "Her Majesty’s Courts and Tribunal Service (HMCTS) is currently undergoing a large-scale digital transformation process [footnote 139] aimed largely at making the court system more affordable and fair, including online dispute resolution and opt-in automated fixed penalties for minor offences where there is a guilty plea. [footnote 140] As part of this transformation, they have recognised a need for more information about people entering and exiting the judicial process. [footnote 141] More protected characteristic data would allow HMCTS to assess the effectiveness of different interventions and the level of dependency on, and uptake of, different parts of the judicial system within different groups. Senior justices would largely prefer to see a general reduction in the number of people going through criminal courts and greater diversity in use of civil courts. It is hard to objectively measure these outcomes, or whether courts are acting fairly and without bias, without data.\n",
      "In order to achieve these goals, HMCTS have focused on access to protected characteristic data, predominantly through data linkage and inference from wider administrative data. They have worked with the Government Statistical Service’s Harmonisation Team and academic researchers to rebuild their data architecture to support this. [footnote 142] The resulting information is intended to both be valuable to the Ministry of Justice for designing fair interventions in the functioning of the courts, but also eventually to be made available for independent academic research (via Administrative Data Research UK and the Office for National Statistics).\n",
      "This is just one example of a drive toward new forms of data collection, designed to test and assure fair processes and services within public bodies. It is also illustrative of a project navigating the Data Protection Act 2018 and the ‘substantial public interest’ provision of the GDPR to assess risks around legal exposure. It is essential for public bodies to establish whether or not their digital services involve personal data, are classed as statistical research, or sit within other legislative ‘carve-outs’. This is especially true when dealing with data that is not necessarily accompanied by end-user consent.\n",
      "House of Commons Justice Committee, ‘Court and Tribunal reforms, Second Report of Session 2019’   ↩\n",
      "Lord Chancellor; Lord Chief Justice; Senior President of Tribunals, ‘Transforming Our Justice System’ , 2016  ↩\n",
      "7.4 Detecting and mitigating bias\n",
      "In the previous section we argue that it is preferable to seek to identify bias and to address it, rather than hope to avoid it by unawareness. There is a high level of focus on this area in the academic literature, and an increasing number of practical algorithmic fairness tools have appeared in the last three years.\n",
      "Approaches for detecting bias include:\n",
      "Comparing training with population datasets to see if they are representative.\n",
      "Analysing the drivers of differences in outcomes that are likely to cause bias. For example, if it could be shown that certain recruiters in an organisation held measurable biases compared to other recruiters (after controlling for other characteristics), it would be possible to train an algorithm with a less biased subset of the data (e.g. by excluding the biased group).\n",
      "Analysing how and where relevant model variables correlate with different groups. For example, if qualifications are a factor in a model for recommending recruitment, analysis can show the extent to which this results in more job offers being made to particular groups.\n",
      "Different approaches are necessary in different contexts. For most organisations, bias monitoring and analysis are a necessary part of their decision-making (whether algorithmic or not). Where that monitoring suggests a biased process, the question is then how to address it. Ensuring that the data being collected (Section 7.3) is both necessary and sufficient is an important first step. Further methods (detailed below) will need to be proportionate to organisational needs.\n",
      "Case study: Bias detection in 1988\n",
      "The 1988 medical school case mentioned in Section 2.1 is an interesting example of bias detection. Their program was developed to match human admissions decisions, doing so with 90-95% accuracy. Despite bias against them, the school still had a higher proportion of non-European students admitted than most other London medical schools. The human admissions officers’ biases would probably never have been demonstrated, but for the use of their program. Had that medical school been equipped with a current understanding of how to assess an algorithm for bias, and been motivated to do so, perhaps they would have been able to use their algorithm to reduce bias rather than propagate it.\n",
      "Organisations that need to directly mitigate bias in their models now have a number of interventions at their disposal. This is a generally positive development but the ecosystem is complex. Organisations see a need for clarity on which mitigation tools and techniques are appropriate and legal in which circumstances. Crucially, what is missing is practical guidance about how to create, deploy, monitor, audit, and adjust fairer algorithms, using the most effective tools and techniques available. It is important to recognise that the growing literature and toolsets on algorithmic fairness often only address part of the issue (that which can be quantified), and wider interventions to promote fairness and equality remain key to success.\n",
      "As part of this review, we contracted [footnote 143] Faculty to analyse, assess and compare the various technical approaches to bias mitigation. This section is informed by their technical work. The outputs from that work are being made available elsewhere .\n",
      "Statistical definitions of fairness\n",
      "If we want model development to include a definition of fairness, we must tell the relevant model what that definition is, and then measure it. There is, however, no single mathematical definition of fairness that can apply to all contexts [footnote 144] . As a result, the academic literature has seen dozens of competing notions of fairness introduced, each with their own merits and drawbacks, and many different terminologies for categorising these notions, none of which are complete. Ultimately, humans must choose which notions of fairness an algorithm will work to, taking wider notions and considerations into account, and recognising that there will always be aspects of fairness outside of any statistical definition.\n",
      "Fairness definitions can be grouped by notion of fairness sought and stage of development involved. In the first instance, these fall into the broad categories of procedural and outcome fairness discussed in Section 2.5. Within the technical aspects of machine learning, procedural fairness approaches often concern the information used by a system, and thus include “Fairness Through Unawareness”, which is rarely an effective strategy. The statistical concept of fairness as applied to algorithms is then focused on achieving unbiased outcomes, rather than other concepts of fairness. Explicit measurement of equality across results for different groups is necessary for most of these approaches.\n",
      "Within Outcome Fairness we can make additional distinctions, between Causal and Observational notions of fairness, as well as Individual and Group notions.\n",
      "Individual notions compare outcomes for individuals to see if they are treated differently. However, circumstances are generally highly specific to individuals, making them difficult to compare without common features.\n",
      "Group notions aggregate individual outcomes by a common feature into a group, then compare aggregated outcomes to each other.\n",
      "Group and individual notions are not mutually exclusive: an idealised ‘fair’ algorithm could achieve both simultaneously.\n",
      "Observational approaches then deal entirely with the measurable facts of a system, whether outcomes, decisions, data, mathematical definitions, or types of model.\n",
      "Causal approaches can consider ‘what if?’ effects of different choices or interventions. This typically requires a deeper understanding of the real-world system that the algorithm interacts with.\n",
      "In their technical review of this area, Faculty describe a way to categorise different bias mitigation strategies within these notions of fairness (see table below). They also identified that the four Group Observational notions (highlighted) are currently the most practical approaches to implement for developers: being relatively easy to compute and providing meaningful measures for simple differences between groups (this does not necessarily mean they are an appropriate choice of fairness definition in all contexts). The majority of existing bias mitigation tools available to developers address (Conditional) Demographic Parity, or Equalised Odds, or are focused on removing sensitive attributes from data. The table below shows how the most commonly used approaches (and other examples) sit within wider definitions as described above. Visual demonstrations of these notions are shown in a web app that accompanies this review. [footnote 145]\n",
      "  Observational Causal Group Demographic Parity (‘Independence’) Conditional Demographic Parity Equalised Odds (‘Separation’) Calibration (‘Sufficiency’) Sub-Group Fairness Unresolved Discrimination Proxy Discrimination Individual Individual Fairness Meritocratic Fairness Counterfactual Fairness * Demographic Parity - outcomes for different protected groups are equally distributed, and statistically independent. Members of one group are as likely to achieve a given outcome as those in a different group, and successes in one group do not imply successes (or failures) in another. At a decision level, Demographic Parity might mean that the same proportion of men and women applying for loans or insurance are successful, but this kind of fairness can also be applied when assigning risk scores, regardless of where a success threshold is applied. * Conditional Demographic Parity - as above, but “legitimate risk factors” might mean that we consider it fair to discriminate for certain groups, such as by age in car insurance. The difficulty then sits in deciding which factors qualify as legitimate, and which may be perpetuating historical biases. * Equalised Odds (separation) - qualified and unqualified candidates are treated the same, regardless of their protected attributes. True positive rates are the same for all protected groups, as are false positive rates: the chance that a qualified individual is overlooked, or that an unqualified individual is approved, is the same across all protected groups. However, if different groups have different rates of education, or claim/repayment risk, or some other qualifier, Equalised Odds can result in different groups being held to different standards. This means that Equalised Odds is capable of entrenching systematic bias, rather than addressing it. * Calibration - outcomes for each protected group are predicted with equal reliability. If outcomes are found to be consistently under or overpredicted for a group (possibly due to a lack of representative data), then an adjustment/calibration needs to be made. Calibration is also capable of perpetuating pre-existing biases.    \n",
      "An example of how these different definitions play out in practice can be seen in the US criminal justice system, as per the box below.\n",
      "Case study: COMPAS\n",
      "For a given risk score in the US COMPAS criminal recidivism model [footnote 146] , the proportion of defendants who reoffend is roughly the same independent of a protected attribute, including ethnicity (Calibration). Otherwise, a risk score of 8 for a white person would mean something different for a Black person. Propublica’s criticism of this model highlighted that Black defendants who didn’t reoffend were roughly twice as likely to be given scores indicating a medium/high risk of recidivism as white defendants. However, ensuring equal risk scores among defendants who didn’t offend or re-offend (Equalised Odds) would result in losing Calibration at least to some degree. Fully satisfying both measures proves impossible. [footnote 147]\n",
      "If attributes of individuals (protected or otherwise) are apparently linked, such as recidivism and race [footnote 148] , then generally equality of opportunity (the generalised form of Equalised Odds) and Calibration cannot be reconciled. [footnote 149] If a model satisfies Calibration, then in each risk category, the proportion of defendants who reoffend is the same, regardless of race. The only way of achieving this if the recidivism rate is higher for one group, is if more individuals from that group are predicted to be high-risk. Consequently, this means that the model will make more false positives for that group than others, meaning Equalised Odds cannot be satisfied.\n",
      "Similarly, if a recidivism model satisfies Demographic Parity, then the chance a defendant ends up in any particular risk category is the same, regardless of their race. If one group has a higher recidivism rate than the others, that means models must make more false negatives for that group to maintain Demographic Parity, which (again) means Equalised Odds cannot be satisfied. Similar arguments apply for other notions of fairness. [footnote 150]\n",
      "It is worth noting that none of these fairness metrics take into account whether or not a given group is more likely to be arrested than another, or treated differently by a given prosecution service. This example serves to illustrate both the mutual incompatibility of many metrics, and their distinct limitations in context.\n",
      "Dieterich, William; Mendoza, Christina and Brennan, Tim; ‘COMPAS Risk Scales: Demonstrating Accuracy Equity and Predictive Parity’ , 2016  ↩\n",
      "Corbett-Davies, Sam; Pierson, Emma; Feller, Avi; Goel, Sharad; and Huq, Aziz . ‘Algorithmic decision-making and the Cost of Fairness’ , 2017  ↩\n",
      "Larson, Jeff; Mattu, Surya; Kirchner, Lauren; and Angwin, Julia; ‘How We Analyzed the COMPAS Recidivism Algorithm’ , 2016  ↩\n",
      "Mitigating bias\n",
      "Once an organisation has understood how different statistical definitions of fairness are relevant to their context, and relate to institutional goals, they can then be used to detect, and potentially mitigate, bias in their statistical approaches. Detection protocols and interventions can take place before, during, or after an algorithm is deployed.\n",
      "Pre-processing protocols and interventions generally concern training data, aiming to detect and remove sources of unfairness before a model is built. Modified data can then be used for any algorithmic approach. Fairness-focused changes in decision-making then exist at the most fundamental level. However, the nature of a given application should inform data and definitions of fairness used. If an organisation seeks to equalise the odds of particular outcomes for different groups (an Equalised Odds approach), pre-processing needs to be informed by those outcomes, and a prior round of model output. Most of the pre-processing interventions present in the machine learning literature do not incorporate model outcomes, only inputs and the use of protected attributes. Some pre-processing methods only require access to the protected attributes in the training data, and not in the test data. [footnote 151] In finance it is clear that companies place more emphasis on detecting and mitigating bias in the pre-processing stages (by carefully selecting variables and involving human judgement in the loop) than in- or post-processing.\n",
      "In-processing methods are applied during model training and analyse or affect the way a model operates. This typically involves a model’s architecture or training objectives, including potential fairness metrics. Modification (and often retraining) of a model can be an intensive process but the resulting high level of specification to a particular problem can allow models to retain a higher level of performance against their (sometimes new) goals. Methods such as constrained optimisation have been used to address both Demographic Parity and Equalised Odds requirements [footnote 152] . In-processing is a rapidly evolving field and often highly model dependent. It is also the biggest opportunity in terms of systemic fairness, but many approaches need to be formalised, incorporated into commonly used toolsets and, most importantly, be accompanied with legal certainty (see below).\n",
      "Post-processing approaches concern a model’s outputs, seeking to detect and correct unfairness in its decisions. This approach only requires scores or decisions from the original model and corresponding protected attributes or labels that otherwise describe the data used. Post-processing approaches are usually model-agnostic, without model modification or retraining. However, they effectively flag and treat symptoms of bias, not original causes. They are also often disconnected from model development, and are relatively easy to distort, making them risky if not deployed as part of a broader oversight process.\n",
      "Different interventions at different stages can sometimes be combined. Striving to achieve a baseline level of fairness in a model via pre-processing, but then looking for bias in particularly sensitive or important decisions during post-processing is an attractive approach. Care must be taken, however, that combinations of interventions do not hinder each other.\n",
      "Bias mitigation methods by stage of intervention and notion of fairness are shown in Appendix A. Detailed references can then be found in Faculty’s “Bias identification and mitigation in decision-making algorithms”, published separately.\n",
      "Practical challenges\n",
      "There are a number of challenges facing organisations attempting to apply some of these statistical notions of fairness in practical situations:\n",
      "Using statistical notions of fairness appropriately\n",
      "Statistical definitions of fairness deliver specific results to specific constraints. They struggle to encompass wider characteristics that do not lend themselves to mathematical formulation.\n",
      "There is no clear decision-making framework (logistical or legal) for selecting between definitions. Decisions over which measure of fairness to impose need extensive contextual understanding and domain knowledge beyond issues of data science. In the first instance, organisations should strive to understand stakeholder and end-user expectations around fairness, scale of impact and retention of agency, and consider these when setting desired outcomes.\n",
      "Any given practitioner is then forced, to some extent, to choose or mathematically trade off between different definitions. Techniques to inform this trade off are currently limited. Although exceptions exist [footnote 153] there seems to be a gap in the literature regarding trade-offs between different notions of fairness, and in the general maturity of the industry in making such decisions in a holistic way (i.e. not relying on data science teams to make them in isolation).​\n",
      "Compatibility with accuracy\n",
      "A large part of the machine learning literature on fairness is concerned with the trade-off between fairness and accuracy, i.e. ensuring that the introduction of fairness metrics has minimal impacts on model accuracy. In a pure statistical sense there is often a genuine trade-off here; imposing a constraint on fairness may lower the statistical accuracy rate. But this is often a false trade-off when thinking more holistically. Applying a fairness constraint to a recruitment algorithm sifting CVs might lower accuracy measured by a loss function over a large dataset, but doesn’t necessarily mean that company recruiting is sifting in worse candidates, or that the company’s sense of accuracy is free from historic bias.\n",
      "The effects of accuracy are relevant even when models attempt to satisfy fairness metrics in ways that run counter to wider notions of fairness. Random allocation of positions in a company would likely satisfy Demographic Parity, but would not generally be considered fair. Implementing any specific fairness measure also fundamentally changes the nature of what a model is trying to achieve. Doing so may make models ‘less accurate’ when compared to prior versions. This apparent incompatibility can lead to models being seen as less desirable because they are less effective at making choices that replicate those of the past. Debiasing credit models might require accepting ‘higher risk’ loans, and thus greater capital reserves, but (as mentioned below) these choices do not exist in isolation.\n",
      "Accuracy can in itself be a fairness issue. Notions of accuracy that are based on average outcomes, or swayed by outcomes for specific (usually large) demographic groups, may miss or conceal substantial biases in unexpected or less evident parts of a model’s output. Accuracy for one individual does not always mean accuracy for another.\n",
      "Organisations need to consider these trade-offs in the round, and understand the limitations of purely statistical notions of both fairness and accuracy when doing so.\n",
      "Understanding causality and reasons for unfairness\n",
      "Causes of unfairness are not part of these definitions and must be assessed on an organisational level. Most techniques look at outcomes, and can’t understand how they come to be, or why biases may exist (except in specific circumstances). Defining fairness based on causal inference [footnote 154] has only been developed to a limited extent [footnote 155] due to the difficulty of validating underlying (apparent) causal factors. Real-world definition of these factors can introduce further bias, especially for less well understood groups with less data.\n",
      "Static measurement and unintended consequences\n",
      "Definitions of fairness are “static”, in the sense that we measure them on a snapshot of the population at a particular moment in time. However, a static view of fairness neglects that most decisions in the real world are taken in sequence. Making any intervention into model predictions, their results, or the way decisions are implemented will cause that population to change over time. Failing to account for this risks leading to interventions that are actively counter-productive, and there are cases where a supposedly fair intervention could lead to greater unfairness. [footnote 156] There is the scope for unintended consequence here, and strategic manipulation on the part of individuals. However, the cost of manipulation will typically be higher for any disadvantaged group. Differing costs of manipulation can result in disparities between protected groups being exaggerated. [footnote 157] In implementing a process to tackle unfairness, organisations must deploy sufficient context-aware oversight, and development teams must ask themselves if they have inadvertently created the potential for new kinds of bias. Checking back against reference data is especially useful over longer time periods.\n",
      "Legal and policy issues\n",
      "Although these bias mitigation techniques can seem complex and mathematical, they are encoding fundamental policy choices concerning organisational aims around fairness and equality, and there are legal risks . Organisations must bring a wide range of expertise into making these decisions. A better set of common language and understanding between the machine learning and equality law communities would assist this.\n",
      "Seeking to detect bias in decision-making processes, and to address it, is a good thing. However, there is a need for care in how some of the bias mitigation techniques listed above are applied. Interventions can affect the outcomes of decisions about individuals, and even if the intent is to improve fairness, this must be done in a way that is compatible with data protection and equality law.\n",
      "Many of the algorithmic fairness tools currently in use have been developed under the US regulatory regime, which is based on a different set of principles to those in the UK and includes different ideas of fairness that rely on threshold levels (most notably the “4/5ths” rule), and enable affirmative action to address imbalances. Tools developed in the US may not be fit for purpose in other legal jurisdictions.\n",
      "Advice to industry\n",
      "Where organisations operating within the UK deploy tools developed in the US, they must be mindful that relevant equality law (along with that across much of Europe) is different.\n",
      "This uncertainty presents a challenge to organisations seeking to ensure their use of algorithms is fair and legally compliant. Further guidance is needed in this area (an example of the general need for clarity on interpretation discussed in Chapter 9); our understanding of the current position is as follows.\n",
      "Data Protection law\n",
      "The bias mitigation interventions discussed involve the processing of personal data, and therefore must have a lawful basis under data protection law. Broadly speaking the same considerations apply as for any other use of data; it must be collected, processed and stored in a lawful, fair and transparent manner, for specific, explicit and legitimate purposes. Its terms of use must be adequately communicated to the people it describes.\n",
      "The ICO has provided guidance on how to ensure that processing of this type complies with data protection law, along with some examples, in their recently published guidance on AI and data protection. [footnote 158] Processing data to support the development of fair algorithms is a legitimate objective (provided it is lawful under the Equality Act, see below), and broadly speaking if the right controls are put in place, data protection law does not seem to present a barrier to these techniques.\n",
      "There are some nuances to consider, especially for pre-processing interventions involving modification of labels on training data. In usual circumstances modifying personal data to be inaccurate would be inappropriate. However, where alterations made to training data are anonymised and not used outside of model development contexts, this can be justified under data protection legislation if care is taken. Required care might include ensuring that model features cannot be related back to an individual, the information that is stored is never used directly to make decisions about an individual, and that there is a lawful basis for processing the data in this way to support training a model (whether consent or another basis).\n",
      "Particular care is needed when dealing with Special Category data, which requires additional protections under data protection law. [footnote 159] While special category data is allowed to be used for measuring bias, this explicitly excludes decisions about individuals: which would include many mitigation techniques, particularly in post-processing. Instead, automated processing of special category data would need to rely on explicit consent from its subjects, or one of a small number of explicit exceptions. It is not enough to rest on the proportionate means to legitimate ends provision (in this case, fairer models) that otherwise applies.\n",
      "Equality law\n",
      "The position regarding the Equality Act 2010 is less clear.\n",
      "All of the mitigation approaches discussed in this section are intended to reduce bias, including indirect discrimination. However, there is a risk that some of the techniques used to do this could themselves be a cause of new direct discrimination. Even if “positive”, i.e. discrimination to promote equality for a disadvantaged group, this is generally illegal under the Equality Act.\n",
      "It is not yet possible to give definitive general guidance on exactly which techniques would or would not be legal in a given situation; organisations will need to think this through on a case-by-case basis. Issues to consider might include:\n",
      "Explicit use of a protected characteristic (or relevant proxies) to reweight models to achieve a fairness metric (e.g. in some applications of Feature Modification, or Decision Threshold Modification) carries risk. Organisations need to think through whether the consequence of using such a technique could disadvantage an individual explicitly on the basis of a protected characteristic (which is direct discrimination) or otherwise place those individuals at a disadvantage (which can lead to indirect discrimination).\n",
      "Resampling data to ensure a representative set of inputs is likely to be acceptable; even if it did have a disparate impact across different groups any potential discrimination would be indirect, and likely justifiable as a proportionate means to a legitimate end.\n",
      "Though there is a need for caution here, the legal risk of attempting to mitigate bias should not be overplayed. If an organisation’s aim is legitimate, and decisions on how to address this are taken carefully with due regard to the requirements of the Equality Act, then the law will generally be supportive. Involving a broad team in these decisions, and documenting them (e.g. in an Equality Impact Assessment) is good practice.\n",
      "If bias exists, and an organisation can identify a non-discriminatory approach to mitigate that, then there seems to be an ethical responsibility to do so. If this can’t be done at the level of a machine learning model itself, then wider action may be required. Organisations developing and deploying algorithmic decision-making should ensure that their mitigation efforts do not lead to direct discrimination, or outcome differences without objective justification.\n",
      "Despite the complexity here, algorithmic fairness approaches will be ​essential to facilitate widespread adoption of algorithmic decision-making.\n",
      "Advice to industry\n",
      "Where organisations face historical issues, attract significant societal concern, or otherwise believe bias is a risk, they will need to measure outcomes by relevant protected characteristics to detect biases in their decision-making, algorithmic or otherwise. They must then address any uncovered direct discrimination, indirect discrimination, or outcome differences by protected characteristics that lack objective justification.\n",
      "In doing so, organisations should ensure that their mitigation efforts do not produce new forms of bias or discrimination. Many bias mitigation techniques, especially those focused on representation and inclusion, can legitimately and lawfully address algorithmic bias when used responsibly. However, some risk introducing positive discrimination, which is illegal under the Equality Act. Organisations should consider the legal implications of their mitigation tools, drawing on industry guidance and legal advice.\n",
      "The best approach depends strongly on the use case and context. Interviews with organisations in the finance sector did not reveal a commonly used approach; companies use a mix of in-house and external tools. There is a general appetite for adapting open-source tools to internal uses, and among the companies consulted, none had developed in-house tools from scratch. In recruitment, we found that vendors of machine learning tools had established processes for examining their models, both off-the-shelf and bespoke tools. The most elaborate processes had three stages: pre-deployment checks with dummy data or sampled real-world data on models prior to deployment; post deployment checks where anonymised data from customers was used for further adjustments and correction of over-fitting; and third-party audits conducted by academic institutions particularly focused on identifying sources of bias. Firms used a mixture of proprietary techniques and open-source software to test their models.\n",
      "In terms of mitigation, there is a lot that can be done within the current legislative framework, but regulators will need to keep an eye on the way the law is applied, what guidance is needed to guide ethical innovation and whether the law might need to change in the future. Engagement with the public and industry will be required in many sectors to identify which notions of fairness and bias mitigation approaches are acceptable and desirable.\n",
      "Recommendations to regulators\n",
      "Recommendation 9 : Sector regulators and industry bodies should help create oversight and technical guidance for responsible bias detection and mitigation in their individual sectors, adding context-specific detail to the existing cross-cutting guidance on data protection, and any new cross-cutting guidance on the Equality Act.\n",
      "We think it is likely that a significant industry and ecosystem will need to develop with the skills to audit systems for bias, in part because this is a highly specialised skill that not all organisations will be able to support; in part because it will be important to have consistency in how the problem is addressed; and in part because regulatory standards in some sectors may require independent audit of systems. Elements of such an ecosystem might be licenced auditors or qualification standards for individuals with the necessary skills. Audit of bias is likely to form part of a broader approach to audit that might also cover issues such as robustness and explainability.\n",
      "7.5 Anticipatory Governance\n",
      "Within an organisation, especially a large one, good intentions in individual teams are often insufficient to ensure that the organisation as a whole achieves the desired outcome. A proportionate level of governance is usually required to enable this. What does this look like in this context?\n",
      "There is no one-size-fits-all approach, and unlike in some other areas (e.g. health and safety or security management), not yet an agreed standard on what such an approach should include. However, there is an increasing range of tools and approaches available. What is clear is that, given the pace of change, and the wide range of potential impacts, governance in this space must be anticipatory.\n",
      "Anticipatory Governance aims to foresee potential issues with new technology, and intervene before they occur, minimising the need for advisory or adaptive approaches, responding to new technologies after their deployment. Tools, ways of working, and organisations already exist to help proactively and iteratively test approaches to emerging challenges while they are still in active development. The goal is to reduce the amount of individual regulatory or corrective action and replace it with more collaborative solutions to reduce costs, and develop best practice, good standards, policy and practice.\n",
      "In practical terms, assessment of impacts and risks, and consultation with affected parties, are core to doing this within individual organisations. However, it is critical that they aren’t simply followed as tick box procedures. Organisations need to show genuine curiosity about the short, medium and long term impacts of increasingly automated decision-making, and ensure that they have considered the views of a wide range of impacted parties both within their organisation and in wider society. Assessments must not only consider the detail of how an algorithm is implemented, but whether it is appropriate at all in the circumstances, and how and where it interacts with human decision-makers.\n",
      "There are many published frameworks and sets of guidance offering approaches to structuring governance processes [footnote 160] , including guidance from GDS and the Alan Turing Institute targeted primarily at the UK public sector. [footnote 161] Different approaches will be appropriate to different organisations, but some key questions that should be covered include the following.\n",
      "Guidance to organisation leaders and boards\n",
      "Those responsible for governance of organisations deploying or using algorithmic decision-making tools to support significant decisions about individuals should ensure that leaders are in place with accountability for:\n",
      "Understanding the capabilities and limits of those tools.\n",
      "Considering carefully whether individuals will be fairly treated by the decision-making process that the tool forms part of.\n",
      "Making a conscious decision on appropriate levels of human involvement in the decision-making process.\n",
      "Putting structures in place to gather data and monitor outcomes for fairness.\n",
      "Understanding their legal obligations and having carried out appropriate impact assessments.\n",
      "This especially applies in the public sector when citizens often do not have a choice about whether to use a service, and decisions made about individuals can often be life-affecting.\n",
      "The list above is far from exhaustive, but organisations that consider these factors early on, and as part of their governance process, will be better placed to form a robust strategy for fair algorithmic deployment. In Chapters 8 and 9 below we discuss some of the more specific assessment processes (e.g. Data Protection Impact Assessments, Equality Impact Assessments, Human Rights Impact Assessments) which can provide useful structures for doing this.\n",
      "8. The regulatory environment\n",
      "Overview of findings:\n",
      "Regulation can help to address algorithmic bias by setting minimum standards, providing clear guidance that supports organisations to meet their obligations, and enforcement to ensure minimum standards are met.\n",
      "AI presents genuinely new challenges for regulation, and brings into question whether existing legislation and regulatory approaches can address these challenges sufficiently well. There is currently little case law or statutory guidance directly addressing discrimination in algorithmic decision-making.\n",
      "The current regulatory landscape for algorithmic decision-making consists of the Equality and Human Rights Commission (EHRC), the Information Commissioner’s Office (ICO) , and sector regulators and non-government industry bodies. At this stage, we do not believe that there is a need for a new specialised regulator or primary legislation to address algorithmic bias.\n",
      "However, algorithmic bias means that the overlap between discrimination law, data protection law and sector regulations is becoming increasingly important. This is particularly relevant for the use of protected characteristics data to measure and mitigate algorithmic bias, the lawful use of bias mitigation techniques, identifying new forms of bias beyond existing protected characteristics, and for sector-specific measures of algorithmic fairness beyond discrimination.\n",
      "Existing regulators need to adapt their enforcement to algorithmic decision-making, and provide guidance on how regulated bodies can maintain and demonstrate compliance in an algorithmic age. Some regulators require new capabilities to enable them to respond effectively to the challenges of algorithmic decision-making. While larger regulators with a greater digital remit may be able to grow these capabilities in-house, others will need external support.\n",
      "Recommendations to government:\n",
      "Recommendation 10: Government should issue guidance that clarifies the Equality Act responsibilities of organisations using algorithmic decision-making. This should include guidance on the collection of protected characteristics data to measure bias and the lawfulness of bias mitigation techniques.\n",
      "Recommendation 11: Through the development of this guidance and its implementation, government should assess whether it provides both sufficient clarity for organisations on their obligations, and leaves sufficient scope for organisations to take actions to mitigate algorithmic bias. If not, government should consider new regulations or amendments to the Equality Act to address this.\n",
      "Recommendations to regulators:\n",
      "Recommendation 12: The EHRC should ensure that it has the capacity and capability to investigate algorithmic discrimination. This may include EHRC reprioritising resources to this area, EHRC supporting other regulators to address algorithmic discrimination in their sector, and additional technical support to the EHRC.\n",
      "Recommendation 13: Regulators should consider algorithmic discrimination in their supervision and enforcement activities, as part of their responsibilities under the Public Sector Equality Duty.\n",
      "Recommendation 14: Regulators should develop compliance and enforcement tools to address algorithmic bias, such as impact assessments, audit standards, certification and/or regulatory sandboxes.\n",
      "Recommendation 15: Regulators should coordinate their compliance and enforcement efforts to address algorithmic bias, aligning standards and tools where possible. This could include jointly issued guidance, collaboration in regulatory sandboxes, and joint investigations.\n",
      "Advice to industry: Industry bodies and standards organisations should develop the ecosystem of tools and services to enable organisations to address algorithmic bias, including sector- specific standards, auditing and certification services for both algorithmic systems and the organisations and developers who create them.\n",
      "Future CDEI work:\n",
      "CDEI plans to grow its ability to provide expert advice and support to regulators, in line with our existing terms of reference. This will include supporting regulators to coordinate efforts to address algorithmic bias and to share best practice.\n",
      "CDEI will monitor the development of algorithmic decision-making and the extent to which new forms of discrimination or bias emerge. This will include referring issues to relevant regulators, and working with government if issues are not covered by existing regulations.\n",
      "8.1 Introduction\n",
      "This report has shown the problem of algorithmic bias, and ways that organisations can try to address the problem. There are good reasons for organisations to address algorithmic bias, ranging from ethical responsibility through to pressure from customers and employees. These are useful incentives for companies to try to do the right thing, and can extend beyond minimum standards to creating a competitive advantage for firms that earn public trust.\n",
      "However, the regulatory environment can help organisations to address algorithmic bias in three ways. Government can set clear minimum standards through legislation that prohibits unacceptable behaviour. Government, regulators and industry bodies can provide guidance and assurance services to help organisations correctly interpret the law and meet their obligations. Finally, regulators can enforce these minimum standards to create meaningful disincentives for organisations who fail to meet these obligations.\n",
      "Alternatively, a regulatory environment with unclear requirements and weak enforcement creates the risk that organisations inadvertently break the law, or alternatively that this risk prevents organisations from adopting beneficial technologies. Both of these situations are barriers to ethical innovation, which can be addressed through clear and supportive regulation.\n",
      "Data-driven technologies and AI present a range of new challenges for regulators. The rapid development of new algorithmic systems means they now interact with many aspects of our daily lives. These technologies have the power to transform the relationship between people and services across most industries by introducing the ability to segment populations using algorithms trained on larger and richer datasets. However, as we have seen in our sector-focused work, there are risks of these approaches reinforcing old biases, or introducing new ones, by treating citizens differently due to features beyond their control, and in ways they may not be aware of. The regulatory approach of every sector where decision-making takes place about individuals will need to adapt and respond to these new practices that algorithmic decision-making brings.\n",
      "Given this widespread shift, it is necessary to reflect both on whether the existing regulatory and legislative frameworks are sufficient to deal with these novel challenges, as well as how compliance and enforcement may operate in an increasingly data-driven world. For example, regulatory approaches that rely on individual complaints may not be sufficient in a time where people are not always aware of how an algorithm has impacted their life. Similarly, the pace of change in the development of decision-making technologies may mean that certain approaches are too slow to respond to the new ways algorithms are already impacting people’s lives. Regulators will need to be ambitious in their thinking, considering the ways algorithms are already transforming their sectors, and what the future may require.\n",
      "The government and some regulators have already recognised the need for anticipatory regulation to respond to these challenges. Regulation For The Fourth Industrial Revolution [footnote 162] lays out the challenge as a need for proactive, flexible, outcome-focused regulation, enabling greater experimentation under appropriate supervision, and supporting innovators to actively seek compliance. It also details the need for regulators to build dialogue across society and industry, and to engage in global partnerships. NESTA adds [footnote 163] that such regulation should be inclusive and collaborative, future-facing, iterative, and experimental, with methods including “sandboxes: experimental testbeds; use of open data; interaction between regulators and innovators; and, in some cases, active engagement of the public”. In this section we look at both the current landscape, and the steps required to go further.\n",
      "8.2 Current landscape\n",
      "The UK’s regulatory environment is made up of multiple regulators, enforcement agencies, inspectorates and ombudsmen (which this report will call ‘regulators’ for simplicity) with a range of responsibilities, powers and accountabilities. These regulators are typically granted powers by the primary legislation that established them, although some ‘private regulators’ may be set up through industry self-regulation.\n",
      "Some regulators have an explicit remit to address bias and discrimination in their enabling legislation, while others may need to consider bias and discrimination in decision-making when regulating their sectors. In practice, however, there is a mixed picture of responsibility and prioritisation of the issue.\n",
      "Data-driven algorithms do not necessarily replace other decision-making mechanisms wholesale, but instead fit into existing decision-making processes. Therefore, rather than a new algorithmic regulatory system, the existing regulatory environment needs to evolve in order to address bias and discrimination in an increasingly data-driven world.\n",
      "The key piece of legislation that governs discrimination is the Equality Act 2010 . The Act provides a legal framework to protect the rights of individuals and provides discrimination law to protect individuals from unfair treatment, including through algorithmic discrimination. Underlying anti-discrimination rights are also set out in the Human Rights Act 1998 (which establishes the European Convention on Human Rights in UK law). When a decision is made by an organisation on the basis of recorded information (which is the case for most significant decisions), the Data Protection Act 2018 and the General Data Protection Regulation (GDPR) are also relevant. This legislation controls how personal information is used by organisations, businesses or the government and sets out data protection principles which includes ensuring that personal information is used lawfully, fairly and transparently. Data protection law takes on a higher level of relevance in the case of algorithmic decision-making, where decisions are inherently data-driven, and specific clauses related to automated processing and profiling apply (see below for more details).\n",
      "In support of this legislation, there are two primary cross-cutting regulators: the Equality and Human Rights Commission (EHRC, for the Equality Act and Human Rights Act) and the Information Commissioner’s Office (ICO, for the Data Protection Act and GDPR).\n",
      "However, given the range of types of decisions that are being made with the use of algorithmic tools, there is clearly a limit in how far cross-cutting regulators can define and oversee what is acceptable practice. Many sectors where significant decisions are made about individuals have their own specific regulatory framework with oversight on how those decisions are made.\n",
      "These sector regulators have a clear role to play: algorithmic bias is ultimately an issue of how decisions are made by organisations, and decision-making is inherently sector-specific. In sectors where algorithmic decision-making is already significant, the relevant enforcement bodies are already considering the issues raised by algorithmic decision-making tools, carrying out dedicated sector-specific research and increasing their internal skills and capability to respond.\n",
      "Overall, the picture is complex, reflecting the overlapping regulatory environment of different types of decisions. Some have called for a new cross-cutting algorithms regulator, for example Lord Sales of the UK Supreme Court. [footnote 164] We do not believe that this is the best response to the issue of bias, given that many of the regulatory challenges raised are inevitably sector-specific, and typically algorithms only form part of an overall decision-making process regulated at sector level. However, more coordinated support for and alignment between regulators may be required (see below) to address the challenge across the regulatory landscape.\n",
      "8.3 Legal background\n",
      "The Equality Act 2010 (the Act) legally protects people from discrimination and sets out nine ‘protected characteristics’ which it is unlawful to discriminate on the basis of: age disability gender reassignment marriage and civil partnership pregnancy and maternity race religion or belief sex sexual orientation\n",
      "The Act prohibits direct discrimination, indirect discrimination, victimisation and harassment based on these characteristics. [footnote 165] It also establishes a requirement to make reasonable adjustments for people with disabilities, and allows for, but does not require, ‘positive action’ to enable or encourage the participation of disadvantaged groups. The Act also establishes the Public Sector Equality Duty [footnote 166] which requires all public sector bodies to address inequality through their day-to-day activities.\n",
      "The Equality Act has effect in England, Wales and Scotland. Although Northern Ireland has similar anti-discrimination principles, they are covered in different legislation. There are some legal differences in the scope of protected characteristics (e.g. political opinions are protected in Northern Ireland), thresholds for indirect discrimination, and some practical differences in the Public Sector Equality Duty. However, for the purpose of this report, we will use the language of the Equality Act.\n",
      "Section 1 of the Equality Act requires public bodies to actively consider the socio-economic outcomes of any given policy. It is currently in effect in Scotland, and will commence in Wales next year. Increasingly large parts of the public sector (and those contracted by it) must show that they have given due diligence to such issues ahead of time, as part of their development and oversight chain. Recent controversies over exam results have highlighted broad public concern about socio-economic disparities.\n",
      "Each of these provisions apply to any area where individuals are treated differently, regardless of whether an algorithm was involved in the decision.\n",
      "Human Rights Act\n",
      "The UK also protects against discrimination in the Human Rights Act 1998, which establishes the European Convention on Human Rights in UK domestic law. This Act explicitly prohibits discrimination in Article 14: “The enjoyment of the rights and freedoms set forth in this Convention shall be secured without discrimination on any ground such as sex, race, colour, language, religion, political or other opinion, national or social origin, association with a national minority, property, birth or other status.”\n",
      "This is a broader set of characteristics, notably preventing discrimination based on language, political opinion and property. However, this also provides narrower protection than the Equality Act, as it applies specifically to realising the other human rights in the Act. This means that government bodies cannot discriminate based on these characteristics when granting or protecting rights such as the right to a fair trial (Article 6), freedom of expression (Article 10), or freedom of assembly (Article 11).\n",
      "The Council of Europe has recently established an Ad-hoc Committee on AI (CAHAI) [footnote 167] to consider a potential legal framework to support the application of AI based on human rights, democracy and the rule of law.\n",
      "Data Protection Law\n",
      "The Data Protection Act 2018 alongside the EU General Data Protection Regulation (GDPR) regulates how personal information is processed [footnote 168] by organisations, businesses or the government. The Data Protection Act supplements and tailors the GDPR in UK domestic law. Under data protection law, organisations processing personal data must follow data protection principles, which includes ensuring that information is used lawfully, fairly and transparently.\n",
      "Data protection law gives individuals (“data subjects” in GDPR language) a number of rights that are relevant to algorithmic decision-making, for example the right to find out what information organisations store about them, including how their data is being used. There are additional specific rights when an organisation is using personal data for fully automated decision-making processes and profiling which have legal or other significant effects on individuals. The introduction of the Data Protection Act and the GDPR, which make organisations liable for significant financial penalties for serious breaches, has led to a strong focus on data protection issues at the top level of organisations, and a significant supporting ecosystem of guidance and consultancy helping organisations to comply.\n",
      "A wide range of data protection provisions are highly relevant to AI generally, and automated decision-making, and there has been widespread public commentary (both positive and negative) on approaches to training and deploying AI tools compliant with them. [footnote 169] The GDPR sets out other provisions relating to algorithmic bias and discrimination, including: Principle for data processing to be lawful and fair. In Article 5(1), there is the general principle that personal data must be “processed lawfully, fairly and in a transparent manner”. [footnote 170] The lawfulness requirement means that data processing must be compliant with other laws, including the Equality Act. The fairness requirement means that the processing is not “unduly detrimental, unexpected, or misleading” to data subjects.\n",
      "Provisions around the illegality of discriminatory profiling. In Recital 71, the GDPR advises that organisations should avoid any form of profiling that results in “discriminatory effects on natural persons on the basis of racial or ethnic origin, political opinion, religion or beliefs, trade union membership, genetic or health status or sexual orientation, or processing that results in measures having such an effect.”\n",
      "Data subjects have a right to not be subject to a solely automated decision-making process with significant effects. Article 22(1) states that “The data subject shall have the right not to be subject to a decision based solely on automated processing, including profiling, which produces legal effects concerning him or her or similarly significantly affects him or her.” The ICO specifies [footnote 171] that organisations have proactive obligations to bring details of these rights to the attention of individuals. Under Article 7.3, the rights of data subjects to withdraw their consent for processing of their data at any time, and under Article 21 the right to object to data processing carried out under a legal basis other than consent.\n",
      "Data protection legislation provides several strong levers to ensure procedural fairness. However, there are some inherent limitations in thinking about fair decisions purely through the lens of data protection; processing of personal data processing is a significant contributor to algorithmic decisions, but is not the decision itself, and other considerations less directly relevant to data may apply. Data protection should therefore not be seen as the entirety of regulation applying to algorithmic decisions. Efforts to comply with data protection law must not distract organisations from considering other ethical and legal obligations, for example those defined in the Equality Act.\n",
      "Consumer protection and sector-specific legislation\n",
      "Beyond the three cross-cutting Acts above, additional laws establish fair or unfair conduct in a specific area of decision-making. These laws also apply in principle where this conduct is made or supported by an algorithm, although this is often untested in case law.\n",
      "Consumer Protection law such as the Consumer Rights Act 2015 sets out consumer rights around misleading sales practices, unfair contract terms, and defective products and services. This law sets out cross-sector standards for commercial behaviour, but is typically enforced through sector-specific Ombudsmen.\n",
      "Some regulated sectors, particularly those that are consumer facing, set out additional requirements for fair treatment, notably the Financial Conduct Authority’s principles [footnote 172] for fair treatment of customers, or Ofcom’s framework for assessing fairness in telecommunications services. [footnote 173] Again, algorithmic decisions would still remain subject to these rules, though it is not always clear how algorithmic decision-making could meet them in practice. The requirement for consumers to be “provided with clear information” and to be “kept appropriately informed before, during and after the point of sale” is straightforward to apply to algorithmic processes, but ‘consumers can be confident they are dealing with firms where the fair treatment of customers is central to the corporate culture’ is less clear.\n",
      "Limitations of current legislation\n",
      "As previously discussed, the Equality Act defines a list of protected characteristics which it is unlawful to use as the basis for less favourable treatment. These characteristics reflect the evidence of systematic discrimination at a point in time, and can (and should) evolve as new forms of discrimination emerge and are recognised by society and the legal system.\n",
      "There are also multiple situations where algorithms could potentially lead to unfair bias that does not amount to discrimination, such as bias based on non-protected characteristics. [footnote 174] In some cases we may expect the emergence of new protected characteristics to cover these issues, but this will reflect society recognising new forms of discrimination that have been amplified by algorithms, rather than the use of algorithms themselves creating a new type of discrimination.\n",
      "While these situations challenge current equality legislation, they do not imply that an entirely new framework is required for algorithmic decision-making. In these examples, data protection legislation would offer affected people some levers to understand and challenge the process by which these decisions had been reached. Furthermore, the requirement for ‘fair’ data processing under GDPR could mean that this kind of bias is non-compliant with data protection law, but this is legally untested.\n",
      "In the public sector, bias based on arbitrary characteristics could also be challenged under the Human Rights Act where Article 14 prohibits discrimination based on ‘other status’, although any specific type of arbitrary bias would also need to be tested by the courts.\n",
      "Therefore, we do not believe there is evidence to justify an entirely new legislative or regulatory regime for algorithmic bias. Furthermore, a specific regulatory regime for algorithmic bias would risk inconsistent standards for bias and discrimination across algorithmic and non-algorithmic decisions, which we believe would be unworkable.\n",
      "Instead, the current focus should be on clarifying how existing legislation applies to algorithmic decision-making , ensuring that organisations know how to comply in an algorithmic context, alongside effective enforcement of these laws to algorithmic decision-making.\n",
      "This is a matter of some urgency; as we have set out in this report, there are clearly risks that algorithmic decision-making can lead to discrimination. This is unlawful and the application of current legislation must be clear and enforced accordingly to ensure bad practice is reduced as much as possible.\n",
      "Case law on the Equality Act\n",
      "While legislation sets out the principles and minimum requirements for behaviour, these principles need to be interpreted in order to be applied in practice. This interpretation can occur by individual decision makers and/or regulators, but this interpretation is only definitive when tested by the courts.\n",
      "While there is a growing body of case law that addresses algorithms in data protection law, there have been very few examples of litigation in which algorithmic or algorithm supported decisions have been challenged under the Equality Act. In the absence of such case law, such interpretations are inherently somewhat speculative.\n",
      "One of the few examples was on the use of facial recognition technology by South Wales Police, which was recently challenged via a judicial review, both on data protection and Equality Act grounds:\n",
      "Case study: Facial recognition technology\n",
      "One of the few legal cases to test the regulatory environment of algorithmic bias was on the use of live facial recognition technology by police forces, following concerns around violations of privacy and potential biases within the system. Facial recognition technology has been frequently criticised for performing differently against people with different skin tones, meaning accuracy of many systems is often higher for white men compared to people with other ethnicities. [footnote 175]\n",
      "South Wales Police have trialled the use of live facial recognition in public spaces on several occasions since 2017. These trials were challenged through judicial review, and were found unlawful in the Court of Appeal on 11 August 2020. [footnote 176]\n",
      "One of the grounds for successful appeal was that South Wales Police failed to adequately consider whether their trial could have a discriminatory impact, and specifically that they did not take reasonable steps to establish whether their facial recognition software contained biases related to race or sex. In doing so, the court found that they did not meet their obligations under the Public Sector Equality Duty.\n",
      "Note that in this case there was no evidence that this specific algorithm was biased in this way, but that South West Police failed to take reasonable steps to consider this. This judgement is very new as this report goes to press, but it seems likely that this could have significant legal implications for public sector use of algorithmic decision-making, suggesting that the Public Sector Equality Duty requires public sector organisations to take reasonable steps to consider potential bias when deploying algorithmic systems, and to detect algorithmic bias on an ongoing basis.\n",
      "See for example the risks section of CDEI’s recent snapshot paper on Facial Recognition Technology   ↩\n",
      "Beyond this, we are not aware of any other litigation in which the use of AI in decision-making has been challenged under the Equality Act. This means there is little understanding of what the Equality Act requires in relation to data-driven technology and AI. Whilst it is clear that if an algorithm was using a protected characteristic as input into a model and was making decisions on this basis, that would likely constitute discrimination, it is less clear in what circumstances use of variables that correlate with protected characteristics would be considered (indirectly) discriminatory.\n",
      "The Equality Act states that in some cases, apparent bias may not constitute indirect discrimination if it involves proportionate means of achieving a legitimate aim. There is guidance and case law to help organisations understand how to interpret this in a non-algorithmic context.\n",
      "However in algorithmic decision-making this is perhaps less clear. For example, the ruling by the European Court of Justice in the Test-Achats case made it unlawful for insurers to charge different rates based on sex or gender. [footnote 177] UK car insurance providers had routinely charged higher premiums for men, based on their higher expected claims profile. These insurers responded by pricing insurance with more opaque algorithms based on other observable characteristics such as occupation, car model and size of engine, or even telematics that tracked individual driver behaviour. This change eliminated direct discrimination by sex and arguably shifted pricing towards more ‘objective’ measures of insurance risk. However, auto insurance prices remain significantly higher for men, and it is unclear and legally untested where these algorithms cross from legitimate pricing based on risk, to indirect discrimination based on proxies for sex, such as occupation.\n",
      "The lack of case law has meant organisations are often left to figure out the appropriate balance for themselves or look to international standards that do not necessarily reflect the equality framework in the UK. The uncertainty in this area is both a risk to fairness and a constraint on innovation. Guidance on appropriate good practice would help organisations navigate some of these challenges, as well as help understand the parameters of what is considered acceptable within the law.\n",
      "Regulations and guidance\n",
      "Government and regulators have several ways to provide clearer guidance on how to interpret the law. These types of guidance and regulations differ in their legal status and their audience.\n",
      "Statutory Codes of Practice are provided by regulators to clarify how existing law applies to a particular context. These are typically prepared by a regulator but presented by a minister in parliament. These codes and guidelines are legal in nature, and are targeted at courts, lawyers and other specialists such as HR professionals. Technical guidelines are similar to statutory codes, but are prepared by a regulator without statutory backing. Courts are not required to follow them, but will generally consider them (and whether an organisation followed them) as evidence. They must draw from existing statute and case law, and focus on how to apply the existing law to particular situations.\n",
      "Regulators can also issue guidance as information and advice for particular audiences, e.g. for employers or service providers. This could extend beyond current statute and case law, but must be compatible with the existing law. EHRC guidance is harmonised with statutory codes, and is focused on making the existing legal rights and obligations accessible to different audiences such as employers or affected individuals. ICO guidance often takes a similar approach, though some ICO guidance (such as that on AI) offers additional best practice recommendations which organisations are not required to follow if they can find another way to meet their legal obligations.\n",
      "The issues of algorithmic bias raised in this report require both clarification of the existing law, and more practical guidance that supports different stakeholders to understand and meet their obligations. In particular, organisations need clarity on the lawfulness of bias mitigation techniques, so that they can understand what they can do to address bias\n",
      "This clarification of existing law requires detailed knowledge of both employment law and how bias mitigation techniques work. This cross-functional effort should be led by government in order to provide official sanction as government policy, but draw on relevant expertise across the broader public sector, including from EHRC and CDEI.\n",
      "Recommendations to government\n",
      "Recommendation 10: Government should issue guidance that clarifies the Equality Act responsibilities of organisations using algorithmic decision-making. This should include guidance on the collection of protected characteristics data to measure bias and the lawfulness of bias mitigation techniques.\n",
      "It is possible that the work to clarify existing legal obligations could still leave specific areas of uncertainty on how organisations can lawfully mitigate algorithmic bias while avoiding direct positive discrimination, or highlight undesirable constraints in what is possible. We believe this situation would be unacceptable, as it could leave organisations with an ethical, and often a legal, obligation to monitor algorithmic bias risks, but make them unable to deploy proportionate methods to address the bias they find.\n",
      "In this case, further clarity or amendments to equality law could be required, for example to help to clarify what lawful positive action means in the context of mitigating algorithmic bias, and where this might cross a line into unlawful (positive) discrimination.\n",
      "Government can clarify or amend existing law by issuing supplementary regulations or statutory instruments. These regulations are usually implemented by a minister presenting a statutory instrument in parliament. In some areas, a regulator is specifically authorised to issue rules or regulations that are also legally enforceable, such as the Financial Conduct Authority Handbook. However, under the Equality Act, the EHRC and other regulators do not have this power, and any regulations would need to be issued by a minister. If current law is unable to provide enough clarity to allow organisations to address algorithmic bias, government should issue regulations to help clarify the law.\n",
      "Recommendations to government\n",
      "Recommendation 11: Through the development of this guidance and its implementation, government should assess whether it provides both sufficient clarity for organisations on their obligations, and leaves sufficient scope for organisations to take actions to mitigate algorithmic bias. If not, government should consider new regulations or amendments to the Equality Act to address this.\n",
      "Beyond clarifying existing obligations, organisations need practical guidance that helps them meet their obligations. This should include their obligations under equality law, but also includes sector-specific concepts of fairness, and best practices and advice that go beyond minimum standards. As described in Recommendation 9 above, we believe that many of the specific issues and methods are likely to be sector-specific. Private sector industry bodies can also play a leadership role to facilitate best practice sharing and guidance within their industry.\n",
      "8.4 The role of regulators\n",
      "The use of algorithms to make decisions will develop and be deployed differently depending on the context and sector. Algorithmic decision-making is taking place increasingly across sectors and industries, and in novel ways. For algorithmic bias, both the EHRC and ICO have explicit responsibilities to regulate, while there are also responsibilities within the mandate of each sector regulator.\n",
      "The Equality and Human Rights Commission\n",
      "The Equality and Human Rights Commission (EHRC) is a statutory body responsible for enforcing the Equality Act 2010, as well as responsibilities as a National Human Rights Institution. Their duties include reducing inequality, eliminating discrimination and promoting and protecting human rights.\n",
      "The EHRC carries out its functions through a variety of means, including providing advice and issuing guidance to ensure compliance with the law. They also take on investigations where substantial breaches of the law are suspected, however these resource intensive investigations are limited to a few high priority areas. In addition to investigations, the EHRC uses an approach of strategic litigation where they pursue legal test cases in areas where the law is unclear. [footnote 178] The EHRC is less likely to be involved in individual cases, and rather directs people to the Equality Advisory Support Service.\n",
      "Given its broad mandate, the EHRC leverages its limited resources by working collaboratively with other regulators to promote compliance with the Equality Act 2010, for example by incorporating equality and human rights in sector-specific standards, compliance and enforcement. They also produce joint guidance in collaboration with sector regulators.\n",
      "Within their 2019-22 strategic plan, the EHRC highlights that technology affects many equality and human rights concerns but does not currently have a strand of work specifically addressing the risks of data-driven technologies. Instead, the implications of new technologies for the justice system, transport provision and decision-making in the workplace are captured within those specific programmes.\n",
      "In March 2020, the EHRC called for the suspension of the use of automated facial recognition and predictive algorithms in policing in England and Wales, until their impact has been independently scrutinised and laws are improved. However this was a specific response to a UN report and does not yet appear to be part of a wider strand of work. [footnote 179] The EHRC continues to monitor the development and implementation of such tools across policy areas to identify opportunities for strategic litigation to clarify privacy and equality implications. It also recently completed an inquiry into the experiences of people with disabilities in the criminal justice system, including the challenges arising from a move towards digital justice, and has undertaken research into the potential for discrimination in using AI in recruitment.\n",
      "Due to the importance of the Equality Act in governing bias and discrimination, the EHRC has a key role to play in supporting the application and enforcement of the Equality Act to algorithmic decision-making. While the EHRC has shown some interest in these issues, we believe they should further prioritise the enforcement of the Equality Act in relation to algorithmic decision-making. This will partly involve a re-prioritisation of the EHRC’s own enforcement, but there is also room to leverage the reach of sector regulators, by ensuring they have the necessary capability to carry out investigations and provide guidance for specific contexts. Data-driven technologies present a genuine shift in how discrimination operates in the 21st Century, so the EHRC will also need to consider whether they have sufficient technical skills in this area to carry out investigations and enforcement work, and how they might build up that expertise.\n",
      "Recommendations to regulators\n",
      "Recommendation 12: The EHRC should ensure that it has the capacity and capability to investigate algorithmic discrimination. This may include EHRC reprioritising resources to this area, EHRC supporting other regulators to address algorithmic discrimination in their sector, and additional technical support to the EHRC.\n",
      "Equalities bodies across Europe are facing similar challenges in addressing these new issues, and others have previously identified the need for additional resourcing. [footnote 180]\n",
      "The Information Commissioner’s Office\n",
      "The Information Commissioner’s Office (ICO) is the UK’s independent regulator for information rights. It is responsible for the implementation and enforcement of a number of pieces of legislation, including the Data Protection Act 2018 and GDPR.\n",
      "The ICO has a range of powers to carry out its work:\n",
      "It can require organisations to provide information.\n",
      "It can issue assessment notices that enable it to assess whether an organisation is complying with data protection regulation.\n",
      "Where it finds a breach of data protection regulation, it can issue an enforcement notice telling the organisation what it needs to do to bring itself into compliance (including the power to instruct an organisation to stop processing).\n",
      "It can impose significant financial penalties for breaches: up to €20m or 4% of annual total worldwide turnover.\n",
      "The ICO has a broad, cross-sectoral remit. It is focused on the challenge of overseeing new legislation: the interpretation and application of the GDPR is still evolving; case law under this legislation remains limited; and organisations and the public are still adapting to the new regime. The ICO has played a prominent role both in the UK and internationally in thinking about regulatory approaches to AI. Relevant activities have included:\n",
      "Leading a Regulators and AI Working Group providing a forum for regulators, and other relevant organisations (including CDEI) to share best practice and collaborate effectively.\n",
      "Developing, at the request of the government, detailed guidance on explainability, in partnership with the Alan Turing Institute. [footnote 181] Publishing guidance on AI and data protection that aims to help organisations consider their legal obligations under data protection as they develop data-driven tools. This guidance is not a statutory code, but contains advice on how to interpret relevant data protection law as it applies to AI, and recommendations on good practice for organisational and technical measures to mitigate the risks to individuals that AI may cause or exacerbate.\n",
      "This activity is, in part, a reflection of the increased scope of responsibilities placed on organisations within the Data Protection Act 2018, but also reflects gradual growth in the importance of data-driven technologies over several decades. These efforts have been useful in pushing forward activity in this space.\n",
      "The ICO has recently stated that bias in algorithms may fall under data protection law via the Equality Act: “The DPA 2018 requires that any processing is lawful, so compliance with the Equality Act 2010 is also a requirement of data protection law.” [footnote 182] The ICO also makes clear in its guidance that data protection also includes broader fairness requirements, for example: “Fairness, in a data protection context, generally means that you should handle personal data in ways that people would reasonably expect and not use it in ways that have unjustified adverse effects on them.” [footnote 183]\n",
      "Sector and specialist regulators\n",
      "In the sectors we studied in this review, relevant bodies include the Financial Conduct Authority (FCA) for financial services, Ofsted for children’s social care and HM Inspectorate of Constabulary and Fire and Rescue Services in policing. Recruitment does not fall under the remit of a specific sector regulator, although it is an area that has been a focus for the EHRC.\n",
      "There are other sector regulators in areas not studied in detail in this review, e.g. Ofgem for energy services. For all consumer-facing services, the remit of the Competition and Markets Authority (CMA) is also relevant, with obligations within consumer protection legislation for consumers to be treated fairly.\n",
      "Public Sector Equality Duty\n",
      "Whilst the Equality Act applies to both the public and private sector, there are further provisions for the public sector under the Public Sector Equality Duty (PSED). This duty sets out a legal mandate for public authorities to undertake activity to promote equality.\n",
      "A public authority must, in the exercise of its functions, have due regard to the need to:\n",
      "eliminate discrimination, harassment, victimisation and any other conduct that is prohibited by or under the Act;\n",
      "advance equality of opportunity between persons who share a relevant protected characteristic * and persons who do not share it;\n",
      "foster good relations between persons who share a relevant protected characteristic and persons who do not share it.\n",
      "Public authorities include sector regulators who should therefore deliver the commitments set out above. These obligations under the Equality Act provide the necessary mandate for regulators to work towards eliminating risks of discrimination from algorithmic decision-making within their sectors.\n",
      "There is a mixed picture of how well enforcement bodies are equipped to respond to bias in algorithmic decision-making. There are regulators such as the FCA who have explored specific research and have been proactive in understanding and addressing these concerns through regulatory guidance such as the Draft Guidance on Fair Treatment of Vulnerable Customers. [footnote 184] The FCA has also deployed innovations such as the regulatory sandbox, which temporarily reduces regulatory requirements for selected products and services, in exchange for more direct supervision and guidance from the FCA. [footnote 185] Some other regulators, for example the CMA, are taking action to build their expertise and activities in this area. However, many others are not as well resourced, do not have the relevant expertise to develop guidance in these areas, or are not treating this issue as a priority. There are particular challenges for enforcement bodies in sectors where these tools are particularly novel.\n",
      "Case study: The Financial Conduct Authority\n",
      "As we set out in Chapter 4, the financial services sector is one where the use of algorithmic decision-making tools are growing in development and deployment. One of the key enforcement bodies in this sector is the FCA, who have a responsibility for consumer protection.\n",
      "The FCA has focused a lot of attention on the sector’s use of technology, big data and AI, and identified this as a key research priority. They have spoken publicly about how the use of big data and algorithmic approaches could raise ethical issues, including concerns of algorithmic bias, and committed to further work to investigate issues in financial markets and present strategies for reducing potential harm.\n",
      "The FCA’s joint survey with the Bank of England of the use of ML by financial institutions demonstrates their focus on this area. Following this study, they have established a public-private working group on AI to further address some of the issues.\n",
      "The FCA sees its role to support the safe, beneficial, ethical and resilient deployment of these technologies across the UK financial sector. It acknowledges that firms are best placed to make decisions on which technologies to use and how to integrate them into their business, but that regulators will seek to ensure that firms identify, understand and manage the risks surrounding the use of new technologies, and apply the existing regulatory framework in a way that supports good outcomes for consumers.\n",
      "As algorithmic decision-making grows, we expect to see similar responses from sector bodies in areas where high stakes decisions are being made about people’s lives. This might involve developing technical standards on how these tools can be assessed for fairness and appropriate routes for challenge and redress for individuals. We believe there is a role for support from both the EHRC, within their regulatory remit, to work with other regulators, as well as CDEI for advice and coordination.\n",
      "This demonstrates the need for regulators to be sufficiently resourced to deal with equality issues related to the use of AI and data-driven technology in their sectors. It also raises the question of how the equality legislation is applied, regardless of the use of algorithms. This concern was also raised by the Women and Equalities Committee in their report “Enforcing the Equality Act: the law and the role of the Equality and Human Rights”, which stated:\n",
      "As public bodies all enforcement bodies should be using their powers to secure compliance with the Equality Act 2010 in the areas for which they are responsible. Such bodies are far better placed than the Equality and Human Rights Commission could ever be to combat the kind of routine, systemic, discrimination matters where the legal requirements are clear and employers, service providers and public authorities are simply ignoring them because there is no realistic expectation of sanction. [footnote 186]\n",
      "Consumer facing regulators such as the FCA, Ofgem and CMA also need to ensure fair treatment for vulnerable customers within their remit. While not an issue of discrimination, regulators set out guidelines for unfair treatment and monitor outcomes for this group. This regulatory activity is conducted separately for each sector, and there is scope for greater collaboration between enforcement bodies to share best practice and develop guidance, as well as being sufficiently skilled and resourced to carry out this work. CDEI can play a key role in providing advice to regulators as well as coordinating activities.\n",
      "Recommendation to regulators\n",
      "Recommendation 13: Regulators should consider algorithmic discrimination in their supervision and enforcement activities, as part of their responsibilities under the Public Sector Equality Duty.\n",
      "8.5 Regulatory tools\n",
      "Beyond enforcement and guidance, there are a range of tools that can help organisations to meet their regulatory requirements. These range from more proactive supervision models to methods that assure whether organisations have compliant processes and suitably skilled staff. All of these complementary tools should be considered by regulators and industry as they attempt to address algorithmic bias.\n",
      "Regulatory sandboxes\n",
      "A regulatory sandbox is a differentiated regulatory approach where a regulator provides more direct supervision for new products and services in a controlled environment. This supervision can range from advice whether new practices are compliant, through to limited exemptions from existing regulatory requirements. A number of regulators currently offer sandbox-based support for their sector, such as the FCA, Ofgem and the ICO.\n",
      "The main focus of these initiatives is to help organisations understand how they can operate effectively within regulatory frameworks, and help regulators understand how innovative products and services interact with existing regulations. However, this service is most useful to those organisations adopting new business models or innovative approaches to persistent problems that may not fit existing regulations. Examples include new applications of blockchain technology in the FCA sandbox, peer-to-peer energy trading in the Ofgem sandbox, and the use of health and social care data to reduce violence in London in the ICO sandbox.\n",
      "Addressing algorithmic bias is an important area of regulatory complexity where closer regulatory supervision may be helpful, particularly when new innovations are being adopted that do not easily fit the existing regulatory model.\n",
      "Regulators with existing sandboxes should consider applications where algorithmic bias is a serious risk, potentially with additional engagement from the EHRC. Regulators in sectors that are seeing accelerated deployment of algorithmic decision-making could consider the regulatory sandbox approach to provide greater support and supervision for innovations that may need new ways of addressing algorithmic bias.\n",
      "Impact assessments\n",
      "In the UK, organisations are already required to produce Data Protection Impact Assessments (DPIAs) when processing personal data that is high risk to individual rights and freedoms. These assessments must consider ‘risks to the rights and freedoms of natural persons’ more generally including the ‘impact on society as a whole’. [footnote 187] As a consequence, issues like discrimination may be considered within the remit of data protection impact assessments. However our sector work suggests that in practice, bias and discrimination are not often considered within DPIAs.\n",
      "Public sector organisations are also required to have due regard to a number of equality considerations when exercising their functions, which are focused on addressing the obligations organisations have under the Equality Act 2010. [footnote 188] Equality Impact Assessments are often carried out by public sector organisations prior to implementing a policy, ascertaining its potential impact on equality. Though not required by law, they are considered good practice as a way of facilitating and evidencing compliance with the Public Sector Equality Duty. There have been efforts to extend the role of Equality Impact Assessments more broadly to assess the risks to fairness raised by AI, [footnote 189] particularly in areas like recruiting. [footnote 190]\n",
      "Algorithmic bias and discrimination should be incorporated into existing Equality and Data Protection Impact Assessments as part of their internal governance and quality assurance processes. However, our research has indicated that there are a variety of challenges with using impact assessments for addressing algorithmic bias as a regulatory approach. There is limited evidence regarding the effectiveness of impact assessments for providing useful course correction in the development and implementation of new technologies. While the impact assessment process can usefully uncover and resolve compliance issues throughout the development and use of algorithms, we found that in practice [footnote 191] impact assessments are usually treated as a static document, completed either at the very beginning or very end of a development process and therefore do not capture the dynamic nature of machine learning algorithms, which is where algorithmic bias issues are likely to occur. It is therefore hard to regulate only against an impact assessment as it only shows one point in time; they should be seen as one tool complemented by others.\n",
      "There have also been efforts to combine equality and data protection concerns into a combined Algorithmic Impact Assessment [footnote 192] or Integrated Impact Assessment. [footnote 193] This could be an effective way to remove duplication and support a more consistent way of managing the regulatory and ethical risks raised by these technologies, including fairness. It may also help to highlight to regulators and organisations any tensions between different aspects of current law or guidance.\n",
      "Audit and certification\n",
      "One of the frequently cited challenges with the governance of algorithmic decision-making is around how organisations demonstrate compliance with equality legislation. For individuals who are the subject of algorithmic decision-making, the systems can appear opaque and commentators often refer to fears around the risk of “black-boxes” that hide the variables making the decisions. These concerns have led to calls for ways to assure that algorithmic systems have met a particular standard of fairness. These calls are often framed in terms of auditing, certification or impact assessments, which could also be used to assess other measures of algorithmic appropriateness, such as privacy or safety.\n",
      "In algorithmic bias, this lack of explainability also raises challenges for the burden of proof. In discrimination cases, the Equality Act (Section 136) reverses the burden of proof, meaning that if outcomes data suggest algorithmic discrimination has occurred, courts will assume this has occurred, unless the accused discriminating organisation can prove otherwise. That is, it is not enough for an organisation to say that it does not believe the discrimination has occurred, it needs to explicitly demonstrate that it doesn’t. It is therefore essential for organisations to know what would constitute a proportionate level of proof that their AI systems are not unintentionally discriminating against protected groups. [footnote 194]\n",
      "There are many contexts where organisations are required to meet standards or regulations, including health and safety, cyber security and financial standards. Each of these systems have evolved into ecosystems of services that allow organisations to prove to themselves, their customers and regulators, that they have met the standard. These ecosystems include auditing, professional accreditation, and product certification.\n",
      "There are some parts of the ‘AI assurance’ ecosystem that are starting to emerge, such as firms offering ‘AI ethics’ consultancy and calls for ‘AI auditing’ or ‘AI certification’. However, these efforts tend to be focused on data protection and accuracy, rather than fairness and discrimination.\n",
      "The ICO has recently published “Guidance on AI and data protection”, which sets out a set of key considerations for development of an AI system. It is focused largely on compliance with data protection principles, but it also touches on the areas of data protection that relate to discrimination, including discussion on the legal basis upon which to collect sensitive data for testing for bias. However, this guidance does not directly address compliance with equality law, including the lawfulness of mitigation. The ICO has also announced a process for assessing GDPR certification schemes [footnote 195] which could be used to show that algorithmic decision-making is GDPR compliant. These steps reflect real progress in the governance of algorithms, but algorithmic bias and discrimination would inevitably be a secondary concern in a data protection centred framework.\n",
      "ICO’s Guidance on AI and Data Protection\n",
      "The ICO published its guidance on AI and data protection [footnote 196] in July 2020. This guidance is aimed at two audiences:\n",
      "those with a compliance focus, such as data protection officers (DPOs), general counsel, risk managers, senior management and the ICO’s own auditors; and\n",
      "technology specialists, including machine learning experts, data scientists, software developers and engineers, and cybersecurity and IT risk managers.\n",
      "This guidance does not provide ethical or design principles for the use of AI, but corresponds to application of data protection principles.\n",
      "There is currently no equivalent assurance ecosystem for bias and discrimination in algorithmic decision-making. We see this as a gap that will need to be filled over time, but will require increasing standardisation and guidance in the steps to prevent, measure and mitigate algorithmic bias.\n",
      "In the US, the National Institute of Standards and Technology (NIST), a non-regulatory agency of the United States Department of Commerce, provides a model for how external auditing of algorithms could emerge. The NIST developed the Facial Recognition Vendor Tests which requested access to commonly used facial recognition algorithms and to then test them under ‘black box’ conditions, by subjecting them all to the same set of validated test images. It initially started these efforts by benchmarking false positive and false negative rates of these algorithms, allowing them to be compared based on their accuracy.\n",
      "In 2019 this test was extended to examine racial bias, and found that many of these algorithms had much higher error rates, particularly false positives for women and minority ethnic groups. It also found that some algorithms had much lower demographic bias, and were often the algorithms that were the most accurate in general. This analysis has allowed benchmarking and standards based on accuracy to evolve into performance comparisons of algorithmic bias.\n",
      "Importantly for this role, NIST is seen as a trusted, independent, third party standards body by algorithm developers. However, this function does not necessarily need to be conducted by the government or regulators. Given sufficient expertise and commonly agreed standards, testing and certification against these standards could just as easily be provided by industry bodies or trusted intermediaries.\n",
      "As well as testing and certification of algorithmic systems themselves, there is a need for good practice standards for organisations and individuals developing these systems, and a relevant ecosystem of training and certification. This ecosystem of private or third sector services to support organisations to address algorithmic bias should be encouraged and is a growth opportunity for the UK. Professional services are a strong and growing area of the UK economy, including those providing audit and related professional services in a number of areas. Many companies are already looking at services that they can provide to help others build fair algorithms. By showing leadership in this area the UK can both ensure fairness for UK citizens, but also unlock an opportunity for growth.\n",
      "Recommendations to regulators\n",
      "Recommendation 14: Regulators should develop compliance and enforcement tools to address algorithmic bias, such as impact assessments, audit standards, certification and/or regulatory sandboxes.\n",
      "Advice to Industry\n",
      "Industry bodies and standards organisations should develop the ecosystem of tools and services to enable organisations to address algorithmic bias, including sector-specific standards, auditing and certification services for both algorithmic systems and the organisations and developers who create them.\n",
      "Regulatory coordination and alignment\n",
      "Algorithmic bias is likely to grow in importance, and this report shows that regulators will need to update regulatory guidance and enforcement to respond to this challenge. Given the overlapping nature of equality, data protection and sector-specific regulations, there is a risk that this could lead to a more fragmented and complex environment. Regulators will need to coordinate their efforts to support regulated organisations through guidance and enforcement tools. This will need to go further than cross-regulator forums, through to practical collaboration in their supervision and enforcement activities. Ideally, regulators should avoid duplicative compliance efforts by aligning regulatory requirements, or jointly issue guidance. Regulators should also pursue joint enforcement activities, where sector regulators pursue non-compliant organisations in their sector, with the support of cross-cutting regulators like the EHRC [footnote 197] and ICO.\n",
      "This will require additional dedicated work to coordinate efforts between regulators, who have traditionally focused on their regulatory responsibility. However, there has been an increasing effort for regulatory collaboration in other areas such as the UK Regulators Network which has more formally brought together economic sector regulators for collaboration and joint projects. Similar efforts to collaborate should be explored by sector regulators when addressing algorithmic bias.\n",
      "Recommendations to regulators\n",
      "Recommendation 15: Regulators should coordinate their compliance and enforcement efforts to address algorithmic bias, aligning standards and tools where possible. This could include jointly issued guidance, collaboration in regulatory sandboxes, and joint investigations.\n",
      "Future CDEI work\n",
      "CDEI plans to grow its ability to provide expert advice and support to regulators, in line with our existing terms of reference. This will include supporting regulators to coordinate efforts to address algorithmic bias and to share best practice.\n",
      "CDEI will monitor the development of algorithmic decision-making and the extent to which new forms of discrimination or bias emerge. This will include referring issues to relevant regulators, and working with government if issues are not covered by existing laws and regulations.\n",
      "9. Transparency in the public sector\n",
      "Overview of findings:\n",
      "Making decisions about individuals is a core responsibility of many parts of the public sector, and there is increasing recognition of the opportunities offered through the use of data and algorithms in decision-making.\n",
      "The use of technology should never reduce real or perceived accountability of public institutions to citizens. In fact, it offers opportunities to improve accountability and transparency, especially where algorithms have significant effects on significant decisions about individuals.\n",
      "A range of transparency measures already exist around current public sector decision-making processes. There is a window of opportunity to ensure that we get transparency right for algorithmic decision-making as adoption starts to increase.\n",
      "The supply chain that delivers an algorithmic decision-making tool will often include one or more suppliers external to the public body ultimately responsible for the decision-making itself. While the ultimate accountability for fair decision-making always sits with the public body, there is limited maturity or consistency in contractual mechanisms to place responsibilities in the right place in the supply chain.\n",
      "Recommendations to government:\n",
      "Recommendation 16: Government should place a mandatory transparency obligation on all public sector organisations using algorithms that have a significant influence on significant decisions affecting individuals. Government should conduct a project to scope this obligation more precisely, and to pilot an approach to implement it, but it should require the proactive publication of information on how the decision to use an algorithm was made, the type of algorithm, how it is used in the overall decision-making process, and steps taken to ensure fair treatment of individuals.\n",
      "Recommendation 17: Cabinet Office and the Crown Commercial Service should update model contracts and framework agreements for public sector procurement to incorporate a set of minimum standards around ethical use of AI, with particular focus on expected levels transparency and explainability, and ongoing testing for fairness.\n",
      "Advice to industry: Industry should follow existing public sector guidance on transparency, principally within the Understanding AI Ethics and Safety guidance developed by the Office for AI, the Alan Turing Institute and the Government Digital Service, which sets out a process-based governance framework for responsible AI innovation projects in the UK public sector\n",
      "9.1 Identifying the issue\n",
      "Why the public sector?\n",
      "Ensuring fairness in how the public sector uses algorithms in decision-making is crucial. The public sector makes many of the highest impact decisions affecting individuals, for example related to individual liberty or entitlement to essential public services. There is also precedent of failures in large scale, but not necessarily algorithmic, decision-making processes causing impacts on a large number of individuals, for example fitness-to-work assessments for disability benefits [footnote 198] or immigration case-working. [footnote 199] These examples demonstrate the significant impact that decisions made at scale by public sector organisations can have if they go wrong and why we should expect the highest standards of transparency and accountability.\n",
      "The lines of accountability are different between the public and private sectors. Democratically-elected governments bear special duties of accountability to citizens. [footnote 200] We expect the public sector to be able to justify and evidence its decisions. Moreover, an individual has the option to opt-out of using a commercial service whose approach to data they do not agree with, but they do not have the same option with essential services provided by the state.\n",
      "There are already specific transparency obligations and measures relevant to fair decision-making in the public sector in the UK, for example: Publication of internal process documentation for large scale decision-making processes such as those within Home Office [footnote 201] , Department of Work and Pensions [footnote 202] and HMRC. [footnote 203]\n",
      "The Freedom of Information Act offers citizens the ability to access a wide range of information about the internal workings of public sector organisations.\n",
      "Subject Access Requests under the Data Protection Act enable individuals to request and challenge information held about them (also applicable to the private sector). Some organisations publish Personal Information Charters describing how they manage personal information in line with the Data Protection Act. [footnote 204]\n",
      "Publication of Equality Impact Assessments for decision-making practices (which is not strictly required by the Equality Act 2010, but is often conducted as part of organisations demonstrating compliance with the Public Sector Equality Duty).\n",
      "Various other existing public sector transparency policies enable an understanding of some of the wider structures around decision-making, for example the publication of spending [footnote 205] and workforce data. [footnote 206]\n",
      "Parliamentary questions and other representation by MPs.\n",
      "Disclosure related to legal challenges to decision-making, e.g. judicial review.\n",
      "Inquiries and investigations by some statutory bodies and commissioners on behalf of individuals, e.g. the EHRC.\n",
      "There is also an opportunity for the government to set an example for the highest levels of transparency. Government can do this through the strong levers it has at its disposal to affect behaviour, either through direct management control over the use of algorithmic decision-making, or strategic oversight of arms-length delivery bodies, for example in policing or the NHS.\n",
      "Setting high ethical standards in how it manages private sector service delivery also offers a potential lever for strong standards of transparency in the public sector to raise standards in the private sector. For example, in a different context, mandation in 2016 of Cyber Essentials certification for all new public sector contracts not only improved public sector cyber security, but also cyber security in a marketplace of service providers who supply both public and private sector organisations. [footnote 207]\n",
      "The public is right to expect services to be delivered responsibly and ethically, regardless of how they are being delivered, or who is providing those services. - The Committee on Standards in Public life (2018) [footnote 208]\n",
      "Public bodies have a duty to use public money responsibly [footnote 209] and in a way that is “conducive to efficiency”. Given that a potential benefit of the use of algorithms to support decision-making, if done well, is optimising the deployment of scarce resources, [footnote 210] it could be argued that the public sector has a responsibility to trial new technological approaches. Nonetheless, this must be done in a way that manages potential risks, builds clear evidence of impact, and upholds the highest standards of transparency and accountability.\n",
      "What is the problem?\n",
      "Currently, it is difficult to find out what algorithmic systems the UK public sector is using and where. [footnote 211] This is a problem because it makes it impossible to get a true sense of the scale of algorithmic adoption in the UK public sector and therefore to understand the potential harms, risks and opportunities with regard to public sector innovation.\n",
      "The recent report by the Committee on Standards in Public Life on ‘AI and Public Standards’ noted that adoption of AI in the UK public sector remains limited, with most examples being under development or at a proof-of-concept stage. [footnote 212] This is consistent with what CDEI has observed in the sectors we have looked at in this Review. Nonetheless, these varying accounts could lead to a perception of intended opacity from government by citizens.\n",
      "Government is increasingly automating itself with the use of data and new technology tools, including AI. Evidence shows that the human rights of the poorest and most vulnerable are especially at risk in such contexts. A major issue with the development of new technologies by the UK government is a lack of transparency. - Philip Alston, The UN Special Rapporteur on Extreme Poverty and Human Rights [footnote 213]\n",
      "What is the value of transparency?\n",
      "The case for transparency has been made in multiple contexts, including for government policy [footnote 214] and algorithms. [footnote 215] Yet the term ‘transparency’ can be ambiguous, mean different things in different contexts, and should not in itself be considered a universal good. [footnote 216] For example, publishing all details of an algorithm could lead to the gaming of rules through people understanding how the algorithm works or disincentivise the development of relevant intellectual property. Another risk is that actors with misaligned interests could abuse transparency as a way of sharing selective pieces of information to serve communication objectives or purposefully manipulating an audience. However, we should be able to mitigate these risks if we consider transparency within the context of decisions being made by the public sector and if it is not seen as an end in itself, but alongside other principles of good governance [footnote 217] including accountability.\n",
      "We should also not assume that greater transparency from public sector organisations will inevitably lead to greater trust in the public sector. In fact, just providing information, if not intelligible to the public could fail to inform the public and even foster concern. Baroness Onora O’Neill established the principle of “intelligent accountability” [footnote 218] in her 2002 Reith Lecture and has since spoken of the need for “intelligent transparency” summarised below.\n",
      "According to Onora O’Neill’s principle of “intelligent transparency” information should be:\n",
      "Accessible: interested people should be able to find it easily.\n",
      "Intelligible: they should be able to understand it.\n",
      "Useable: it should address their concerns.\n",
      "Assessable: if requested, the basis for any claims should be available. [footnote 219]\n",
      "These are useful requirements to bear in mind when considering what type of transparency is desirable given that simply providing more information just for the sake of it will not automatically build trust.\n",
      "Trust requires an intelligent judgement of trustworthiness. So those who want others’ trust have to do two things. First, they have to be trustworthy, which requires competence, honesty and reliability. Second, they have to provide intelligible evidence that they are trustworthy, enabling others to judge intelligently where they should place or refuse their trust. - Onora O’Neill, ‘How to trust intelligently’ [footnote 220]\n",
      "Sir David Spiegelhalter has built on Onora O’Neill’s work by articulating the need to be able to interrogate the trustworthiness of claims made about an algorithm, and those made by an algorithm. This led him to produce the following set of questions that we should expect to be able to answer about an algorithm: [footnote 221]\n",
      "Is it any good (when tried in new parts of the real world)?\n",
      "Would something simpler, and more transparent and robust, be just as good?\n",
      "Could I explain how it works (in general) to anyone who is interested?\n",
      "Could I explain to an individual how it reached its conclusion in their particular case?\n",
      "Does it know when it is on shaky ground, and can it acknowledge uncertainty?\n",
      "Do people use it appropriately, with the right level of scepticism?\n",
      "Does it actually help in practice?\n",
      "These questions are a helpful starting point for public sector organisations when evaluating an algorithm they are developing or using and considering the sort of information they need to know and share in order to ensure it is meaningful in the public’s eyes.\n",
      "9.2 Delivering public sector transparency\n",
      "Based on the discussion above, we believe that more concrete action is needed to ensure a consistent standard of transparency across the public sector related to the use of algorithmic decision-making.\n",
      "In this section, we assess in some detail how this could work, the key conclusion of which is the following:\n",
      "Recommendations to government\n",
      "Recommendation 16: Government should place a mandatory transparency obligation on all public sector organisations using algorithms that have a significant influence on significant decisions affecting individuals. Government should conduct a project to scope this obligation more precisely, and to pilot an approach to implement it, but it should require the proactive publication of information on how the decision to use an algorithm was made, the type of algorithm, how it is used in the overall decision-making process, and steps taken to ensure fair treatment of individuals.\n",
      "Below we discuss in more detail where this recommendation comes from. Further work is needed to precisely scope this, and define what is meant by transparency. But rooting this thinking in O’Neill’s principle of “intelligent transparency” and Spiegelhalter’s questions of what we should expect from a trustworthy algorithm provide a solid basis to ensure there is careful thinking about the algorithm itself and the information that is published.\n",
      "What is in scope?\n",
      "The use of the word significant clearly requires more careful definition:\n",
      "Significant influence means that the output of the machine learning model is likely to meaningful affect the overall decision made about an individual, i.e. not just providing automation of a routine process but informing decision-making in a more meaningful way e.g. by assessing risk or categorising applications in a way that influences the outcome.\n",
      "Significant decision means that the decision has a direct impact on the life of an individual or group of individuals. In the Data Protection Act 2018, a decision is a “significant decision” if it produces an adverse legal effect concerning an individual or otherwise significantly affects them. Although according to the Data Protection Act this applies specifically to fully automated significant decisions, we would suggest a similar interpretation here which includes decisions made with human input.\n",
      "Some potential examples of algorithmic decision-making that would be in or out of scope are shown in Figure 5.\n",
      "Our scope is defined by the significance of the “effect” of the algorithm on the overall outcome of a decision-making process, and the significance of the “impact” of the overall decision on the life of the person(s) affected. A low impact decision can involve an algorithm which has a significant effect on the overall outcome of the decision-making process. An example of this would be a chatbot suggesting a webpage to a user on a government website. A decision which is only marginally affected by an algorithm can have a high impact on a person’s life. For example, assigning applications to a given queue, pure process automation (generating a standard form letter after a decision has been made), or moving data between two systems. Finally, a decision can have a high impact on a person’s life and be significantly affected by an algorithm during the decision-making process. An example of this would be risk allocation - high, medium or low. Decisions that fall into this category are in scope for the purposes of this review.\n",
      "Figure 5: Decisions can be differentiated by the influence of algorithms over the decision, and the significance of the overall decision\n",
      "When defining impactful or significant decisions, due consideration should be paid to where decisions relate to potentially sensitive areas of government policy, or where there may be low levels of trust in public sector institutions. These could include social care, criminal justice or benefits allocation.\n",
      "The definition of public sector in this context could be sensibly aligned with that used in the Equality Act 2010 or Freedom of Information Act 2000.\n",
      "Some exemptions to this general scoping statement will clearly be needed, which will require careful consideration. Potential reasons for exemption are:\n",
      "Transparency risks compromising outcomes: e.g. Where publication of too many details could undermine the use of the algorithm by enabling malicious outsiders to game it, such as in a fraud detection use case.\n",
      "Intellectual Property: In some cases the full details of an algorithm or model will be proprietary to an organisation that is selling it. We believe that it is possible to achieve a balance, and achieve a level of transparency that is compatible with intellectual property concerns of suppliers to the public sector. This is already achieved in other areas where suppliers accept standard terms around public sector spending data etc. There is some detailed thinking around this area that needs to be worked through as part of government’s detailed design of these transparency processes.\n",
      "National Security and Defence: e.g. there may be occasional cases where the existence of work in this area cannot be placed in the public domain.\n",
      "In general, our view is that risks in areas 1 and 2 should be managed by being careful about the actual information that is being published (i.e. keeping details at a sufficiently high level), while area 3 is likely to require a more general exemption scoped under the same principles as those under Freedom of Information legislation.\n",
      "What information should be published?\n",
      "Defining the precise details of what should be published is a complex task, and will require extensive further consultation across government and elsewhere. This section sets out a proposed draft scope, which will need to be refined as the government considers its response to this recommendation.\n",
      "A number of views on this have been expressed previously. For example, the Committee on Standards in Public Life report defines openness, which they use as interchangeable with transparency in their report, as: “fundamental information about the purpose of the technology, how it is being used, and how it affects the lives of citizens must be disclosed to the public.” [footnote 222]\n",
      "As a starting point, we would anticipate a mandatory transparency publication to include:\n",
      "A. Overall details of the decision-making process in which an algorithm/model is used.\n",
      "B. A description of how the algorithm/model is used within this process (including how humans provide oversight of decisions and the overall operation of the decision-making process).\n",
      "C. An overview of the algorithm/model itself and how it was developed, covering for example:\n",
      "The type of machine learning technique used to generate the model.\n",
      "A description of the data on which it was trained, an assessment of the known limitations of the data and any steps taken to address or mitigate these.\n",
      "The steps taken to consider and monitor fairness.\n",
      "D. An explanation of the rationale for why the overall decision-making process was designed in this way, including impact assessments covering data protection, equalities, human rights, carried out in line with relevant legislation. It is important to emphasise that this cannot be limited to the detailed design of the algorithm itself, but also needs to consider the impact of automation within the overall process, circumstances where the algorithm isn’t applicable, and indeed whether the use of an algorithm is appropriate at all in the context.\n",
      "Much of this is already common practice for public sector decision-making. However, identifying the right level of information on the algorithm is the most novel aspect. There are examples elsewhere that can help guide this. For example: Google’s model cards [footnote 223] aim to provide an explanation of how a model works to experts and non-experts alike. The model cards can assist in exploring limitations and bias risk, by asking questions such as: ‘does a model perform consistently across a diverse range of people, or does it vary in unintended ways as characteristics like skin colour or region change?’ The Government of Canada’s Algorithmic Impact Assessment which is a questionnaire designed to help organisations assess and mitigate the risks associated with deploying an automated decision system as part of wider efforts to ensure the responsible use of AI. [footnote 224] New York City Council passed the algorithmic accountability law in 2019 which has resulted in the setting up of a task force that will monitor the fairness and validity of algorithms used by municipal agencies, whilst ensuring they are transparent and accountable to the public. [footnote 225] The United Kingdom’s Departmental Returns prepared by different parts of government as part of the MacPherson review of government modelling in 2013. [footnote 226]\n",
      "The Office for AI, Turing Institute and Government Digital Service’s Understanding AI Ethics and Safety guidance [footnote 227] have set out a process-based governance framework for responsible AI innovation projects in the UK public sector. Within this guidance document they provide a definition of transparency within AI ethics as including both the interpretability of an AI system and the justifiability of its processes and outcome. This Guidance should be the starting point, along with the ideas and other examples set out in this report, for the UK government when considering precisely what set of information makes sense in the UK public sector. CDEI is happy to provide independent input into this work if required.\n",
      "How does this fit with existing transparency measures?\n",
      "We listed above a variety of existing public sector transparency measures related to decision-making. A theme of public commentary on the use of algorithms is that they can potentially undermine this transparency and accountability. Government should seek to demonstrate that this is not the case.\n",
      "In fact, existing FOI and DPA obligations arguably already give individuals the right to request access to all of the information listed in the scope above. Moreover, initiatives like the local government transparency code [footnote 228] which sets out the minimum data that local authorities should be publishing, the frequency it should be published and how it should be published are good examples to build on. In some regards, we are not proposing more transparency but more effective transparency. Whilst there are obligations for proactive disclosure under FOI and the DPA, these are not always effective as a transparency tool in practice and are often more reactive. By making publication of information a truly proactive process it can help government:\n",
      "Build in expectations of what will eventually have to be published at the early stages of projects.\n",
      "Structure releases in a consistent way which hopefully helps external groups (e.g. journalists, academia and civil society) engage with the data being published in an effective way, i.e. over time fewer genuine misunderstandings in the communication.\n",
      "Manage the overhead of responding to large numbers of similar reactive requests.\n",
      "Managing the process of transparency\n",
      "The House of Lords Science and Technology Select Committee and the Law Society have both recently recommended that parts of the public sector should maintain a register of algorithms in development or use.\n",
      "…the Government should produce, publish, and maintain a list of where algorithms with significant impacts are being used within Central Government, along with projects underway or planned for public service algorithms, to aid not just private sector involvement but also transparency. - House of Lords Science and Technology Select Committee [footnote 229]\n",
      "A National Register of Algorithmic Systems should be created as a crucial initial scaffold for further openness, cross-sector learning and scrutiny. - The Law Society, ‘Algorithms in the Criminal Justice System’ [footnote 230]\n",
      "CDEI agrees that there are some significant advantages both to government and citizens in some central coordination around this transparency. For example it would enable easier comparisons across different organisations, e.g. by promoting consistent style of transparency. Moreover, there are delivery and innovation benefits in allowing public sector organisations themselves to see what their peers are doing.\n",
      "However, implementing this transparency process in a coordinated way across the entire public sector is a challenging task, much greater in extent than either of the proposals quoted above (e.g. the use by local government in social care settings that we discussed in Chapter 6 would not be included in either of those examples).\n",
      "There are a number of comparators to consider in levels of coordination:\n",
      "Centralised for central government only: GDS Spend Controls\n",
      "Devolved to individual organisations: Publication of transparency data\n",
      "Central publication across public and private sector: Gender pay gap reporting [footnote 231]\n",
      "We suspect that there is a sensible middle ground in this case. The complexities of coordinating such a register across the entire public sector would be high, and subtle differences in what is published in transparency data might well apply in different sectors. We therefore conclude that the starting point here is to set an overall transparency obligation, and for the government to decide on the best way to coordinate this as it considers implementation.\n",
      "The natural approach to such an implementation is to pilot in a specific part of the public sector. For example, it could be done for services run directly by central government departments (or some subset of them), making use of existing coordination mechanisms managed by the Government Digital Service.\n",
      "It is likely that a collection of sector-specific registers might be the best approach, with any public sector organisations out of scope of any sector register remaining responsible for publishing equivalent transparency data themselves.\n",
      "The relationship between transparency and explainability\n",
      "To uphold accountability, public sector organisations should be able to provide some kind of explanation of how an algorithm operates and reaches its conclusion. As David Spiegelhalter says “a trustworthy algorithm should be able to ‘show its working’ to those who want to understand how it came to its conclusions”. [footnote 232] Crucially, the working needs to be intelligible to a non-expert audience and therefore focusing on publishing the algorithm’s source code or technical details as a demonstration of transparency can be a red herring.\n",
      "An area of explainability which previous reports and research have focused on is the black box. Indeed, the House of Lords Select Committee on AI expressed that it was unacceptable to deploy any AI system that could have a substantial impact on an individuals’ life, unless it can generate “a full and satisfactory explanation” for the decisions it will take and that this was extremely difficult to do with a black box algorithm. [footnote 233] In the case of many key administrative decisions, often based on well structured data, there may not be a need to develop highly sophisticated, black box algorithms to inform decisions; often simpler statistical techniques may perform as well. Where an algorithm is proposed that does have limitations in its explainability (i.e. a black box) the organisation should be able to satisfactorily answer Spiegelhalter’s questions in particular around whether something simpler would be just as good and whether you can explain how it works and how it reaches its conclusion.\n",
      "As mentioned in Chapter 4 the ICO and ATI have jointly developed guidance for organisations on how to explain decisions made with AI. The guidance offers several types of examples of explanations for different contexts and decisions, along with advice on the practicalities of explaining these decisions to internal teams and individuals. Whilst the guidance is not directed exclusively at the public sector, it contains valuable information for public sector organisations who are using AI to make decisions. There is also the potential for public sector organisations to publish case studies and examples of where they are applying the guidance to explain decisions made with AI.\n",
      "Ultimately, the algorithmic element of the decision-making process should not be so unexplainable and untransparent that it undermines the extent to which the public sector organisation is able to publish intelligent and intelligible information about the whole decision-making process.\n",
      "9.3 Public sector procurement\n",
      "The development and delivery of an algorithmic decision-making tool will often include one or more suppliers, whether acting as technology suppliers or business process outsourcing providers. Even where development and delivery of an algorithmic decision-making tool is purely internal, there is always reliance on externally developed tools and libraries, e.g. open source machine learning libraries in Python or R.\n",
      "In such supply chain models, the ultimate accountability for good decision-making always sits with the public body. Ministers are still held to account by Parliament and the public for the overall quality and fairness of decisions made (along with locally elected councillors or Police and Crime Commissioners where relevant). The Committee on Standards in Public Life noted in 2018 that the public is right to expect services to be delivered responsibly and ethically, regardless of how they are being delivered, or who is providing those services. [footnote 234]\n",
      "The transparency mechanisms discussed in the section above form part of this overall accountability, and therefore need to be practical in all of these different potential supply chain models.\n",
      "Supply chain models\n",
      "Some examples of possible models for outsourcing a decision-making process are as follows.\n",
      "  Public Body in house IT partner Business process outsourcing Policy and accountability for decision-making Public body Public body Public body Operational decision-making Public body Public body Supplier Model and tool development Public body Supplier Supplier (or subcontractor) Underlying algorithms and libraries Mostly open source, potentially some 3rd party proprietary Mostly open source, potentially some 3rd party proprietary Mostly open source, potentially some 3rd party proprietary Training data Public body and/or 3rd party Public body and/or 3rd party Public body and/or 3rd party\n",
      "Many of the issues around defining and managing such a supply chain in a sensible way are common with any government procurement of services dependent on technology. But the source and ownership of the data on which a machine learning model is trained can make the interdependency between customer and supplier more complex in this context than in many others. Where a model is trained on data provided by the customer, it’s not straightforward to flow down requirements on fairness in a supplier contract, as the ability to meet those requirements will be dependent in part on the customer’s data.\n",
      "This is not just a public sector issue. In the wider marketplace, the ecosystem around contracting for AI is not fully developed. There is a natural desire from those at the top of the tree to push some of the responsibilities for ethics and legal compliance of AI systems down their supply chain. This is common practice in a number of other areas, e.g. TUPE regulations create obligations on organisations involved in the transfer of services between suppliers, related to the employees providing those services. There are commonly understood standard clauses included in contracts that make it clear where those any financial liabilities associated with this sit. A similar notion of commonly understood contractual wording does not exist in this case.\n",
      "There are pros and cons of this position. On the positive side, it ensures that organisations with responsibility for the overall decision-making process cannot attempt to pass this off onto their suppliers without properly considering the end-to-end picture. But conversely, it means that there may be limited commercial incentive for suppliers further down the supply chain to really focus on how their products and services can support ethical and legally compliant practices.\n",
      "Addressing the issue\n",
      "The Office for AI, working in partnership with the World Economic Forum, has developed detailed draft guidance [footnote 235] on effective procurement of AI in the public sector, which includes useful consideration of how ethics issues can be handled in procurement. This is a helpful step forward, and it is encouraging that the UK government is taking a leading role in getting this right globally.\n",
      "The recent Committee on Standards in Public Life report on AI and Public Standards [footnote 236] noted that “…firms did not feel that the public sector often had the capability to make their products and services more explainable, but that they were rarely asked to do so by those procuring technology for the public sector.” This guidance aims to help address this, but there is clearly more to do to implement this effectively across the UK public sector.\n",
      "The guidance as drafted is focused on projects that are primarily focused on buying AI solutions. This is a relevant situation, but as AI increasingly becomes a generic technology present in a whole variety of use cases, much public sector procurement of AI will be implicitly within wider contracts. It is unlikely (and not necessarily desirable) that procurement teams across all areas will focus specifically on AI procurement amongst a range of other guidance and best practice.\n",
      "Similar issues occur for other common underlying requirements, such as those around data protection, cyber security and open book accounting. Part of the approach taken for these is to include standard terms with model contracts and framework agreements used across the public sector that capture a minimum set of core principles. These can never achieve as much as careful thought about how to contract for the right outcome in a specific context, but help establish a minimum common standard.\n",
      "A similar approach should be taken for AI ethics. For procurement activity where AI is a specific focus then procurement teams need to be designing specific requirements applicable to the use case, drawing on the Office for AI and World Economic Forum guidelines. But where use of algorithmic decision-making is not specifically expected, but could form part of possible supplier solutions to an output based requirement, a common baseline requirement is needed to give the contracting authority the ability to manage that risk in life.\n",
      "Given the range of different possible use cases it is difficult to place highly specific requirements in a model contract. The focus should be on enabling the contracting authority to have an appropriate level of oversight on the development and deployment of an algorithmic decision-making tool to oversee whether fairness considerations have been taken into account, along with rights to reject or request changes if they are not.\n",
      "Helpfully, in central government, and to some extent in the wider public sector, there is a centrally managed set of procurement policies, model contracts and framework agreements which underpin the majority of procurement processes. These are mainly managed by Cabinet Office’s Government Commercial Function (policy and model contracts), and the Crown Commercial Service (framework agreements). Work is already underway by these bodies to incorporate findings from the Office for AI/WEF procurement guidelines into AI-specific procurement activities, and the new AI framework RM6200. [footnote 237] However, there is scope to go further than this to cover all procurement activity which could potentially result in purchasing an AI-reliant service:\n",
      "Recommendations to government\n",
      "Recommendation 17: Cabinet Office and the Crown Commercial Service should update model contracts and framework agreements for public sector procurement to incorporate a set of minimum standards around ethical use of AI, with particular focus on expected levels of transparency and explainability, and ongoing testing for fairness.\n",
      "In developing the details of such terms, the government will need to consult with the marketplace to ensure that eventual terms are commercially palatable. The intention of this recommendation is to find a balance that gives commercial mechanisms for public bodies to manage concerns about bias in algorithmic decision-making (and indeed other ethical concerns around AI), but does not impose a burden on the market that is disproportionate to the risk or to other common terms within public sector procurement.\n",
      "In developing such standard terms, the government may want to draw on support from the Office for AI and CDEI.\n",
      "10. Next steps and future challenges\n",
      "This review has considered a complex and rapidly evolving field. Recognising the breadth of the challenge, we have focused heavily on surveying the maturity of the landscape, identifying the gaps, and setting out some concrete next steps. There is plenty to do across industry, regulators and government to manage the risks and maximise the benefits of algorithmic decision-making.\n",
      "Some of the next steps fall within CDEI’s remit, and we are keen to help industry, regulators and government in taking forward the practical delivery work to address the issues we have identified and future challenges which may arise.\n",
      "Government, industry bodies and regulators need to give more help to organisations building and deploying algorithmic decision-making tools on how to interpret the Equality Act in this context. Drawing on the understanding built up through this review, CDEI is happy to support several aspects of the work in this space by, for example:\n",
      "Supporting the development of any guidance on the application of the Equality Act to algorithmic decision-making.\n",
      "Supporting government on developing guidance on collection and use of protected characteristics to meet responsibilities under the Equality Act, and in identifying any potential future need for a change in the law, with an intent to reduce barriers to innovation.\n",
      "Drawing on the draft technical standards work produced in the course of this review and other inputs to help industry bodies, sector regulators and government departments in defining norms for bias detection and mitigation.\n",
      "Supporting the Government Digital Service as they seek to scope and pilot an approach to transparency.\n",
      "Growing our ability to provide expert advice and support to regulators, in line with our terms of reference, including supporting regulators to coordinate efforts to address algorithmic bias and to share best practice. As an example, we have been invited to take an observer role on the Financial Conduct Authority and Bank of England’s AI Public Private Forum which will explore means to support the safe adoption of machine learning and artificial intelligence within financial services, with an intent to both support that work, and draw lessons from a relatively mature sector to share with others.\n",
      "We have noted the need for an ecosystem of skilled professionals and expert supporting services to help organisations in getting fairness right, and provide assurance. Some of the development needs to happen organically, but we believe that action may be needed to catalyse this. CDEI plans to bring together a diverse range of organisations with interest in this area, and identifying what would be needed to foster and develop a strong AI accountability ecosystem in the UK. This is both an opportunity to manage ethical risks for AI in the UK, but also to support innovation in an area where there is potential for UK companies to offer audit services worldwide.\n",
      "Through the course of the review, a number of public sector organisations have expressed interest in working further with us to apply the general lessons learnt in specific projects. For example, we will be supporting a police force and a local authority as they develop practical governance structures to support responsible and trustworthy data innovation.\n",
      "Looking across the work listed above, and the future challenges that will undoubtedly arise, we see a key need for national leadership and coordination to ensure continued focus and pace in addressing these challenges across sectors.\n",
      "Government should be clear on where it wants this coordination to sit. There are a number of possible locations; for example in central government directly, in a regulator or in CDEI. Government should be clear on where responsibilities sit for tracking progress across sectors in this area, and driving the pace of change. As CDEI agrees our future priorities with government, we hope to be able to support them in this area.\n",
      "This review has been, by necessity, a partial look at a very wide field. Indeed, some of the most prominent concerns around algorithmic bias to have emerged in recent months have unfortunately been outside of our core scope, including facial recognition and the impact of bias within how platforms target content (considered in CDEI’s Review of online targeting).\n",
      "Our AI Monitoring function will continue to monitor the development of algorithmic decision-making and the extent to which new forms of discrimination or bias emerge. This will include referring issues to relevant regulators, and working with government if issues are not covered by existing regulations.\n",
      "Experience from this review suggests that many of the steps needed to address the risk of bias overlap with those for tackling other ethical challenges, for example structures for good governance, appropriate data sharing, and explainability of models. We anticipate that we will return to issues of bias, fairness and equality through much of our future work, though likely as one cross-cutting ethical issue in wider projects.\n",
      "If you are interested in knowing more about the projects listed above, or CDEI’s future work, please get in touch via bias@cdei.gov.uk .\n",
      "Appendix A\n",
      "Bias mitigation methods by stage of intervention and notion of fairness. Detailed references for each of these techniques can be found in Faculty’s “Bias identification and mitigation in decision-making algorithms”, published separately .\n",
      "Pre-processing In-processing Post-processing Demographic Parity     Data reweighting / resampling: * (Calders, Kamiran, and Pechenizkiy 2009) * (Faisal Kamiran and Calders 2012) Label modification: * (Calders, Kamiran, and Pechenizkiy 2009) * (Faisal Kamiran and Calders 2012) * (Luong, Ruggieri, and Turini 2011) Feature modification: * (Feldman et al. 2015) Optimal clustering / constrained optimisation: * (Zemel et al. 2013) * (Calmon et al. 2017) Auto-encoding: * (Louizos et al. 2016) Constrained optimisation: * (Corbett-Davies et al. 2017) * (Agarwal et al. 2018) * (Zafar, Valera, Rodriguez, et al. 2017) Regularisation: * (Kamishima et al. 2012) Naive Bayes/Balance models for each group: * (Calders and Verwer 2010) Naive Bayes/Training via modified labels: * (Calders and Verwer 2010) Tree-based splits adaptation: * (F. Kamiran, Calders, and Pechenizkiy 2010) Adversarial debiasing: * (Zhang et al. 2018) * (Adel et al. 2019) Naive Bayes/Modification of model probabilities: * (Calders and Verwer 2010) Tree-based leaves relabelling: * (F. Kamiran, Calders, and Pechenizkiy 2010) Label modification: * (Lohia et al. 2019) * (F. Kamiran, Karim, and Zhang 2012) Conditional Demographic Parity       Constrained optimisation: * (Corbett-Davies et al. 2017) Adversarial debiasing: * (Zhang et al. 2018) * (Adel et al. 2019) - by passing cond. variable to adversarial   Equalised Odds       Constrained optimisation: * (Corbett-Davies et al. 2017) (predictive equality) * (Agarwal et al. 2018) * (Zafar, Valera, Gomez Rodriguez, et al. 2017) * (Woodworth et al. 2017) Adversarial debiasing: * (Zhang et al. 2018) * (Adel et al. 2019) Decision threshold modification (ROC curve)/ constrained optimisation: * (Hardt, Price, and Srebro 2016; Woodworth et al. 2017) Calibration       Unconstrained optimisation: * (Corbett-Davies et al. 2017) Information Withholding: * (Pleiss et al. 2017) - achieves simultaneously a relaxation of Equalised Odds Individual Fairness     Optimal clustering / constrained: * (Zemel et al. 2013) Constrained optimisation: * (Dwork et al. 2012) * (Biega, Gummadi, and Weikum 2018) Label modification: * (Lohia et al. 2019) Counterfactual Fairness     Prediction via non-descendants in causal graph: * (Kusner et al. 2017)     Subgroup Fairness       Two-player zero-sum game: * (Kearns et al. 2018) * (Kearns et al. 2019)  \n",
      "Note that Roger Taylor, the chair of the CDEI Board, is also the chair of Ofqual, the English exams regulator. Following the controversy around August 2020 exam results, Roger has stepped away from involvement in any changes made to the final version of the review. CDEI has not had any direct role in assessing Ofqual’s approach, at the time of writing we understand a number of regulators are looking into the issues raised in detail.  ↩\n",
      "See, for example, IBM’s initiative around this here   ↩\n",
      "For avoidance of confusion, in place of the more neutral meaning often used in machine learning or other scientific literature (e.g. “to discriminate between”) we use “distinguish”.  ↩\n",
      "Note that in addition to discrimination the Equality Act also forbids victimisation and harassment, and places a requirement on organisations to make reasonable adjustments for people with disabilities, see Section 8.3 for more details.  ↩\n",
      "Equality and Human Rights Commission, ‘Words and terms used in the Equality Act’   ↩\n",
      "Note that public sector bodies in Scotland must address socio-economic inequalities in their decision-making under the Fairer Scotland Duty.  ↩\n",
      "There are of course many other sets of ethical principles and frameworks for AI from a variety of organisations, including various non-profit organisations, consultancies and the Council of Europe .  ↩\n",
      "Correll, Shelley J., and Stephen Benard. ‘Gender and racial bias in hiring. Memorandum report for University of Pennsylvania’ , 2006  ↩\n",
      "For a comprehensive analysis of different tools and the associated risks see Bogen, Miranda and Aaron Rieke, ‘Help wanted: an examination of hiring algorithms, equity, and bias.’ , Upturn, 2018  ↩\n",
      "CDEI’s recent review of online targeting covers this in more detail  ↩\n",
      "For an detailed report on the challenges and gaps related to auditing AI in recruitment, see the report from the Institute for the Future of Work, ‘Artificial intelligence in hiring: Assessing impacts on equality’ , 2020  ↩\n",
      "Crenshaw, Kimberlé. ‘Demarginalizing the intersection of race and sex: A black feminist critique of antidiscrimination doctrine, feminist theory and antiracist politics.’ in University of Chicago Legal Forum, Volume 1989, Issue 1, pp.139-167  ↩\n",
      "U.S. Equal Employment Opportunity Commission, Questions and Answers on EEOC Final Rule on Disparate Impact and “Reasonable Factors Other Than Age” Under the Age Discrimination in Employment Act of 1967   ↩\n",
      "Due to the focus of our review being bias, we were less concerned in our research with the accuracy of the tools involved. This is clearly an important question because if tools are ineffective, they are also arguably unethical however this sits outside the scope of this report.  ↩\n",
      "Carter, S., Mwaura, S., Ram, M., Trehan, K., and Jones, T., ‘Barriers to ethnic minority and women’s enterprise: Existing evidence, policy tensions and unsettled questions’, in International Small Business Journal, no.33, 2015, pp.49-69  ↩\n",
      "In the case of credit scoring, credit reference agency data tends to only go back six years, and lenders generally only look at the last few years, which should provide some mitigation against discriminatory lending practices from decades ago.  ↩\n",
      "The survey was sent to almost 300 firms and a total of 106 responses were received.  ↩\n",
      "Financial Conduct Authority and Bank of England, ‘Machine learning in UK financial services’ , 2019  ↩\n",
      "The term was coined in the early days of computing to describe the concept that nonsense input data produces nonsense output.  ↩\n",
      "Bank of England, Speech by James Proudman, ‘Managing machines: the governance of artificial intelligence’ , 2019  ↩\n",
      "Hurley, M., and Adebayo, J.; ‘Credit scoring in the era of big data.’, in Yale Journal of Law and Technology, Volume 18, Issue 1, 2016; pp.148-216  ↩\n",
      "Validation techniques including detecting errors and risks in the data.  ↩\n",
      "Royal Society for the encouragement of Arts, Manufactures and Commerce, ‘Artificial Intelligence: Real Public Engagement’ , 2018  ↩\n",
      "The Bank of England prudentially regulates and supervises financial services firms through the Prudential Regulation Authority   ↩\n",
      "Institutional Racism was defined in The Stephen Lawrence Inquiry as: “the collective failure of an organisation to provide an appropriate and professional service to people because of their colour, culture or ethnic origin” - The Stephen Lawrence Inquiry , 1999  ↩\n",
      "Policing codes, such as the College of Policing’s National Decision Model and Code of Ethics are key reference points for decision-making in policing.  ↩\n",
      "CDEI have published a research paper on facial recognition technology, which covers police use of live facial recognition technology, along with other uses.  ↩\n",
      "See for example: Richardson, Rashida and Schultz, Jason and Crawford, Kate, ‘Dirty Data, Bad Predictions: How Civil Rights Violations Impact Police Data, Predictive Policing Systems, and Justice’ , AI Now Institute; Kearns, Ian and Rick Muir, ‘Data Driven Policing and Public Value’, The Police Foundation , 2019; ‘Policing By Machine’ , Liberty, 2019  ↩\n",
      "RUSI sent Freedom of Information requests to all police forces in England and Wales, interviewed over 60 people from police forces, technology providers, academia, civil society, government, and regulation, and ran roundtables, jointly with CDEI and TechUK.  ↩\n",
      "Royal Society for the encouragement of Arts, Manufactures and Commerce, ‘Artificial Intelligence: Real Public Engagement’ , 2018  ↩\n",
      "‘Ipsos MORI Veracity Index 2019’ ; 76% survey respondents trust the police to tell the truth - an increase of 15ppt since 1983.  ↩\n",
      "Oxford Internet Institute, University of Oxford, ‘Data Science in Local Government’ , 2019  ↩\n",
      "Eubanks, Virginia. Automating inequality: How high-tech tools profile, police, and punish the poor. St. Martin’s Press, 2018.  ↩\n",
      "As outlined in Chapter 5 on the policing sector, we are considering the use of tools as part of the full decision-making process.  ↩\n",
      "The Guardian, ‘One in three councils using algorithms to make welfare decisions’ , 2019; and, Dencik, L. et al., ‘Data Scores as Governance: Investigating uses of citizen scoring in public services’ , Data Justice Lab, Cardiff University, 2018  ↩\n",
      "Kilbertus, N.; Gascon, A.; Kusner, M.; Veale, M.; Gummadi, K. P.; and Weller, A.; ‘Blind Justice: Fairness with Encrypted Sensitive Attributes’ . In the International Conference on Machine Learning (ICML), 2018  ↩\n",
      "See, for example, here and here   ↩\n",
      "With over 75% of respondents comfortable sharing information on age and ethnicity, and over 65% sharing disability, religious belief or sex information with new employers in order to test for and prevent unintentional bias in their algorithms.  ↩\n",
      "Kilbertus, N., Gascon, A., Kusner, M., Veale, M., Gummadi, K. P., and Weller, A., ‘Blind Justice: Fairness with Encrypted Sensitive Attributes’ . In the International Conference on Machine Learning (ICML), 2018  ↩\n",
      "Under contract ref 101579   ↩\n",
      "Note that in the machine learning literature on fairness, some terms used throughout this report take on specific, often narrower, definitions. Discrimination is sometimes used to refer to both different outcomes for different groups, and the statistical ability to distinguish between them. Bias is both favourable or unfavourable treatment of a group, and the statistical over or under-estimation of their quantitative properties. The field of study of how to create a mathematical system that is unbiased, is called “algorithmic fairness”. In this report we use “discrimination” and “bias” in the common language sense as defined in Chapter 2 (rather than their statistical meanings), and note that the concept of “fairness” discussed in this section is narrower than that described above.  ↩\n",
      "Calders, Toon, Kamiran, Faisal; and Pechenizkiy, Mykola; ‘Building Classifiers with Independency Constraints​’ , ICDMW ‘09: Proceedings of the 2009 IEEE International Conference on Data Mining Workshops, 2009; pp.13–18; and Zemel, Rich; Wu, Yu; Swersky, Kevin; Pitassi, Toni; and Dwork, Cynthia, ‘Learning Fair Representations, in ​International Conference on Machine Learning/’ ,​ 2013, pp.325–33  ↩\n",
      "Larson, Jeff; Mattu, Surya; Kirchner, Lauren; and Angwin, Julia; ‘How We Analyzed the COMPAS Recidivism Algorithm’ 2016; and Corbett-Davies, Sam; Pierson, Emma; Feller, Avi; Goel, Sharad; and Huq, Aziz; ‘Algorithmic decision-making and the Cost of Fairness’ , 2017  ↩\n",
      "Kusner, Matt J.; Joshua, Loftus R.; , Russell, Chris and Silva, Ricardo; ‘Counterfactual Fairness’ , in Advances in Neural Information Processing Systems, 2017; and Kilbertus, Niki; Rojas-Carulla, Mateo; Parascandolo, Giambattista; Hardt, Moritz; Janzing, Dominik; and Schölkopf, Bernhard, ‘Avoiding Discrimination through Causal Reasoning’ , in Advances in Neural Information Processing Systems, 2018  ↩\n",
      "Garg, Sahaj; Perot, Vincent; Limtiaco, Nicole; Taly, Ankur; Chi, Ed., and Beutel, Alex; ‘Counterfactual Fairness in Text Classification through Robustness’ , in AAAI/ACM Conference on AI, Ethics, and Society, 2019; and Chiappa, Silvia and Gillam, Thomas P. S.; ‘Path-Specific Counterfactual Fairness’ , in AAAI Conference on Artificial Intelligence, 2018; and Russell, Chris; Kusner, Matt J.; Loftus, Joshua and Ricardo Silva, ‘When Worlds Collide: Integrating Different Counterfactual Assumptions in Fairness’ , In Advances in Neural Information Processing Systems, edited by Guyon, I.; Luxburg, U. V.; Bengio, S.; Wallach, H.; Fergus, R.; Vishwanathan, S. and Garnett, R.; Curran Associates, Inc., 2017  ↩\n",
      "Kusner, Matt J.; Joshua, Loftus R.; , Russell, Chris and Silva, Ricardo; ‘Counterfactual Fairness’ , in Advances in Neural Information Processing Systems, 2017; and Liu, Lydia T.; Dean, Sarah; Rolf, Esther; Simchowitz, Max and Hardt, Moritz; ‘Delayed Impact of Fair Machine Learning’ , in International Conference on Machine Learning, 2018  ↩\n",
      "Hu, Lily; Immorlica, Nicole; Wortman Vaughan, Jennifer, ‘The Disparate Effects of Strategic Manipulation’ , in ACM Conference on Fairness, Accountability, and Transparency, 2018  ↩\n",
      "See, for example, many of those highlighted in the open source list curated by the Institute for Ethical AI and Machine Learning here   ↩\n",
      "Lord Sales, Justice of the UK Supreme Court, ‘Algorithms, Artificial Intelligence and the Law’ , The Sir Henry Brooke Lecture for BAILI, 2019  ↩\n",
      "For a more detailed discussion on direct and indirect discrimination, see Section 2.4  ↩\n",
      "See https://www.coe.int/en/web/artificial-intelligence/cahai , CDEI is providing expert input into this work.  ↩\n",
      "GDPR defines data processing broadly in Article 4(2): collection, recording, organisation, structuring, storage, adaptation or alteration, retrieval, consultation, use, disclosure by transmission, dissemination or otherwise making available, alignment or combination, restriction, erasure or destruction  ↩\n",
      "See Section 2.4 for detailed discussion on these types of unfair bias. In other cases, algorithms could lead to bias based on arbitrary characteristics. It would not be practical to address these issues through discrimination law, as these biases are based on characteristics that differ by algorithm, and may not be identified in advance.  ↩\n",
      "Case C 236/09 Test-Achats ECLI:EU:C:2011:100, see summary here   ↩\n",
      "Equality and Human Rights Commission, ‘Our powers’   ↩\n",
      "Equality and Human Rights Commission, Civil and political rights in Great Britain: Submission to the UN’ , 2020  ↩\n",
      "It is of course good practice to update impact assessments over time, and indeed GDPR requires DPIAs to be revisited when there is a change in the risk profile (see GDPR Article 35(11)), but there is not always a clear trigger point for an organisation to invest the time to do this.  ↩\n",
      "For further discussion of this issue, see Allen, R and Masters, D, 2020. Cloisters, September 2019, The Legal Education Foundation, In the matter of Automated Data Processing in Government Decision Making, available here   ↩\n",
      "See, for example, GOV.UK, Department of Health and Social Care, ‘Personal information charter’ ; and GOV.UK, Department for Work and Pensions, ‘Personal information charter ; and GOV.UK, Home Office, ‘Personal information charter’   ↩\n",
      "Oxford Internet Institute, University of Oxford, ‘Data Science in Local Government’ , 2019  ↩\n",
      "There are differing accounts. For example, an investigation by The Guardian last year showed some 140 of 408 councils in the UK are using privately-developed algorithmic ‘risk assessment’ tools, particularly to determine eligibility for benefits and to calculate entitlements; the New Statesman revealed that Experian secured £2m from British councils in 2018; and Data Justice Lab research in late 2018 showed 53 out of 96 local authorities and about a quarter of police authorities are now using algorithms for prediction, risk assessment and assistance in decision-making.  ↩\n",
      "Committee on Standards in Public Life, ‘Artificial Intelligence and Public Standards’ , 2020  ↩\n",
      "Vishwanath, T., Kaufmann, D.: Toward transparency: New approaches and their application to financial markets. The World Bank Research Observer 16(1), 2001; 41–57  ↩\n",
      "Mortier, R.; Haddadi, H.; Henderson, T.; McAuley, D.; Crowcroft, J.; ‘Human-data interaction: the human face of the data-driven society’ , 2014  ↩\n",
      "The Centre for Data Ethics and Innovation’s approach to the governance of data-driven technology   ↩\n",
      "O’Neill, Onora, ‘Reith 2002: A Question of Trust’, Open University , 2002  ↩\n"
     ]
    }
   ],
   "source": [
    "print(texts[filename][\"dragnet\"][\"doc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a150337f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preface\n",
      "\n",
      "Fairness is a highly prized human value. Societies in which individuals can flourish need to be held together by practices and institutions that are regarded as fair. What it means to be fair has been much debated throughout history, rarely more so than in recent months. Issues such as the global Black Lives Matter movement, the “levelling up” of regional inequalities within the UK, and the many complex questions of fairness raised by the COVID-19 pandemic have kept fairness and equality at the centre of public debate.\n",
      "\n",
      "Inequality and unfairness have complex causes, but bias in the decisions that organisations make about individuals is often a key aspect. The impact of efforts to address unfair bias in decision-making have often either gone unmeasured or have been painfully slow to take effect. However, decision-making is currently going through a period of change. Use of data and automation has existed in some sectors for many years, but it is currently expanding rapidly due to an explosion in the volumes of available data, and the increasing sophistication and accessibility of machine learning algorithms. Data gives us a powerful weapon to see where bias is occurring and measure whether our efforts to combat it are effective; if an organisation has hard data about differences in how it treats people, it can build insight into what is driving those differences, and seek to address them.\n",
      "\n",
      "However, data can also make things worse. New forms of decision-making have surfaced numerous examples where algorithms have entrenched or amplified historic biases; or even created new forms of bias or unfairness. Active steps to anticipate risks and measure outcomes are required to avoid this.\n",
      "\n",
      "Concern about algorithmic bias was the starting point for this policy review. When we began the work this was an issue of concern to a growing, but relatively small, number of people. As we publish this report, the issue has exploded into mainstream attention in the context of exam results, with a strong narrative that algorithms are inherently problematic. This highlights the urgent need for the world to do better in using algorithms in the right way: to promote fairness, not undermine it. Algorithms, like all technology, should work for people, and not against them.\n",
      "\n",
      "This is true in all sectors, but especially key in the public sector. When the state is making life-affecting decisions about individuals, that individual often can’t go elsewhere. Society may reasonably conclude that justice requires decision-making processes to be designed so that human judgement can intervene where needed to achieve fair and reasonable outcomes for each person, informed by individual evidence.\n",
      "\n",
      "As our work has progressed it has become clear that we cannot separate the question of algorithmic bias from the question of biased decision-making more broadly. The approach we take to tackling biased algorithms in recruitment, for example, must form part of, and be consistent with, the way we understand and tackle discrimination in recruitment more generally.\n",
      "\n",
      "A core theme of this report is that we now have the opportunity to adopt a more rigorous and proactive approach to identifying and mitigating bias in key areas of life, such as policing, social services, finance and recruitment. Good use of data can enable organisations to shine a light on existing practices and identify what is driving bias. There is an ethical obligation to act wherever there is a risk that bias is causing harm and instead make fairer, better choices.\n",
      "\n",
      "The risk is growing as algorithms, and the datasets that feed them, become increasingly complex. Organisations often find it challenging to build the skills and capacity to understand bias, or to determine the most appropriate means of addressing it in a data-driven world. A cohort of people is needed with the skills to navigate between the analytical techniques that expose bias and the ethical and legal considerations that inform best responses. Some organisations may be able to create this internally, others will want to be able to call on external experts to advise them. Senior decision-makers in organisations need to engage with understanding the trade-offs inherent in introducing an algorithm. They should expect and demand sufficient explainability of how an algorithm works so that they can make informed decisions on how to balance risks and opportunities as they deploy it into a decision-making process.\n",
      "\n",
      "Regulators and industry bodies need to work together with wider society to agree best practice within their industry and establish appropriate regulatory standards. Bias and discrimination are harmful in any context. But the specific forms they take, and the precise mechanisms needed to root them out, vary greatly between contexts. We recommend that there should be clear standards for anticipating and monitoring bias, for auditing algorithms and for addressing problems. There are some overarching principles, but the details of these standards need to be determined within each sector and use case. We hope that CDEI can play a key role in supporting organisations, regulators and government in getting this right.\n",
      "\n",
      "Lastly, society as a whole will need to be engaged in this process. In the world before AI there were many different concepts of fairness. Once we introduce complex algorithms to decision-making systems, that range of definitions multiplies rapidly. These definitions are often contradictory with no formula for deciding which is correct. Technical expertise is needed to navigate these choices, but the fundamental decisions about what is fair cannot be left to data scientists alone. They are decisions that can only be truly legitimate if society agrees and accepts them. Our report sets out how organisations might tackle this challenge.\n",
      "\n",
      "Transparency is key to helping organisations build and maintain public trust. There is a clear, and understandable, nervousness about the use and consequences of algorithms, exacerbated by the events of this summer. Being open about how and why algorithms are being used, and the checks and balances in place, is the best way to deal with this. Organisational leaders need to be clear that they retain accountability for decisions made by their organisations, regardless of whether an algorithm or a team of humans is making those decisions on a day-to-day basis.\n",
      "\n",
      "In this report we set out some key next steps for the government and regulators to support organisations to get their use of algorithms right, whilst ensuring that the UK ecosystem is set up to support good ethical innovation. Our recommendations are designed to produce a step change in the behaviour of all organisations making life changing decisions on the basis of data, however limited, and regardless of whether they used complex algorithms or more traditional methods.\n",
      "\n",
      "Enabling data to be used to drive better, fairer, more trusted decision-making is a challenge that countries face around the world. By taking a lead in this area, the UK, with its strong legal traditions and its centres of expertise in AI, can help to address bias and inequalities not only within our own borders but also across the globe.\n",
      "\n",
      "The Board of the Centre for Data Ethics and Innovation\n",
      "\n",
      "Executive summary\n",
      "\n",
      "Unfair biases, whether conscious or unconscious, can be a problem in many decision-making processes. This review considers the impact that an increasing use of algorithmic tools is having on bias in decision-making, the steps that are required to manage risks, and the opportunities that better use of data offers to enhance fairness. We have focused on the use of algorithms in significant decisions about individuals, looking across four sectors (recruitment, financial services, policing and local government), and making cross-cutting recommendations that aim to help build the right systems so that algorithms improve, rather than worsen, decision-making.\n",
      "\n",
      "It is well established that there is a risk that algorithmic systems can lead to biased decisions, with perhaps the largest underlying cause being the encoding of existing human biases into algorithmic systems. But the evidence is far less clear on whether algorithmic decision-making tools carry more or less risk of bias than previous human decision-making processes. Indeed, there are reasons to think that better use of data can have a role in making decisions fairer, if done with appropriate care.\n",
      "\n",
      "When changing processes that make life-affecting decisions about individuals we should always proceed with caution. It is important to recognise that algorithms cannot do everything. There are some aspects of decision-making where human judgement, including the ability to be sensitive and flexible to the unique circumstances of an individual, will remain crucial.\n",
      "\n",
      "Using data and algorithms in innovative ways can enable organisations to understand inequalities and to reduce bias in some aspects of decision-making. But there are also circumstances where using algorithms to make life-affecting decisions can be seen as unfair by failing to consider an individual’s circumstances, or depriving them of personal agency. We do not directly focus on this kind of unfairness in this report, but note that this argument can also apply to human decision-making, if the individual who is subject to the decision does not have a role in contributing to the decision.\n",
      "\n",
      "History to date in the design and deployment of algorithmic tools has not been good enough. There are numerous examples worldwide of the introduction of algorithms persisting or amplifying historical biases, or introducing new ones. We must and can do better. Making fair and unbiased decisions is not only good for the individuals involved, but it is good for business and society. Successful and sustainable innovation is dependent on building and maintaining public trust. Polling undertaken for this review suggested that, prior to August’s controversy over exam results, 57% of people were aware of algorithmic systems being used to support decisions about them, with only 19% of those disagreeing in principle with the suggestion of a “fair and accurate” algorithm helping to make decisions about them. By October, we found that awareness had risen slightly (to 62%), as had disagreement in principle (to 23%). This doesn’t suggest a step change in public attitudes, but there is clearly still a long way to go to build trust in algorithmic systems. The obvious starting point for this is to ensure that algorithms are trustworthy.\n",
      "\n",
      "The use of algorithms in decision-making is a complex area, with widely varying approaches and levels of maturity across different organisations and sectors. Ultimately, many of the steps needed to challenge bias will be context specific. But from our work, we have identified a number of concrete steps for industry, regulators and government to take that can support ethical innovation across a wide range of use cases. This report is not a guidance manual, but considers what guidance, support, regulation and incentives are needed to create the right conditions for fair innovation to flourish.\n",
      "\n",
      "It is crucial to take a broad view of the whole decision-making process when considering the different ways bias can enter a system and how this might impact on fairness. The issue is not simply whether an algorithm is biased, but whether the overall decision-making processes are biased. Looking at algorithms in isolation cannot fully address this.\n",
      "\n",
      "It is important to consider bias in algorithmic decision-making in the context of all decision-making systems. Even in human decision-making, there are differing views about what is and isn’t fair. But society has developed a range of standards and common practices for how to manage these issues, and legal frameworks to support this. Organisations have a level of understanding on what constitutes an appropriate level of due care for fairness. The challenge is to make sure that we can translate this understanding across to the algorithmic world, and apply a consistent bar of fairness whether decisions are made by humans, algorithms or a combination of the two. We must ensure decisions can be scrutinised, explained and challenged so that our current laws and frameworks do not lose effectiveness, and indeed can be made more effective over time.\n",
      "\n",
      "Significant growth is happening both in data availability and use of algorithmic decision-making across many sectors; we have a window of opportunity to get this right and ensure that these changes serve to promote equality, not to entrench existing biases.\n",
      "\n",
      "Sector reviews\n",
      "\n",
      "The four sectors studied in Part II of this report are at different maturity levels in their use of algorithmic decision-making. Some of the issues they face are sector-specific, but we found common challenges that span these sectors and beyond.\n",
      "\n",
      "In recruitment we saw a sector that is experiencing rapid growth in the use of algorithmic tools at all stages of the recruitment process, but also one that is relatively mature in collecting data to monitor outcomes. Human bias in traditional recruitment is well evidenced and therefore there is potential for data-driven tools to improve matters by standardising processes and using data to inform areas of discretion where human biases can creep in.\n",
      "\n",
      "However, we also found that a clear and consistent understanding of how to do this well is lacking, leading to a risk that algorithmic technologies will entrench inequalities. More guidance is needed on how to ensure that these tools do not unintentionally discriminate against groups of people, particularly when trained on historic or current employment data. Organisations must be particularly mindful to ensure they are meeting the appropriate legislative responsibilities around automated decision-making and reasonable adjustments for candidates with disabilities.\n",
      "\n",
      "The innovation in this space has real potential for making recruitment fairer. However, given the potential risks, further scrutiny of how these tools work, how they are used and the impact they have on different groups, is required, along with higher and clearer standards of good governance to ensure that ethical and legal risks are anticipated and managed.\n",
      "\n",
      "In financial services, we saw a much more mature sector that has long used data to support decision-making. Finance relies on making accurate predictions about peoples’ behaviours, for example how likely they are to repay debts. However, specific groups are historically underrepresented in the financial system, and there is a risk that these historic biases could be entrenched further through algorithmic systems. We found financial service organisations ranged from being highly innovative to more risk averse in their use of new algorithmic approaches. They are keen to test their systems for bias, but there are mixed views and approaches regarding how this should be done. This was particularly evident around the collection and use of protected characteristic data, and therefore organisations’ ability to monitor outcomes.\n",
      "\n",
      "Our main focus within financial services was on credit scoring decisions made about individuals by traditional banks. Our work found the key obstacles to further innovation in the sector included data availability, quality and how to source data ethically, available techniques with sufficient explainability, risk averse culture, in some parts, given the impacts of the financial crisis and difficulty in gauging consumer and wider public acceptance. The regulatory picture is clearer in financial services than in the other sectors we have looked at. The Financial Conduct Authority (FCA) is the main regulator and is showing leadership in prioritising work to understand the impact and opportunities of innovative uses of data and AI in the sector.\n",
      "\n",
      "The use of data from non-traditional sources could enable population groups who have historically found it difficult to access credit, due to lower availability of data about them from traditional sources, to gain better access in future. At the same time, more data and more complex algorithms could increase the potential for the introduction of indirect bias via proxy as well as the ability to detect and mitigate it.\n",
      "\n",
      "Adoption of algorithmic decision-making in the public sector is generally at an early stage. In policing, we found very few tools currently in operation in the UK, with a varied picture across different police forces, both on usage and approaches to managing ethical risks.\n",
      "\n",
      "There have been notable government reviews into the issue of bias in policing, which is important context when considering the risks and opportunities around the use of technology in this sector. Again, we found potential for algorithms to support decision-making, but this introduces new issues around the balance between security, privacy and fairness, and there is a clear requirement for strong democratic oversight.\n",
      "\n",
      "Police forces have access to more digital material than ever before, and are expected to use this data to identify connections and manage future risks. The £63.7 million funding for police technology programmes announced in January 2020 demonstrates the government’s drive for innovation. But clearer national leadership is needed. Though there is strong momentum in data ethics in policing at a national level, the picture is fragmented with multiple governance and regulatory actors, and no single body fully empowered or resourced to take ownership. The use of data analytics tools in policing carries significant risk. Without sufficient care, processes can lead to outcomes that are biased against particular groups, or systematically unfair. In many scenarios where these tools are helpful, there is still an important balance to be struck between automated decision-making and the application of professional judgement and discretion. Given the sensitivities in this area it is not sufficient for care to be taken internally to consider these issues; it is also critical that police forces are transparent in how such tools are being used to maintain public trust.\n",
      "\n",
      "In local government, we found an increased use of data to inform decision-making across a wide range of services. Whilst most tools are still in the early phase of deployment, there is an increasing demand for sophisticated predictive technologies to support more efficient and targeted services.\n",
      "\n",
      "By bringing together multiple data sources, or representing existing data in new forms, data-driven technologies can guide decision-makers by providing a more contextualised picture of an individual’s needs. Beyond decisions about individuals, these tools can help predict and map future service demands to ensure there is sufficient and sustainable resourcing for delivering important services.\n",
      "\n",
      "However, these technologies also come with significant risks. Evidence has shown that certain people are more likely to be overrepresented in data held by local authorities and this can then lead to biases in predictions and interventions. A related problem occurs when the number of people within a subgroup is small. Data used to make generalisations can result in disproportionately high error rates amongst minority groups.\n",
      "\n",
      "Data-driven tools present genuine opportunities for local government. However, tools should not be considered a silver bullet for funding challenges and in some cases additional investment will be required to realise their potential. Moreover, we found that data infrastructure and data quality were significant barriers to developing and deploying data-driven tools effectively and responsibly. Investment in this area is needed before developing more advanced systems.\n",
      "\n",
      "Sector-specific recommendations to regulators and government Most of the recommendations in this report are cross-cutting, but we identified the following recommendations specific to individual sectors. More details are given in sector chapters below. Recruitment Recommendation 1: The Equality and Human Rights Commission should update its guidance on the application of the Equality Act 2010 to recruitment, to reflect issues associated with the use of algorithms, in collaboration with consumer and industry bodies. Recommendation 2: The Information Commissioner’s Office should work with industry to understand why current guidance is not being consistently applied, and consider updates to guidance (e.g. in the Employment Practices Code), greater promotion of existing guidance, or other action as appropriate. Policing Recommendation 3: The Home Office should define clear roles and responsibilities for national policing bodies with regards to data analytics and ensure they have access to appropriate expertise and are empowered to set guidance and standards. As a first step, the Home Office should ensure that work underway by the National Police Chiefs’ Council and other policing stakeholders to develop guidance and ensure ethical oversight of data analytics tools is appropriately supported. Local government Recommendation 4: Government should develop national guidance to support local authorities to legally and ethically procure or develop algorithmic decision-making tools in areas where significant decisions are made about individuals, and consider how compliance with this guidance should be monitored.\n",
      "\n",
      "Addressing the challenges\n",
      "\n",
      "We found underlying challenges across the four sectors, and indeed other sectors where algorithmic decision-making is happening. In Part III of this report, we focus on understanding these challenges, where the ecosystem has got to on addressing them, and the key next steps for organisations, regulators and government. The main areas considered are:\n",
      "\n",
      "The enablers needed by organisations building and deploying algorithmic decision-making tools to help them do this in a fair way, see Chapter 7.\n",
      "\n",
      "The regulatory levers, both formal and informal, needed to incentivise organisations to do this, and create a level playing field for ethical innovation see Chapter 8.\n",
      "\n",
      "How the public sector, as a major developer and user of data-driven technology, can show leadership in this area through transparency see Chapter 9.\n",
      "\n",
      "There are inherent links between these areas. Creating the right incentives can only succeed if the right enablers are in place to help organisations act fairly, but conversely, there is little incentive for organisations to invest in tools and approaches for fair decision-making if there is insufficient clarity on expected norms. We want a system that is fair and accountable; one that preserves, protects or improves fairness in decisions being made with the use of algorithms. We want to address the obstacles that organisations may face to innovate ethically, to ensure the same or increased levels of accountability for these decisions and how society can identify and respond to bias in algorithmic decision-making processes. We have considered the existing landscape of standards and laws in this area, and whether they are sufficient for our increasingly data-driven society.\n",
      "\n",
      "To realise this vision we need clear mechanisms for safe access to data to test for bias; organisations that are able to make judgements based on data about bias; a skilled industry of third parties who can provide support and assurance, and regulators equipped to oversee and support their sectors and remits through this change.\n",
      "\n",
      "Enabling fair innovation\n",
      "\n",
      "We found that many organisations are aware of the risks of algorithmic bias, but are unsure how to address bias in practice.\n",
      "\n",
      "There is no universal formulation or rule that can tell you an algorithm is fair. Organisations need to identify what fairness objectives they want to achieve and how they plan to do this. Sector bodies, regulators, standards bodies and the government have a key role in setting out clear guidelines on what is appropriate in different contexts; getting this right is essential not only for avoiding bad practice, but for giving the clarity that enables good innovation. However, all organisations need to be clear about their own accountability for getting it right. Whether an algorithm or a structured human process is being used to make a decision doesn’t change an organisation’s accountability.\n",
      "\n",
      "Improving diversity across a range of roles involved in the development and deployment of algorithmic decision-making tools is an important part of protecting against bias. Government and industry efforts to improve this must continue, and need to show results.\n",
      "\n",
      "Data is needed to monitor outcomes and identify bias, but data on protected characteristics is not available often enough. One reason for this is an incorrect belief that data protection law prevents collection or usage of this data. Indeed, there are a number of lawful bases in data protection legislation for using protected or special characteristic data when monitoring or addressing discrimination. But there are some other genuine challenges in collecting this data, and more innovative thinking is needed in this area; for example around the potential for trusted third party intermediaries.\n",
      "\n",
      "The machine learning community has developed multiple techniques to measure and mitigate algorithmic bias. Organisations should be encouraged to deploy methods that address bias and discrimination. However, there is little guidance on how to choose the right methods, or how to embed them into development and operational processes. Bias mitigation cannot be treated as a purely technical issue; it requires careful consideration of the wider policy, operational and legal contexts. There is insufficient legal clarity concerning novel techniques in this area. Many can be used legitimately, but care is needed to ensure that the application of some techniques does not cross into unlawful positive discrimination.\n",
      "\n",
      "Recommendations to government Recommendation 5: Government should continue to support and invest in programmes that facilitate greater diversity within the technology sector, building on its current programmes and developing new initiatives where there are gaps. Recommendation 6: Government should work with relevant regulators to provide clear guidance on the collection and use of protected characteristic data in outcome monitoring and decision-making processes. They should then encourage the use of that guidance and data to address current and historic bias in key sectors. Recommendation 7: Government and the Office for National Statistics (ONS) should open the Secure Research Service more broadly, to a wider variety of organisations, for use in evaluation of bias and inequality across a greater range of activities. Recommendation 8: Government should support the creation and development of data-focused public and private partnerships, especially those focused on the identification and reduction of biases and issues specific to under-represented groups. The Office for National Statistics (ONS) and Government Statistical Service should work with these partnerships and regulators to promote harmonised principles of data collection and use into the private sector, via shared data and standards development. Recommendations to regulators Recommendation 9: Sector regulators and industry bodies should help create oversight and technical guidance for responsible bias detection and mitigation in their individual secin individual sectors, adding context-specific detail to the existing cross-cutting guidance on data protection, and any new cross-cutting guidance on the Equality Act.\n",
      "\n",
      "Good, anticipatory governance is crucial here. Many of the high profile cases of algorithmic bias could have been anticipated with careful evaluation and mitigation of the potential risks. Organisations need to make sure that the right capabilities and structures are in place to ensure that this happens both before algorithms are introduced into decision-making processes, and through their life. Doing this well requires understanding of, and empathy for, the expectations of those who are affected by decisions, which can often only be achieved through the right engagement with groups. Given the complexity of this area, we expect to see a growing role for expert professional services supporting organisations. Although the ecosystem needs to develop further, there is already plenty that organisations can and should be doing to get this right. Data Protection Impact Assessments and Equality Impact Assessments can help with structuring thinking and documenting the steps taken.\n",
      "\n",
      "Guidance to organisation leaders and boards Those responsible for governance of organisations deploying or using algorithmic decision-making tools to support significant decisions about individuals should ensure that leaders are in place with accountability for: Understanding the capabilities and limits of those tools\n",
      "\n",
      "Considering carefully whether individuals will be fairly treated by the decision-making process that the tool forms part of\n",
      "\n",
      "Making a conscious decision on appropriate levels of human involvement in the decision-making process\n",
      "\n",
      "Putting structures in place to gather data and monitor outcomes for fairness\n",
      "\n",
      "Understanding their legal obligations and having carried out appropriate impact assessments This especially applies in the public sector when citizens often do not have a choice about whether to use a service, and decisions made about individuals can often be life-affecting.\n",
      "\n",
      "The regulatory environment\n",
      "\n",
      "Clear industry norms, and good, proportionate regulation, are key both for addressing risks of algorithmic bias, and for promoting a level playing field for ethical innovation to thrive.\n",
      "\n",
      "The increased use of algorithmic decision-making presents genuinely new challenges for regulation, and brings into question whether existing legislation and regulatory approaches can address these challenges sufficiently well. There is currently limited case law or statutory guidance directly addressing discrimination in algorithmic decision-making, and the ecosystems of guidance and support are at different maturity levels in different sectors.\n",
      "\n",
      "Though there is only a limited amount of case law, the recent judgement of the Court of Appeal in relation to the usage of live facial recognition technology by South Wales Police seems likely to be significant. One of the grounds for successful appeal was that South Wales Police failed to adequately consider whether their trial could have a discriminatory impact, and specifically that they did not take reasonable steps to establish whether their facial recognition software contained biases related to race or sex. In doing so, the court found that they did not meet their obligations under the Public Sector Equality Duty, even though there was no evidence that this specific algorithm was biased. This suggests a general duty for public sector organisations to take reasonable steps to consider any potential impact on equality upfront and to detect algorithmic bias on an ongoing basis. The current regulatory landscape for algorithmic decision-making consists of the Equality and Human Rights Commission (EHRC), the Information Commissioner’s Office (ICO) and sector regulators. At this stage, we do not believe that there is a need for a new specialised regulator or primary legislation to address algorithmic bias.\n",
      "\n",
      "However, algorithmic bias means the overlap between discrimination law, data protection law and sector regulations is becoming increasingly important. We see this overlap playing out in a number of contexts, including discussions around the use of protected characteristics data to measure and mitigate algorithmic bias, the lawful use of bias mitigation techniques, identifying new forms of bias beyond existing protected characteristics. The first step in resolving these challenges should be to clarify the interpretation of the law as it stands, particularly the Equality Act 2010, both to give certainty to organisations deploying algorithms and to ensure that existing individual rights are not eroded, and wider equality duties are met. However, as use of algorithmic decision-making grows further, we do foresee a future need to look again at the legislation itself, which should be kept under consideration as guidance is developed and case law evolves.\n",
      "\n",
      "Existing regulators need to adapt their enforcement to algorithmic decision-making, and provide guidance on how regulated bodies can maintain and demonstrate compliance in an algorithmic age. Some regulators require new capabilities to enable them to respond effectively to the challenges of algorithmic decision-making. While larger regulators with a greater digital remit may be able to grow these capabilities in-house, others will need external support. Many regulators are working hard to do this, and the ICO has shown leadership in this area both by starting to build a skills base to address these new challenges, and in convening other regulators to consider issues arising from AI. Deeper collaboration across the regulatory ecosystem is likely to be needed in future.\n",
      "\n",
      "Outside of the formal regulatory environment, there is increasing awareness within the private sector of the demand for a broader ecosystem of industry standards and professional services to help organisations address algorithmic bias. There are a number of reasons for this: it is a highly specialised skill that not all organisations will be able to support, it will be important to have consistency in how the problem is addressed, and because regulatory standards in some sectors may require independent audit of systems. Elements of such an ecosystem might be licenced auditors or qualification standards for individuals with the necessary skills. Audit of bias is likely to form part of a broader approach to audit that might also cover issues such as robustness and explainability. Government, regulators, industry bodies and private industry will all play important roles in growing this ecosystem so that organisations are better equipped to make fair decisions.\n",
      "\n",
      "Recommendations to government Recommendation 10: Government should issue guidance that clarifies the Equality Act responsibilities of organisations using algorithmic decision-making. This should include guidance on the collection of protected characteristics data to measure bias and the lawfulness of bias mitigation techniques. Recommendation 11: Though the development of this guidance and its implementation, government should assess whether it provides both sufficient clarity for organisations on meeting their obligations, and leaves sufficient scope for organisations to take actions to mitigate algorithmic bias. If not, government should consider new regulations or amendments to the Equality Act to address this. Recommendations to regulators Recommendation 12: The EHRC should ensure that it has the capacity and capability to investigate algorithmic discrimination. This may include EHRC reprioritising resources to this area, EHRC supporting other regulators to address algorithmic discrimination in their sector, and additional technical support to the EHRC. Recommendation 13: Regulators should consider algorithmic discrimination in their supervision and enforcement activities, as part of their responsibilities under the Public Sector Equality Duty. Recommendation 14: Regulators should develop compliance and enforcement tools to address algorithmic bias, such as impact assessments, audit standards, certification and/or regulatory sandboxes. Recommendation 15: Regulators should coordinate their compliance and enforcement efforts to address algorithmic bias, aligning standards and tools where possible. This could include jointly issued guidance, collaboration in regulatory sandboxes, and joint investigations.\n",
      "\n",
      "Public sector transparency\n",
      "\n",
      "Making decisions about individuals is a core responsibility of many parts of the public sector, and there is increasing recognition of the opportunities offered through the use of data and algorithms in decision-making. The use of technology should never reduce real or perceived accountability of public institutions to citizens. In fact, it offers opportunities to improve accountability and transparency, especially where algorithms have significant effects on significant decisions about individuals.\n",
      "\n",
      "A range of transparency measures already exist around current public sector decision-making processes; both proactive sharing of information about how decisions are made, and reactive rights for citizens to request information on how decisions were made about them. The UK government has shown leadership in setting out guidance on AI usage in the public sector, including a focus on techniques for explainability and transparency. However, more is needed to make transparency about public sector use of algorithmic decision-making the norm. There is a window of opportunity to ensure that we get this right as adoption starts to increase, but it is sometimes hard for individual government departments or other public sector organisations to be first in being transparent; a strong central drive for this is needed.\n",
      "\n",
      "The development and delivery of an algorithmic decision-making tool will often include one or more suppliers, whether acting as technology suppliers or business process outsourcing providers. While the ultimate accountability for fair decision-making always sits with the public body, there is limited maturity or consistency in contractual mechanisms to place responsibilities in the right place in the supply chain. Procurement processes should be updated in line with wider transparency commitments to ensure standards are not lost along the supply chain.\n",
      "\n",
      "Recommendations to government Recommendation 16: Government should place a mandatory transparency obligation on all public sector organisations using algorithms that have a significant influence on significant decisions affecting individuals. Government should conduct a project to scope this obligation more precisely, and to pilot an approach to implement it, but it should require the proactive publication of information on how the decision to use an algorithm was made, the type of algorithm, how it is used in the overall decision-making process, and steps taken to ensure fair treatment of individuals. Recommendation 17: Cabinet Office and the Crown Commercial Service should update model contracts and framework agreements for public sector procurement to incorporate a set of minimum standards around ethical use of AI, with particular focus on expected levels of transparency and explainability, and ongoing testing for fairness.\n",
      "\n",
      "Next steps and future challenges\n",
      "\n",
      "This review has considered a complex and rapidly evolving field. There is plenty to do across industry, regulators and government to manage the risks and maximise the benefits of algorithmic decision-making. Some of the next steps fall within CDEI’s remit, and we are happy to support industry, regulators and government in taking forward the practical delivery work to address the issues we have identified and future challenges which may arise. Outside of specific activities, and noting the complexity and range of the work needed across multiple sectors, we see a key need for national leadership and coordination to ensure continued focus and pace in addressing these challenges across sectors. This is a rapidly moving area. A level of coordination and monitoring will be needed to assess how organisations building and using algorithmic decision-making tools are responding to the challenges highlighted in this report, and to the proposed new guidance from regulators and government. Government should be clear on where it wants this coordination to sit; for example in central government directly, in a specific regulator or in CDEI.\n",
      "\n",
      "In this review we have concluded that there is significant scope to address the risks posed by bias in algorithmic decision-making within the law as it stands, but if this does not succeed then there is a clear possibility that future legislation may be required. We encourage organisations to respond to this challenge; to innovate responsibly and think through the implications for individuals and society at large as they do so.\n",
      "\n",
      "Part I: Introduction\n",
      "\n",
      "1. Background and scope\n",
      "\n",
      "1.1 About CDEI\n",
      "\n",
      "The adoption of data-driven technology affects every aspect of our society and its use is creating opportunities as well as new ethical challenges.\n",
      "\n",
      "The Centre for Data Ethics and Innovation (CDEI) is an independent expert committee, led by a board of specialists, set up and tasked by the UK government to investigate and advise on how we maximise the benefits of these technologies.\n",
      "\n",
      "Our goal is to create the conditions in which ethical innovation can thrive: an environment in which the public are confident their values are reflected in the way data-driven technology is developed and deployed; where we can trust that decisions informed by algorithms are fair; and where risks posed by innovation are identified and addressed.\n",
      "\n",
      "More information about CDEI can be found at www.gov.uk/cdei.\n",
      "\n",
      "1.2 About this review\n",
      "\n",
      "In the October 2018 Budget, the Chancellor announced that we would investigate the potential bias in decisions made by algorithms. This review formed a key part of our 2019/2020 work programme, though completion was delayed by the onset of COVID-19. This is the final report of CDEI’s review and includes a set of formal recommendations to the government.\n",
      "\n",
      "Government tasked us to draw on expertise and perspectives from stakeholders across society to provide recommendations on how they should address this issue. We also provide advice for regulators and industry, aiming to support responsible innovation and help build a strong, trustworthy system of governance. The government has committed to consider and respond publicly to our recommendations.\n",
      "\n",
      "1.3 Our focus\n",
      "\n",
      "The use of algorithms in decision-making is increasing across multiple sectors of our society. Bias in algorithmic decision-making is a broad topic, so in this review, we have prioritised the types of decisions where potential bias seems to represent a significant and imminent ethical risk.\n",
      "\n",
      "This has led us to focus on:\n",
      "\n",
      "Areas where algorithms have the potential to make or inform a decision that directly affects an individual human being (as opposed to other entities, such as companies). The significance of decisions of course varies, and we have typically focused on areas where individual decisions could have a considerable impact on a person’s life, i.e. decisions that are significant in the sense of the Data Protection Act 2018.\n",
      "\n",
      "The extent to which algorithmic decision-making is being used now, or is likely to be soon, in different sectors.\n",
      "\n",
      "Decisions made or supported by algorithms, and not wider ethical issues in the use of artificial intelligence.\n",
      "\n",
      "The changes in ethical risk in an algorithmic world as compared to an analogue world.\n",
      "\n",
      "Circumstances where decisions are biased (see Chapter 2 for a discussion of what this means), rather than other forms of unfairness such as arbitrariness or unreasonableness.\n",
      "\n",
      "This scope is broad, but it doesn’t cover all possible areas where algorithmic bias can be an issue. For example, the CDEI Review of online targeting, published earlier this year, highlighted the risk of harm through bias in targeting within online platforms. These are decisions which are individually very small, for example on targeting an advert or recommending content to a user, but the overall impact of bias across many small decisions can still be problematic. This review did touch on these issues, but they fell outside of our core focus on significant decisions about individuals.\n",
      "\n",
      "It is worth highlighting that the main work of this review was carried out before a number of highly relevant events in mid 2020; the COVID-19 pandemic, Black Lives Matter, the awarding of exam results without exams, and (with less widespread attention, but very specific relevance) the judgement of the Court of Appeal in Bridges v South Wales Police. We have considered links to these issues in our review, but have not been able to treat them in full depth.[footnote 1]\n",
      "\n",
      "1.4 Our approach\n",
      "\n",
      "Sector approach\n",
      "\n",
      "The ethical questions in relation to bias in algorithmic decision-making vary depending on the context and sector. We chose four initial areas of focus to illustrate the range of issues. These were recruitment, financial services, policing and local government. Our rationale for choosing these sectors is set out in the introduction to Part II.\n",
      "\n",
      "Cross-sector themes\n",
      "\n",
      "From the work we carried out on the four sectors, as well as our engagement across government, civil society, academia and interested parties in other sectors, we were able to identify themes, issues and opportunities that went beyond the individual sectors.\n",
      "\n",
      "We set out three key cross-cutting questions in our interim report, which we have sought to address on a cross-sector basis:\n",
      "\n",
      "1. Data: Do organisations and regulators have access to the data they require to adequately identify and mitigate bias?\n",
      "\n",
      "2. Tools and techniques: What statistical and technical solutions are available now or will be required in future to identify and mitigate bias and which represent best practice?\n",
      "\n",
      "3. Governance: Who should be responsible for governing, auditing and assuring these algorithmic decision-making systems?\n",
      "\n",
      "These questions have guided the review. While we have made sector-specific recommendations where appropriate, our recommendations focus more heavily on opportunities to address these questions (and others) across multiple sectors.\n",
      "\n",
      "Evidence\n",
      "\n",
      "Our evidence base for this final report is informed by a variety of work including:\n",
      "\n",
      "A landscape summary led by Professor Michael Rovatsos of the University of Edinburgh, which assessed the current academic and policy literature.\n",
      "\n",
      "An open call for evidence which received responses from a wide cross section of academic institutions and individuals, civil society, industry and the public sector.\n",
      "\n",
      "A series of semi-structured interviews with companies in the financial services and recruitment sectors developing and using algorithmic tools.\n",
      "\n",
      "Work with the Behavioural Insights Team on attitudes to the use of algorithms in personal banking.[footnote 2]\n",
      "\n",
      "Commissioned research from the Royal United Services Institute (RUSI) on data analytics in policing in England and Wales.[footnote 3]\n",
      "\n",
      "Contracted work by Faculty on technical bias mitigation techniques.[footnote 4]\n",
      "\n",
      "Representative polling on public attitudes to a number of the issues raised in this report, conducted by Deltapoll as part of CDEI’s ongoing public engagement work.\n",
      "\n",
      "Meetings with a variety of stakeholders including regulators, industry groups, civil society organisations, academics and government departments, as well as desk-based research to understand the existing technical and policy landscape.\n",
      "\n",
      "2. The issue\n",
      "\n",
      "Summary Algorithms are structured processes, which have long been used to aid human decision-making. Recent developments in machine learning techniques and exponential growth in data has allowed for more sophisticated and complex algorithmic decisions, and there has been corresponding growth in usage of algorithm supported decision-making across many areas of society.\n",
      "\n",
      "This growth has been accompanied by significant concerns about bias ; that the use of algorithms can cause a systematic skew in decision-making that results in unfair outcomes. There is clear evidence that algorithmic bias can occur, whether through entrenching previous human biases or introducing new ones.\n",
      "\n",
      "Some forms of bias constitute discrimination under the Equality Act 2010, namely when bias leads to unfair treatment based on certain protected characteristics. There are also other kinds of algorithmic bias that are non-discriminatory, but still lead to unfair outcomes.\n",
      "\n",
      "There are multiple concepts of fairness , some of which are incompatible and many of which are ambiguous. In human decisions we can often accept this ambiguity and allow for human judgement to consider complex reasons for a decision. In contrast, algorithms are unambiguous.\n",
      "\n",
      "Fairness is about much more than the absence of bias : fair decisions need to also be non-arbitrary, reasonable, consider equality implications and respect the circumstances and personal agency of the individuals concerned.\n",
      "\n",
      "Despite concerns about ‘black box’ algorithms, in some ways algorithms can be more transparent than human decisions; unlike a human it is possible to reliably test how an algorithm responds to changes in parts of the input. There are opportunities to deploy algorithmic decision-making transparently, and enable the identification and mitigation of systematic bias in ways that are challenging with humans. Human developers and users of algorithms must decide the concepts of fairness that apply to their context, and ensure that algorithms deliver fair outcomes.\n",
      "\n",
      "Fairness through unawareness is often not enough to prevent bias : ignoring protected characteristics is insufficient to prevent algorithmic bias and it can prevent organisations from identifying and addressing bias.\n",
      "\n",
      "The need to address algorithmic bias goes beyond regulatory requirements under equality and data protection law. It is also critical for innovation that algorithms are used in a way that is both fair, and seen by the public to be fair.\n",
      "\n",
      "2.1 Introduction\n",
      "\n",
      "Human decision-making has always been flawed, shaped by individual or societal biases that are often unconscious. Over the years, society has identified ways of improving it, often by building processes and structures that encourage us to make decisions in a fairer and more objective way, from agreed social norms to equality legislation. However, new technology is introducing new complexities. The growing use of algorithms in decision-making has raised concerns around bias and fairness.\n",
      "\n",
      "Even in this data-driven context, the challenges are not new. In 1988, the UK Commission for Racial Equality found a British medical school guilty of algorithmic discrimination when inviting applicants to interview.[footnote 5] The computer program they had used was determined to be biased against both women and applicants with non-European names.\n",
      "\n",
      "The growth in this area has been driven by the availability and volume of (often personal) data that can be used to train machine learning models, or as inputs into decisions, as well as cheaper and easier availability of computing power, and innovations in tools and techniques. As usage of algorithmic tools grows, so does their complexity. Understanding the risks is therefore crucial to ensure that these tools have a positive impact and improve decision-making.\n",
      "\n",
      "Algorithms have different but related vulnerabilities to human decision-making processes. They can be more able to explain themselves statistically, but less able to explain themselves in human terms. They are more consistent than humans but are less able to take nuanced contextual factors into account. They can be highly scalable and efficient, but consequently capable of consistently applying errors to very large populations. They can also act to obscure the accountabilities and liabilities that individual people or organisations have for making fair decisions.\n",
      "\n",
      "2.2 The use of algorithms in decision-making\n",
      "\n",
      "In simple terms, an algorithm is a structured process. Using structured processes to aid human decision-making is much older than computation. Over time, the tools and approaches available to deploy such decision-making have become more sophisticated. Many organisations responsible for making large numbers of structured decisions (for example, whether an individual qualifies for a welfare benefits payment, or whether a bank should offer a customer a loan), make these processes scalable and consistent by giving their staff well-structured processes and rules to follow. Initial computerisation of such decisions took a similar path, with humans designing structured processes (or algorithms) to be followed by a computer handling an application.\n",
      "\n",
      "However, technology has reached a point where the specifics of those decision-making processes are not always explicitly manually designed. Machine learning tools often seek to find patterns in data without requiring the developer to specify which factors to use or how exactly to link them, before formalising relationships or extracting information that could be useful to make decisions. The results of these tools can be simple and intuitive for humans to understand and interpret, but they can also be highly complex.\n",
      "\n",
      "Some sectors, such as credit scoring and insurance, have a long history of using statistical techniques to inform the design of automated processes based on historical data. An ecosystem has evolved that helps to manage some of the potential risks, for example credit reference agencies offer customers the ability to see their own credit history, and offer guidance on the factors that can affect credit scoring. In these cases, there are a range of UK regulations that govern the factors that can and cannot be used.\n",
      "\n",
      "We are now seeing the application of data-driven decision-making in a much wider range of scenarios. There are a number of drivers for this increase, including:\n",
      "\n",
      "The exponential growth in the amount of data held by organisations, which makes more decision-making processes amenable to data-driven approaches.\n",
      "\n",
      "Improvements in the availability and cost of computing power and skills.\n",
      "\n",
      "Increased focus on cost saving, driven by fiscal constraints in the public sector, and competition from disruptive new entrants in many private sector markets.\n",
      "\n",
      "Advances in machine learning techniques, especially deep neural networks, that have rapidly brought many problems previously inaccessible to computers into routine everyday use (e.g. image and speech recognition). In simple terms, an algorithm is a set of instructions designed to perform a specific task. In algorithmic decision-making, the word is applied in two different contexts:\n",
      "\n",
      "A machine learning algorithm takes data as an input to create a model. This can be a one-off process, or something that happens continually as new data is gathered.\n",
      "\n",
      "takes data as an input to create a This can be a one-off process, or something that happens continually as new data is gathered. Algorithm can also be used to describe a structured process for making a decision, whether followed by a human or computer, and possibly incorporating a machine learning model.\n",
      "\n",
      "The usage is usually clear from context. In this review we are focused mainly on decision-making processes involving machine learning algorithms, although some of the content is also relevant to other structured decision-making processes. Note that there is no hard definition of exactly which statistical techniques and algorithms constitute novel machine learning. We have observed that many recent developments are associated with applying existing statistical techniques more widely in new sectors, not about novel techniques.\n",
      "\n",
      "We interpret algorithmic decision-making to include any decision-making process where an algorithm makes, or meaningfully assists, the decision. This includes what is sometimes referred to as algorithmically-assisted decision-making. In this review we are focused mainly on decisions about individual people.\n",
      "\n",
      "Figure 1 below shows an example of how a machine learning algorithm can be used within a decision-making process, such as a bank making a decision on whether to offer a loan to an individual.\n",
      "\n",
      "Machine-learning algorithms can be used within decision-making processes. First, a set of data is gathered, for example a collection of input data from historical applications for a service (e.g. a loan) along with the decisions reached, and any data on whether those outcomes were the right ones (e.g. was the loan repaid). A human decides what data to make available to the model. Second, A machine learning algorithm is chosen, and uses historical data (e.g. a set of past input data (e.g. a set of past input data, the decisions reached) to build a model, optimising against a set of criteria specified by a human. The model can take a number of different forms in different machine learning techniques, but might be a weighted average of a number of a number of input data fields, or a complex structured decision tree. Third, the resulting model is then used repeatedly as part of the decision-making process, either to make an automated decision, or to offer guidance to a human making the final decision. A human can be involved at this stage to vet the machine-learning model’s outputs and make judgements about how to incorporate this information into a final decision. Fourth, new input data and associated decisions can be fed back into the data set to enable the model to be updated (either periodically or continuously).\n",
      "\n",
      "Figure 1: How data and algorithms come together to support decision-making\n",
      "\n",
      "It is important to emphasise that algorithms often do not represent the complete decision-making process. There may be elements of human judgement, exceptions treated outside of the usual process and opportunities for appeal or reconsideration. In fact, for significant decisions, an appropriate provision for human review will usually be required to comply with data protection law. Even before an algorithm is deployed into a decision-making process, it is humans that decide on the objectives it is trying to meet, the data available to it, and how the output is used.\n",
      "\n",
      "It is therefore critical to consider not only the algorithmic aspect, but the whole decision-making process that sits around it. Human intervention in these processes will vary, and in some cases may be absent entirely in fully automated systems. Ultimately the aim is not just to avoid bias in algorithmic aspects of a process, but that the process as a whole achieves fair decision-making.\n",
      "\n",
      "2.3 Bias\n",
      "\n",
      "As algorithmic decision-making grows in scale, increasing concerns are being raised around the risks of bias. Bias has a precise meaning in statistics, referring to a systematic skew in results, that is an output that is not correct on average with respect to the overall population being sampled.\n",
      "\n",
      "However in general usage, and in this review, bias is used to refer to an output that is not only skewed, but skewed in a way that is unfair (see below for a discussion on what unfair might mean in this context).\n",
      "\n",
      "Bias can enter algorithmic decision-making systems in a number of ways, including:\n",
      "\n",
      "Historical bias: The data that the model is built, tested and operated on could introduce bias. This may be because of previously biased human decision-making or due to societal or historical inequalities. For example, if a company’s current workforce is predominantly male then the algorithm may reinforce this, whether the imbalance was originally caused by biased recruitment processes or other historical factors. If your criminal record is in part a result of how likely you are to be arrested (as compared to someone else with the same history of behaviour, but not arrests), an algorithm constructed to assess risk of reoffending is at risk of not reflecting the true likelihood of reoffending, but instead reflects the more biased likelihood of being caught reoffending.\n",
      "\n",
      "Data selection bias: How the data is collected and selected could mean it is not representative. For example, over or under recording of particular groups could mean the algorithm was less accurate for some people, or gave a skewed picture of particular groups. This has been the main cause of some of the widely reported problems with accuracy of some facial recognition algorithms across different ethnic groups, with attempts to address this focusing on ensuring a better balance in training data.[footnote 6]\n",
      "\n",
      "Algorithmic design bias: It may also be that the design of the algorithm leads to introduction of bias. For example, CDEI’s Review of online targeting noted examples of algorithms placing job advertisements online designed to optimise for engagement at a given cost, leading to such adverts being more frequently targeted at men because women are more costly to advertise to.\n",
      "\n",
      "Human oversight is widely considered to be a good thing when algorithms are making decisions, and mitigates the risk that purely algorithmic processes cannot apply human judgement to deal with unfamiliar situations. However, depending on how humans interpret or use the outputs of an algorithm, there is also a risk that bias re-enters the process as the human applies their own conscious or unconscious biases to the final decision.\n",
      "\n",
      "There is also risk that bias can be amplified over time by feedback loops, as models are incrementally re-trained on new data generated, either fully or partly, via use of earlier versions of the model in decision-making. For example, if a model predicting crime rates based on historical arrest data is used to prioritise police resources, then arrests in high risk areas could increase further, reinforcing the imbalance. CDEI’s Landscape summary discusses this issue in more detail.\n",
      "\n",
      "2.4 Discrimination and equality\n",
      "\n",
      "In this report we use the word discrimination in the sense defined in the Equality Act 2010, meaning unfavourable treatment on the basis of a protected characteristic.[footnote 7]\n",
      "\n",
      "The Equality Act 2010[footnote 8] makes it unlawful to discriminate against someone on the basis of certain protected characteristics (for example age, race, sex, disability) in public functions, employment and the provision of goods and services.\n",
      "\n",
      "The choice of these characteristics is a recognition that they have been used to treat people unfairly in the past and that, as a society, we have deemed this unfairness unacceptable. Many, albeit not all, of the concerns about algorithmic bias relate to situations where that bias may lead to discrimination in the sense set out in the Equality Act 2010.\n",
      "\n",
      "The Equality Act 2010[footnote 9] defines two main categories of discrimination:[footnote 10]\n",
      "\n",
      "Direct Discrimination: When a person is treated less favourably than another because of a protected characteristic.\n",
      "\n",
      "Indirect Discrimination: When a wider policy or practice, even if it applies to everyone, disadvantages a group of people who share a protected characteristic (and there is not a legitimate reason for doing so).\n",
      "\n",
      "Where this discrimination is direct, the interpretation of the law in an algorithmic decision-making process seems relatively clear. If an algorithmic model explicitly leads to someone being treated less favourably on the basis of a protected characteristic that would be unlawful. There are some very specific exceptions to this in the case of direct discrimination on the basis of age (where such discrimination could be lawful if a proportionate means to a proportionate aim, e.g. services targeted at a particular age range) or limited positive actions in favour of those with disabilities.\n",
      "\n",
      "However, the increased use of data-driven technology has created new possibilities for indirect discrimination. For example, a model might consider an individual’s postcode. This is not a protected characteristic, but there is some correlation between postcode and race. Such a model, used in a decision-making process (perhaps in financial services or policing) could in principle cause indirect racial discrimination. Whether that is the case or not depends on a judgement about the extent to which such selection methods are a proportionate means of achieving a legitimate aim.[footnote 11] For example, an insurer might be able to provide good reasons why postcode is a relevant risk factor in a type of insurance. The level of clarity about what is and is not acceptable practice varies by sector, reflecting in part the maturity in using data in complex ways. As algorithmic decision-making spreads into more use cases and sectors, clear context-specific norms will need to be established. Indeed as the ability of algorithms to deduce protected characteristics with certainty from proxies continues to improve, it could even be argued that some examples could potentially cross into direct discrimination.\n",
      "\n",
      "Unfair bias beyond discrimination\n",
      "\n",
      "Discrimination is a narrower concept than bias. Protected characteristics have been included in law due to historical evidence of systematic unfair treatment, but individuals can also experience unfair treatment on the basis of other characteristics that are not protected.\n",
      "\n",
      "There will always be grey areas where individuals experience systematic and unfair bias on the basis of characteristics that are not protected, for example accent, hairstyle, education or socio-economic status.[footnote 12] In some cases, these may be considered as indirect discrimination if they are connected with protected characteristics, but in other cases they may reflect unfair biases that are not protected by discrimination law.\n",
      "\n",
      "However the increased use of algorithms may exacerbate this difficulty. The introduction of algorithms can encode existing biases into algorithms, if they are trained from existing decisions. This can reinforce and amplify existing unfair bias, whether on the basis of protected characteristics or not.\n",
      "\n",
      "Algorithmic decision-making can also go beyond amplifying existing biases, to creating new biases that may be unfair, though difficult to address through discrimination law. This is because machine learning algorithms find new statistical relationships, without necessarily considering whether the basis for those relationships is fair, and then apply this systematically in large numbers of individual decisions.\n",
      "\n",
      "2.5 Fairness\n",
      "\n",
      "Overview\n",
      "\n",
      "We defined bias as including an element of unfairness. This highlights challenges in defining what we mean by fairness, which is a complex and long debated topic. Notions of fairness are neither universal nor unambiguous, and they are often inconsistent with one another.\n",
      "\n",
      "In human decision-making systems, it is possible to leave a degree of ambiguity about how fairness is defined. Humans may make decisions for complex reasons, and are not always able to articulate their full reasoning for making a decision, even to themselves. There are pros and cons to this. It allows for good fair-minded decision-makers to consider the specific individual circumstances, and human understanding of the reasons for why these circumstances might not conform to typical patterns. This is especially important in some of the most critical life-affecting decisions, such as those in policing or social services, where decisions often need to be made on the basis of limited or uncertain information; or where wider circumstances, beyond the scope of the specific decision, need to be taken into account. It is hard to imagine that automated decisions could ever fully replace human judgement in such cases. But human decisions are also open to the conscious or unconscious biases of the decision-makers, as well as variations in their competence, concentration levels or mood when specific decisions are made.\n",
      "\n",
      "Algorithms, by contrast, are unambiguous. If we want a model to comply with a definition of fairness, we must tell it explicitly what that definition is. How significant a challenge that is depends on context. Sometimes the meaning of fairness is very clearly defined; to take an extreme example, a chess playing AI achieves fairness by following the rules of the game. Often though, existing rules or processes require a human decision-maker to exercise discretion or judgement, or to account for data that is difficult to include in a model (e.g. context around the decision that cannot be readily quantified). Existing decision-making processes must be fully understood in context in order to decide whether algorithmic decision-making is likely to be appropriate. For example, police officers are charged with enforcing the criminal law, but it is often necessary for officers to apply discretion on whether a breach of the letter of the law warrants action. This is broadly a good thing, but such discretion also allows an individual’s personal biases, whether conscious or unconscious, to affect decisions.\n",
      "\n",
      "Even in cases where fairness can be more precisely defined, it can still be challenging to capture all relevant aspects of fairness in a mathematical definition. In fact, the trade-offs between mathematical definitions demonstrate that a model cannot conform to all possible fairness definitions at the same time. Humans must choose which notions of fairness are appropriate for a particular algorithm, and they need to be willing to do so upfront when a model is built and a process is designed.\n",
      "\n",
      "The General Data Protection Regulation (GDPR) and Data Protection Act 2018 contain a requirement that organisations should use personal data in a way that is fair. The legislation does not elaborate further on the meaning of fairness, but the ICO guides organisations that “In general, fairness means that you should only handle personal data in ways that people would reasonably expect and not use it in ways that have unjustified adverse effects on them.”[footnote 13] Note that the discussion in this section is wider than the notion in GDPR, and does not attempt to define how the word fair should be interpreted in that context.\n",
      "\n",
      "Notions of fairness\n",
      "\n",
      "Notions of fair decision-making (whether human or algorithmic) are typically gathered into two broad categories:\n",
      "\n",
      "procedural fairness is concerned with ‘fair treatment’ of people, i.e. equal treatment within the process of how a decision is made. It might include, for example, defining an objective set of criteria for decisions, and enabling individuals to understand and challenge decisions about them.\n",
      "\n",
      "outcome fairness is concerned with what decisions are made i.e. measuring average outcomes of a decision-making process and assessing how they compare to an expected baseline. The concept of what a fair outcome means is of course highly subjective; there are multiple different definitions of outcome fairness.\n",
      "\n",
      "Some of these definitions are complementary to each other, and none alone can capture all notions of fairness. A ‘fair’ process may still produce ‘unfair’ results, and vice versa, depending on your perspective. Even within outcome fairness there are many mutually incompatible definitions for a fair outcome. Consider for example a bank making a decision on whether an applicant should be eligible for a given loan, and the role of an applicant’s sex in this decision. Two possible definitions of outcome fairness in this example are:\n",
      "\n",
      "A. The probability of getting a loan should be the same for men and women.\n",
      "\n",
      "B. The probability of getting a loan should be the same for men and women who earn the same income. Taken individually, either of these might seem like an acceptable definition of fair. But they are incompatible. In the real world sex and income are not independent of each other; the UK has a gender pay gap meaning that, on average, men earn more than women.[footnote 14] Given that gap, it is mathematically impossible to achieve both A and B simultaneously.\n",
      "\n",
      "This example is by no means exhaustive in highlighting the possible conflicting definitions that can be made, with a large collection of possible definitions identified in the machine learning literature.[footnote 15]\n",
      "\n",
      "In human decision-making we can often accept ambiguity around this type of issue, but when determining if an algorithmic decision-making process is fair, we have to be able to explicitly determine what notion of fairness we are trying to optimise for. It is a human judgement call whether the variable (in this case salary) acting as a proxy for a protected characteristic (in this case sex) is seen as reasonable and proportionate in the context. We investigated public reactions to a similar example to this in work with the Behavioural Insights Team (see further detail in Chapter 4.\n",
      "\n",
      "Addressing fairness\n",
      "\n",
      "Even when we can agree what constitutes fairness, it is not always clear how to respond. Conflicting views about the value of fairness definitions arise when the application of a process intended to be fair produces outcomes regarded as unfair. This can be explained in several ways, for example:\n",
      "\n",
      "Differences in outcomes are evidence that the process is not fair. If in principle, there is no good reason why there should be differences on average in the ability of men and women to do a particular job, differences in the outcomes between male and female applicants may be evidence that a process is biased and failing to accurately identify those most able. By correcting this, the process is both fairer and more efficient.\n",
      "\n",
      "If in principle, there is no good reason why there should be differences on average in the ability of men and women to do a particular job, differences in the outcomes between male and female applicants may be evidence that a process is biased and failing to accurately identify those most able. By correcting this, the process is both fairer and more efficient. Differences in outcomes are the consequence of past injustices. For example, a particular set of previous experience might be regarded as a necessary requirement for a role, but might be more common among certain socio-economic backgrounds due to past differences in access to employment and educational opportunities. Sometimes it might be appropriate for an employer to be more flexible on requirements to enable them to get the benefits of a more diverse workforce (perhaps bearing a cost of additional training); but sometimes this may not be possible for an individual employer to resolve in their recruitment, especially for highly specialist roles.\n",
      "\n",
      "The first argument implies greater outcome fairness is consistent with more accurate and fair decision-making. The second argues that different groups ought to be treated differently to correct for historical wrongs and is the argument associated with quota regimes. It is not possible to reach a general opinion on which argument is correct, this is highly dependent on the context (and there are also other possible explanations).\n",
      "\n",
      "In decision-making processes based on human judgement it is rarely possible to fully separate the causes of differences in outcomes. Human recruiters may believe they are accurately assessing capabilities, but if the outcomes seem skewed it is not always possible to determine the extent to which this in fact reflects bias in methods of assessing capabilities.\n",
      "\n",
      "How do we handle this in the human world? There are a variety of techniques, for example steps to ensure fairness in an interview-based recruitment process might include:\n",
      "\n",
      "Training interviewers to recognise and challenge their own individual unconscious biases.\n",
      "\n",
      "Policies on the composition of interview panels.\n",
      "\n",
      "Designing assessment processes that score candidates against objective criteria.\n",
      "\n",
      "Applying formal or informal quotas (though a quota based on protected characteristics would usually be unlawful in the UK).\n",
      "\n",
      "Why algorithms are different\n",
      "\n",
      "The increased use of more complex algorithmic approaches in decision-making introduces a number of new challenges and opportunities.\n",
      "\n",
      "The need for conscious decisions about fairness: In data-driven systems, organisations need to address more of these issues at the point a model is built, rather than relying on human decision-makers to interpret guidance appropriately (an algorithm can’t apply “common sense” on a case-by-case basis). Humans are able to balance things implicitly, machines will optimise without any balance if asked to do so.\n",
      "\n",
      "Explainability: Data-driven systems allow for a degree of explainability about the factors causing variation in the outcomes of decision-making systems between different groups and to assess whether or not this is regarded as fair. For example, it is possible to examine more directly the degree to which relevant characteristics are acting as a proxy for other characteristics, and causing differences in outcomes between different groups. If a recruitment process included requirements for length of service and qualification, it would be possible to see whether, for example, length of service was generally lower for women due to career breaks and that this was causing an imbalance.\n",
      "\n",
      "The extent to which this is possible depends on the complexity of the algorithm used. Dynamic algorithms drawing on large datasets may not allow for a precise attribution of the extent to which the outcome of the process for an individual woman was attributable to a particular characteristic and its association with gender. However, it is possible to assess the degree to which over a time period, different characteristics are influencing recruitment decisions and how they correlate with characteristics during that time.\n",
      "\n",
      "The term ‘black box’ is often used to describe situations where, for a variety of different reasons, an explanation for a decision is unobtainable. This can include commercial issues (e.g. the decision-making organisation does not understand the details of the algorithm which their supplier considers their own intellectual property) or technical reasons (e.g. machine learning techniques that are less accessible for easy human explanation of individual decisions). The Information Commissioner’s Office and the Alan Turing Institute have recently published detailed joint advice on how organisations can overcome some of these challenges and provide a level of explanation of decisions.[footnote 16]\n",
      "\n",
      "Scale of impact: The potential breadth of impact of an algorithm links to the market dynamics. Many algorithmic software tools are developed as platforms and sold across many companies. It is therefore possible, for example, that individuals applying to multiple jobs could be rejected at sift by the same algorithm (perhaps sold to a large number of companies recruiting for the same skill sets in the same industry). If the algorithm does this for reasons irrelevant to their actual performance, but on the basis of a set of characteristics that are not protected, then this feels very much like systematic discrimination against a group of individuals, but the Equality Act provides no obvious protection against this.\n",
      "\n",
      "Algorithmic decision-making will inevitably increase over time; the aim should be to ensure that this happens in a way that acts to challenge bias, increase fairness and promote equality, rather than entrenching existing problems. The recommendations of this review are targeted at making this happen.\n",
      "\n",
      "Case study: Exam results in August 2020 Due to COVID-19, governments across the UK decided to cancel school examinations in summer 2020, and find an alternative approach to awarding grades. All four nations of the UK attempted to implement similar processes to deliver this; combining teacher assessments with a statistical moderation process that attempted to achieve a similar distribution of grades to previous years. The approaches were changed in response to public concerns, and significant criticism about both individual fairness and concerns that grades were biased. How should fairness have been interpreted in this case? There were a number of different notions of fairness to consider, including:[footnote 17] Fairness between year groups: Achieve a similar distribution of grades to previous and future year groups.\n",
      "\n",
      "Group fairness between different schools: Attempt to standardise teacher assessed grades, given the different levels of strictness/optimism in grading between different schools to be fair to individual students from different schools.\n",
      "\n",
      "Group fairness and discrimination: Avoid exacerbating differences in outcomes correlated with protected characteristics; particularly sex and race. This did not include addressing any systematic bias in results based on inequality of opportunity; this was seen as outside the mandate of an exam body.\n",
      "\n",
      "Avoid any bias based on socio-economic status.\n",
      "\n",
      "A fair process for allocating grades to individual students, i.e. allocating them a grade that was seen to be a fair representation of their own individual capabilities and efforts. The main work of this review was complete prior to the release of summer 2020 exam results, but there are some clear links between the issues raised and the contents of this review, including issues of public trust, transparency and governance.\n",
      "\n",
      "2.6 Applying ethical principles\n",
      "\n",
      "The way decisions are made, the potential biases which they are subject to, and the impact these decisions have on individuals, are highly context dependent. It is unlikely that all forms of bias can be entirely eliminated. This is also true in human decision-making; it is important to understand the status quo prior to the introduction of data-driven technology in any given scenario. Decisions may need to be made about what kinds and degrees of bias are tolerable in certain contexts and the ethical questions will vary depending on the sector.\n",
      "\n",
      "We want to help create the conditions where ethical innovation using data-driven technology can thrive. It is therefore essential to ensure our approach is grounded in robust ethical principles.\n",
      "\n",
      "The UK government, along with 41 other countries, has signed up to the OECD Principles on Artificial Intelligence[footnote 18]. They provide a good starting point for considering our approach to dealing with bias, as follows:[footnote 19]\n",
      "\n",
      "1: AI should benefit people and the planet by driving inclusive growth, sustainable development and well-being.\n",
      "\n",
      "There are many potential advantages of algorithmic decision-making tools when used appropriately, such as the potential efficiency and accuracy of predictions. There is also the opportunity for these tools to support good decision-making by reducing human error and combating existing bias. When designed correctly, they can offer a more objective alternative (or supplement) to human subjective interpretation. It is core to this review, and the wider purpose of CDEI, to identify how we can collectively ensure that these opportunities outweigh the risks.\n",
      "\n",
      "2: AI systems should be designed in a way that respects the rule of law, human rights, democratic values and diversity, and they should include appropriate safeguards – for example, enabling human intervention where necessary – to ensure a fair and just society.\n",
      "\n",
      "This principle sets out some core terms for what we mean by fairness in an algorithmic decision-making process. We cover a number of aspects of it throughout the review.\n",
      "\n",
      "Our focus in this review on significant decisions means that we have been largely considering decisions where the algorithm forms only part of an overall decision-making process, and hence there is a level of direct human oversight of individual decisions. However, consideration is always needed on whether the role of the human remains meaningful; does the human understand the algorithm (and its limitations) sufficiently well to exercise that oversight effectively? Does the organisational environment that they are working within empower them to do so? Is there a risk that human biases could be reintroduced through this oversight?\n",
      "\n",
      "In Chapter 8 below we consider the ability of existing UK legal and regulatory structures to ensure fairness in this area, especially data protection and equality legislation, and how they will need to evolve to adapt to an algorithmic world.\n",
      "\n",
      "3: There should be transparency and responsible disclosure around AI systems to ensure that people understand AI-based outcomes and can challenge them.\n",
      "\n",
      "Our sector-led work has identified variable levels of transparency on the usage of algorithms. A variety of other recent reviews have called for increased levels of transparency across the public sector.\n",
      "\n",
      "It is clear that more work is needed to achieve this level of transparency in a consistent way across the economy, and especially in the public sector where many of the highest stakes decisions are made. We discuss how this can be achieved in Chapter 9.\n",
      "\n",
      "4: AI systems must function in a robust, secure and safe way throughout their life cycles and potential risks should be continually assessed and managed.\n",
      "\n",
      "In Chapter 7 we identify approaches taken to mitigate the risk of bias through the development lifecycle of an algorithmic decision-making system, and suggest action that the government can take to support development teams in taking a fair approach.\n",
      "\n",
      "5: Organisations and individuals developing, deploying or operating AI systems should be held accountable for their proper functioning in line with the above principles.\n",
      "\n",
      "The use of algorithmic decision-making tools within decisions can have a significant impact on individuals or society, raising a requirement for clear lines of accountability in their use and impact.\n",
      "\n",
      "When decisions are made by humans in large organisations, we don’t generally consider it possible to get it right every time. Instead, we expect organisations to have appropriate structures, policies and procedures to anticipate and address potential bias, offer redress when it occurs, and set clear governance processes and lines of accountability for decisions.\n",
      "\n",
      "Organisations that are introducing algorithms into decisions that were previously purely made by humans should be looking to achieve at least equivalent standards of fairness, accountability and transparency, and in many cases should look to do better. Defining equivalence is not always easy of course, there may be occasions where these standards have to be achieved in a different way in an algorithmic world. We discuss this issue in more detail in Part III of the report.\n",
      "\n",
      "For all of these issues, it is important to remember that we are not just interested in the output of an algorithm, but the overall decision-making process that sits around it. Organisations have existing accountability processes and standards, and the use of algorithms in decision-making needs to sit within existing accountability processes to ensure that they are used intentionally and effectively, and therefore that the organisation is as accountable for the outcome as they are for traditional human decision-making.\n",
      "\n",
      "We must decide how far to mitigate bias and how we should govern our approach to doing so. These decisions require value judgements and trade-offs between competing values. Humans are often trusted to make these trade-offs without having to explicitly state how much weight they have put on different considerations. Algorithms are different. They are programmed to make trade-offs according to rules and their decisions can be interrogated and made explicit.\n",
      "\n",
      "2.7 The opportunity\n",
      "\n",
      "The OECD principles are clearly high level, and only take us so far when making difficult ethical balances for individual decision-making systems. The work in this review suggests that as algorithmic decision-making continues to grow in scale, we should be ambitious in aiming not only to avoid new bias, but to use this as an opportunity to address historical unfairness.\n",
      "\n",
      "Organisations responsible for using algorithms require more specific guidance on how principles apply in their circumstances. The principles are often context specific and are discussed in more detail in the sector sections below. However, we can start to outline some rules of thumb that can guide all organisations using algorithms to support significant decision-making processes:\n",
      "\n",
      "History shows that most decision-making processes are biased, often unintentionally. If you want to make fairer decisions, then using data to measure this is the best approach; certainly assuming the non-existence of bias in a process is a highly unreliable approach.\n",
      "\n",
      "If your data shows historical patterns of bias, this does not mean that algorithms should not be considered. The bias should be addressed, and the evidence from the data should help inform that approach. Algorithms designed to mitigate bias may be part of the solution.\n",
      "\n",
      "If an algorithm is introduced to replace a human decision system, the bias mitigation strategy should be designed to result in fairer outcomes and a reduction in unwarranted differences between groups.\n",
      "\n",
      "While it is important to test the outputs of algorithms and assess their fairness, the key measure of the fairness of an algorithm is the impact it has on the whole decision process. In some cases, resolving fairness issues may only be possible outside of the actual decision-making process, e.g. by addressing wider systemic issues in society.\n",
      "\n",
      "Putting a ‘human in the loop’ is a way of addressing concern about the ‘unforgiving nature’ of algorithms (as they can bring perspectives or contextual information not available to the algorithm) but can also introduce human bias into the system. Humans ‘over the loop’ monitoring the fairness of the whole decision process are also needed, with responsibility for the whole process.\n",
      "\n",
      "Humans over the loop need to understand how the machine learning model works, and the limitations and trade-offs that it is making, to a great enough extent to make informed judgements on whether it is performing effectively and fairly.\n",
      "\n",
      "Part II: Sector reviews\n",
      "\n",
      "The ethical questions in relation to bias in algorithmic decision-making vary depending on the context and sector. We therefore chose four initial areas of focus to illustrate the range of issues. These were recruitment, financial services, policing and local government.\n",
      "\n",
      "All of these sectors have the following in common:\n",
      "\n",
      "They involve making decisions at scale about individuals which involve significant potential impacts on those individuals’ lives.\n",
      "\n",
      "There is a growing interest in the use of algorithmic decision-making tools in these sectors, including those involving machine learning in particular.\n",
      "\n",
      "There is evidence of historic bias in decision-making within these sectors, leading to risks of this being perpetuated by the introduction of algorithms.\n",
      "\n",
      "There are of course other sectors that we could have considered; these were chosen as a representative sample across the public and private sector, not because we have judged that the risk of bias is most acute in these specific cases.\n",
      "\n",
      "In this part of the review, we focus on the sector-specific issues, and reach a number of recommendations specific to individual sectors. The sector studies then inform the cross-cutting findings and recommendations in Part III below.\n",
      "\n",
      "3. Recruitment\n",
      "\n",
      "Summary Overview of findings: The use of algorithms in recruitment has increased in recent years, in all stages of the recruitment process. Trends suggest these tools will become more widespread, meaning that clear guidance and a robust regulatory framework are essential.\n",
      "\n",
      "When developed responsibly, data-driven tools have the potential to improve recruitment by standardising processes and removing discretion where human biases can creep in, however if using historical data, these human biases are highly likely to be replicated.\n",
      "\n",
      "Rigorous testing of new technologies is necessary to ensure platforms do not unintentionally discriminate against groups of people, and the only way to do this is to collect demographic data on applicants and use this data to monitor how the model performs. Currently, there is little standardised guidance for how to do this testing, meaning companies are largely self-regulated.\n",
      "\n",
      "Algorithmic decision-making in recruitment is currently governed primarily by the Equality Act 2010 and the Data Protection Act 2018, however we found in both cases there is confusion regarding how organisations should enact their legislative responsibilities. Recommendations to regulators: Recommendation 1: The Equality and Human Rights Commission should update its guidance on the application of the Equality Act 2010 to recruitment, to reflect issues associated with the use of algorithms, in collaboration with consumer and industry bodies.\n",
      "\n",
      "Recommendation 2: The Information Commissioner’s Office should work with industry to understand why current guidance is not being consistently applied, and consider updates to guidance (e.g. in the Employment Practices Code), greater promotion of existing guidance, or other action as appropriate. Advice to industry Organisations should carry out Equality Impact Assessments to understand how their models perform for candidates with different protected characteristics, including intersectional analysis for those with multiple protected characteristics. Future CDEI work CDEI will consider how it can work with relevant organisations to assist with developing guidance on applying the Equality Act 2010 to algorithms in recruitment.\n",
      "\n",
      "3.1 Background\n",
      "\n",
      "Decisions about who to shortlist, interview and employ have significant effects on the lives of individuals and society. When certain groups are disadvantaged either directly or indirectly from the recruitment process, social inequalities are broadened and embedded.\n",
      "\n",
      "The existence of human bias in traditional recruitment is well-evidenced.[footnote 20] A famous study found that when orchestral players were kept behind a screen for their audition, there was a significant increase in the number of women who were successful.[footnote 21] Research in the UK found that candidates with ethnic minority backgrounds have to send as many of 60% more applications than white candidates to receive a positive response.[footnote 22] Even more concerning is the fact that there has been very little historical improvement in these figures over the last few decades.[footnote 23] Recruitment is also considered a barrier to employment for people with disabilities.[footnote 24] A range of factors from affinity biases, where recruiters tend to prefer people similar to them, to informal processes that recruit candidates already known to the organisation all amplify these biases, and some people believe technology could play a role in helping to standardise processes and make them fairer.[footnote 25]\n",
      "\n",
      "The internet has also meant that candidates are able to apply for a much larger number of jobs, thus creating a new problem for organisations needing to review hundreds, sometimes thousands, of applications. These factors have led to an increase in new data-driven tools, promising greater efficiency, standardisation and objectivity. There is a consistent upwards trend in adoption, with around 40% of HR functions in international companies now using AI.[footnote 26] It is however important to distinguish between new technologies and algorithmic decision-making. Whilst new technology is increasingly being applied across the board in recruitment, our research was focused on tools that utilise algorithmic decision-making systems, training on data to predict a candidate’s future success.\n",
      "\n",
      "There are concerns about the potential negative impacts of algorithmic decision-making in recruitment. There are also concerns about the effectiveness of technologies to be able to predict good job performance given the relative inflexibility of systems and the challenge of conducting a thorough assessment using automated processes at scale. For the purpose of this report, our focus is on bias rather than effectiveness.\n",
      "\n",
      "How we approached our work\n",
      "\n",
      "Our work on recruitment as a sector began with a call for evidence and the landscape summary. This evidence gathering provided a broad overview of the challenges and opportunities presented by using algorithmic tools in hiring.\n",
      "\n",
      "In addition to desk-based research, we conducted a series of semi-structured interviews with a broad range of software providers and recruiters. In these conversations we focused on how providers currently test and mitigate bias in their tools. We also spoke with a range of other relevant organisations and individuals including think tanks, academics, government departments, regulators and civil society groups.\n",
      "\n",
      "3.2 Findings\n",
      "\n",
      "Tools are being created and used for every stage of the recruitment process\n",
      "\n",
      "There are many stages in a recruitment process and algorithms are increasingly being used throughout.[footnote 27] Starting with the sourcing of applicants via targeting online advertisements[footnote 28] through to CV screening, then interview and selection phases. Data-driven tools are sold as a more efficient, accurate and objective way of assisting with recruiting decisions.\n",
      "\n",
      "Figure 2: Examples of algorithmic tools used through the sourcing, screening, interview and selection stages of the recruitment process\n",
      "\n",
      "In recruitment, AI tools are available to support all stages of the hiring process: sourcing, screening, interview and selection. At the sourcing stage, tools are available to review job descriptions, target job advertising to potential candidates, power recruiting chatbots and ‘headhunt’ for high-performing candidates. At the screening stage, AI tools can be used to screen qualifications, match CVs to specific job roles, run psychometric and game-based tests to assess cognitive skills, and rank applications. At the interview stage, voice and face recognition technology can be used in video interviewing. At the selection stage, AI software can be used to perform background checks and predict offers.\n",
      "\n",
      "Organisations may use different providers for the stages of the recruitment process and there are increasing options to integrate different types of tools.\n",
      "\n",
      "Algorithms trained on historic data carry significant risks for bias\n",
      "\n",
      "There are many ways bias can be introduced into the recruiting process when using data-driven technology. Decisions such as how data is collected, which variables to collect, how the variables are weighted, and the data the algorithm is trained on all have an impact and will vary depending on the context. However one theme that arises consistently is the risk of training algorithms on biased historical data.\n",
      "\n",
      "High profile cases of biased recruiting algorithms include those trained using historical data on current and past employees within an organisation, which is then used to try and predict the performance of future candidates.[footnote 29] Similar systems are used for video interviewing software where existing employees or prospective applicants undertake the assessment and this is assessed and correlated in line with a performance benchmark.[footnote 30] The model is then trained on this data to understand the traits of people who are considered high performers.\n",
      "\n",
      "Without rigorous testing, these kinds of predictive systems can pull out characteristics that have no relevance to job performance but are rather descriptive features that correlate with current employees. For example, one company developed a predictive model trained on their company data that found having the name “Jared” was a key indicator of a successful applicant.[footnote 31] This is an example where a machine learning process has picked up a very explicit bias, others are often more subtle but can be still as damaging. In the high profile case of Amazon, an application system trained on existing employees never made it past the development phase when testing showed that women’s CVs were consistently rated worse.[footnote 32] Pattern detection of this type is likely to identify various factors that correspond with protected characteristics if development goes unchecked, so it is essential that organisations interrogate their models to identify proxies or risk indirectly discriminating against protected groups.\n",
      "\n",
      "Another way bias can arise is through having a dataset that is limited in respect to candidates with certain characteristics. For example, if the training set was from a company that had never hired a woman, the algorithm would be far less accurate in respect to female candidates. This type of bias arises from imbalance, and can easily be replicated across other demographic groups. Industry should therefore be careful about the datasets used to develop these systems both with respect to biases arising through historical p\n"
     ]
    }
   ],
   "source": [
    "print(texts[filename][\"newspaper3k\"][\"doc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a2c8b0ee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.gov.uk/government/publications/cdei-publishes-review-into-bias-in-algorithmic-decision-making/main-report-cdei-review-into-bias-in-algorithmic-decision-making\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "title            REVIEW INTO BIAS IN ALGORITHMIC DECISION-MAKING\n",
       "country                                           United Kingdom\n",
       "documentUrl    https://www.gov.uk/government/publications/cde...\n",
       "startDate                                                 2020.0\n",
       "endDate                                                      NaN\n",
       "oecdId                                                     26980\n",
       "Name: 545, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = oecd.loc[int(filename[:-5])]\n",
    "print(tmp[\"documentUrl\"])\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f732c93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "afeb0ed2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "425.html\n",
      "word diff 16028\n",
      "newspaper3k: 17888, dragnet 33916\n"
     ]
    }
   ],
   "source": [
    "filename = s[-2][0]\n",
    "print(filename)\n",
    "print(\"word diff %s\" % lens[filename])\n",
    "print(\"newspaper3k: %s, dragnet %s\" %(texts[filename][\"newspaper3k\"][\"len\"], texts[filename][\"dragnet\"][\"len\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5a5a67b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ministerial foreword\n",
      "When I became Digital Secretary, I vowed to be unashamedly pro-tech. This has to begin with data. Data is now the driving force of the world’s modern economies. It fuels innovation in businesses large and small, and has been a lifeline during the global coronavirus pandemic. The fact that governments, businesses, organisations and public services were able to share vital information quickly, efficiently and ethically during the pandemic has not only saved countless lives, but has enabled us to work from home, keep the economy running and stay connected with loved ones during a period of unprecedented disruption. As we enter into recovery, it is vital that we make the most of what we have learnt.\n",
      "This National Data Strategy aims to do exactly that, building on our manifesto pledge to improve data use in government, and going further. It seeks to maintain the high watermark of data use set during the pandemic, and to free up businesses and organisations to keep using data to innovate, experiment and drive a new era of growth. It seeks to harness the power of data to boost productivity, create new businesses and jobs, improve public services and position the UK as the forerunner of the next wave of innovation.\n",
      "Under this strategy, data and data use are seen as opportunities to be embraced, rather than threats against which to be guarded.\n",
      "This means asking fundamental questions about what data should and should not be made available across the UK. It means maintaining a regulatory regime that is not overly burdensome for smaller businesses and that supports responsible innovation. It means driving a radical transformation of how the government understands and unlocks the value of its own data to improve a range of public services and inform decisions at scale, through a whole-government approach driven from the centre. It means taking the risks of increased data use seriously. And it means positioning the UK as a global champion of data use, and encouraging the international flow of information across borders.\n",
      "The strategy is a central part of the government’s wider ambition for a thriving, fast-growing digital sector in the UK, underpinned by public trust. We want the UK to be a nation of digital entrepreneurs, innovators and investors, the best place in the world to start and grow a digital business, as well as the safest place in the world to go online. We will set out more on how we propose to support a digital drive for growth in our Digital Strategy, which we will be publishing in the Autumn.\n",
      "This document is a framework for the action this government will take on data. It is not the final answer, but part of a conversation about the way that we support the use of data in the UK. We lay out the opportunities that we want to realise, the pillars that we have identified as core to unlocking the power of data for the UK, and the missions that we must prioritise now.\n",
      "Rt Hon. Oliver Dowden CBE MP Secretary of State for Digital, Culture, Media and Sport\n",
      "The opportunity\n",
      "Better use of data can help organisations of every kind succeed – across the public, private and third sectors. It can support the delivery of existing services, from manufacturing to logistics, and it can be used to create entirely new products. It is a driver of scientific and technological innovation, and central to the delivery of a whole range of vital public services and societal goals, from tackling climate change to supporting the National Health Service. As businesses embrace technology, data creates jobs, opens up whole new markets and drives demand for a highly skilled workforce.\n",
      "On an individual level, the use of data benefits us every day – from the lives saved due to data-driven medical discoveries, to personal budgeting, understanding how much we have exercised and identifying better transport routes.\n",
      "The UK is already a leading digital nation. The data market in the UK (i.e. money made from products or services derived from digitised data) is the largest in Europe . UK tech grew dramatically in 2019 , with the UK securing 33% of European tech investment. [footnote 1] Globally, the UK now sits behind only the US and China in terms of venture capital investment.\n",
      "But the last five years have seen huge technological changes, and national governments need to respond accordingly. We need a data strategy that reflects the opportunities and challenges of our new hyper-digital world, one that ensures we weigh the priorities and potential trade-offs of data in a deliberate and evidence-based way, and, above all, one that drives growth in the UK economy and powers our recovery from the coronavirus pandemic.\n",
      "This strategy looks at how we can leverage existing UK strengths to boost the better use of data across businesses, government, civil society and individuals. Having left the European Union, we will take advantage of being an independent, sovereign nation to maximise those strengths domestically, and position ourselves internationally to influence the global approach to data sharing and use. We will act ambitiously at home and on the international stage, aligning our history of problem-solving in science and technology with progressive values and the competence and pragmatism of our regulatory institutions.\n",
      "The UK response to the global coronavirus pandemic has powerfully illustrated the potential benefits of data. Our understanding of this disease, our ability to support people and our cooperation across borders have all relied on the responsible and effective use and sharing of data.\n",
      "But we have a duty to do more – especially with the data that the government itself holds, which can be used and shared for the benefit of society. Individual transactions, from applying for social security benefits to buying a house, are more resilient when personal information can be shared from trusted sources – for example, in the form of a digital identity.\n",
      "Data is a non-depletable resource in theory, but its use is limited by barriers to its access – such as when data is hoarded, when access rights are unclear or when organisations do not make good use of the data they already have. These barriers undermine the performance of public services and our economy, risking poorer outcomes for citizens. We will ensure that data can be leveraged to deliver new and innovative services, promote stronger competition, and better prices and choice for consumers and small businesses. We will drive an approach to data that holds that all can benefit when data is used responsibly, and that withholding data can negatively impact society.\n",
      "This strategy\n",
      "As part of this document, we are asking for your views. Consultation questions are included throughout the text and in an accompanying document . In future updates, we will lay out the steps that we will take to implement the strategy, and the way that your responses and the evidence you have provided have shaped our approach.\n",
      "The pillars\n",
      "A number of interconnected issues currently prevent the best use of data in the UK. These are reflected in the core pillars of this strategy:\n",
      "Data foundations : The true value of data can only be fully realised when it is fit for purpose, recorded in standardised formats on modern, future-proof systems and held in a condition that means it is findable, accessible, interoperable and reusable. By improving the quality of the data, we can use it more effectively, and drive better insights and outcomes from its use.\n",
      "Data skills : To make the best use of data, we must have a wealth of data skills to draw on. That means delivering the right skills through our education system, but also ensuring that people can continue to develop the data skills they need throughout their lives.\n",
      "Data availability : For data to have the most effective impact, it needs to be appropriately accessible, mobile and re-usable. That means encouraging better coordination, access to and sharing of data of appropriate quality between organisations in the public, private and third sectors, and ensuring appropriate protections for the flow of data internationally.\n",
      "Responsible data : As we drive increased use of data, we must ensure that it is used responsibly, in a way that is lawful, secure, fair, ethical, sustainable and accountable, while also supporting innovation and research.\n",
      "The missions\n",
      "From these pillars, we have identified five priority areas of action. These missions address key challenges that can prevent us from taking advantage of the opportunities that data offers.\n",
      "Unlocking the value of data across the economy. Data is an incredibly valuable resource for businesses and other organisations. However, there is increasing evidence to suggest its full value is not being realised because vital information is not getting to where it needs to be. To ensure the UK is a world leader in data, our first mission will be to set the correct conditions to make data usable, accessible and available across the economy, while protecting people’s data rights and private enterprises’ intellectual property. Using a considered and evidence-based approach, we will develop a clear policy framework to determine what government interventions are needed to do so.\n",
      "Securing a pro-growth and trusted data regime. We want the data revolution to benefit businesses large and small. That means maintaining a data regime in the UK that is not too burdensome for the average company; one that helps innovators and entrepreneurs to use data responsibly and securely, without undue regulatory uncertainty or risk, to drive growth across the economy. But we also want the public to be active agents in the thriving digital economy and to have confidence and trust in how data, including personal data, is used. The UK’s data regime will support vibrant competition and innovation, building trust and maintaining high data protection standards without creating unnecessary barriers to data use.\n",
      "Transforming government’s use of data to drive efficiency and improve public services. The coronavirus pandemic showed that there is massive untapped potential in the way government and public services use and share data to help and protect people. To sustain the high watermark set by the pandemic, the government will undertake an ambitious and radical transformation of its own approach, driving major improvements in the way information is efficiently managed, used and shared across government. To succeed, we need a whole-government approach that ensures alignment around the best practice and standards needed to drive value and insights from data; and the creation of an appropriately safeguarded, joined-up and interoperable data infrastructure to support this. We also need the right skills and leadership within the public sector to understand and unlock the potential of data.\n",
      "Ensuring the security and resilience of the infrastructure on which data relies. The use of data is now a central part of modern life, so we need to make sure that the infrastructure underpinning it is safe and secure. The infrastructure on which data relies is a vital national asset that needs to be protected from security risks and other concerns, such as service disruption. Interruption to data-driven services and activities can cause disruption to businesses, organisations and public services. While these are also commercial risks to manage, the government has a responsibility to ensure that data and its supporting infrastructure is resilient in the face of established, new and emerging risks, protecting the economy as it grows.\n",
      "Championing the international flow of data. The flow of information across borders fuels global business operations, supply chains and trade, powering growth across the world. It also plays a wider societal role. The transfer of personal data ensures people’s salaries are paid, and helps them connect with loved ones from afar. And, as the coronavirus pandemic has demonstrated, sharing health data can aid vital scientific research into diseases while uniting countries in their response to global health emergencies. Having left the European Union, the UK will champion the benefits that data can deliver. We will promote domestic best practice and work with international partners to ensure data is not inappropriately constrained by national borders and fragmented regulatory regimes so that it can be used to its full potential.\n",
      "Together, the steps identified in this strategy build on UK strengths to drive better use of data – data use that is more secure, more innovative and more widely recognised as a force for good. Better use of data will drive growth and productivity, improve our society and public services and position the UK as a leader of the next wave of data-driven innovation. We have an obligation to realise this ambition.\n",
      "1. About the National Data Strategy\n",
      "This strategy sets out how best to unlock the power of data for the UK. It builds upon initiatives such as the Industrial Strategy , the AI Review , the AI Sector Deal and the Research and Development Roadmap – setting out a framework for how we approach and invest in data to strengthen our economy and create big opportunities for us in the future. The government believes that unlocking the value of data is key to driving growth both within the digital sector and across the economy. This will be part of our Digital Strategy, which will be published in the Autumn and will consider more broadly how we can support a digital drive for growth.\n",
      "In this publication, we set out the framework for the approach this government will take, the improvements we seek to deliver and the priority missions we will focus on now to realise that change. The increasing importance of data raises novel and complex policy questions. Some of these need further consideration before the UK government confirms its direction. As such, we are also asking for your views in relation to our general framing, along with some of the actions we are considering. These questions are included throughout the document and also collated in an accompanying publication.\n",
      "This consultation is on a UK-wide basis: we welcome responses from organisations and individuals across the UK. The strategy covers both reserved and devolved areas: where the strategy covers reserved areas (and, in respect of Northern Ireland, excepted areas), it does so for the whole of the UK, and where it covers devolved or transferred areas, it applies to England only. We will publish a response to this consultation in early 2021.\n",
      "A National Data Strategy will require activity and focus beyond government. In this framework strategy we have focused on the government’s role in harnessing data. Following this consultation period, and as we move to implementation, we will work with stakeholders to set out how we will work with business and actors across the wider data landscape to land a strategy for the whole of the UK.\n",
      "What we mean by data\n",
      "Data is notoriously hard to define – and it means different things to different people. For an application developer, data is what enables the creation of rich and complex digital services. For a scientist, it is what is collected as part of experiments or surveys. For a data protection practitioner, it is the names and addresses of staff organised in a spreadsheet. For a personal trainer, it is the information in an app recording our heart rate during a workout. And for every one of us, it is the tool that powers our online maps, helps us book supermarket delivery slots, and allows us to check tomorrow’s weather forecast.\n",
      "When we refer to data, we mean information about people, things and systems. While the legal definition of data covers paper and digital records, the focus of this strategy is on digital information. Data about people can include personal data, such as basic contact details, records generated through interaction with services or the web, or information about their physical characteristics (biometrics) – and it can also extend to population-level data, such as demographics. Data can also be about systems and infrastructure, such as administrative records about businesses and public services. Data is increasingly used to describe location, such as geospatial reference details, and the environment we live in, such as data about biodiversity or the weather. It can also refer to the information generated by the burgeoning web of sensors that make up the Internet of Things.\n",
      "When thinking about the government’s own data use, the strategy covers administrative, operational and transactional data – that is, data collected in the process of running services or businesses – as well as analytical and statistical data.\n",
      "To ensure that data dependency risks are well managed, we are also interested in the infrastructure underpinning the storage of data, such as physical and virtualised data centres/the Cloud.\n",
      "1.1 Evidence and working with stakeholders\n",
      "This strategy, and the structure and substance of the pillars on which it is built, is drawn from a range of evidence sources, including desk research covering both case studies and published academic/sector research.\n",
      "In June 2019, the government launched a call for evidence on the proposed framework of the strategy, receiving over 100 responses. Alongside this, a programme of stakeholder engagement was undertaken, including the hosting of 20 roundtables and workshops, with representatives from over 250 organisations across business, the third sector and local government. Through our call for evidence, roundtables and workshops held across the country in 2019, we consulted on our parameters and objectives, and gathered evidence that has underpinned this framework National Data Strategy.\n",
      "We have analysed the call for evidence submissions and the discussions at the roundtables to make sure our evidence base is as wide and inclusive as possible. A summary of our call for evidence and of the stakeholder engagement can be found in the accompanying publication .\n",
      "Data policy is a rapidly evolving area globally, and for many of the issues highlighted in this strategy a number of questions remain unanswered, with further research and analysis required. As the government takes this strategy forward, we will continue to draw upon available evidence to inform our actions. We will develop a monitoring and evaluation process for the strategy to help ensure it is achieving its intended outcomes, as well as building the evidence base on which we develop and evaluate future policy decisions. As we move to implementation, we will further continue to work with stakeholders to set out how we will engage with business and the wider data economy to land a strategy for the whole of the UK.\n",
      "2. The data opportunity\n",
      "We are currently in the middle of a fourth industrial revolution. Technological innovation has transformed our lives, changing the way we live, work and play. At the same time, this innovation has brought with it an exponential growth in data: in its generation and use, and in the world’s increasing reliance upon it. [footnote 2] By embracing data and the benefits its use brings, the UK now faces tangible opportunities to improve our society and grow our economy. If we get this right, data and data-driven technologies like AI could provide a significant boost to the entire economy. Data can improve productivity and provide better-quality jobs. But it can also transform our public services and dramatically improve health outcomes nationally. It can keep us safe and assist the reduction of crime, speed the journey to decarbonisation, and, used well, drive efforts to create a more inclusive, less biased society.\n",
      "Importantly, data can also be used to harness the potential of regions right across the country, [footnote 3] ensuring that people and organisations from the whole of the UK can benefit from the full value of the digital revolution.\n",
      "Like many things, the use of data also presents risks; those risks need to be fully understood and taken into account. Used badly, data could harm people or communities, or have its overwhelming benefits overshadowed by public mistrust. Equally, misplaced government reluctance to securely share and use data undermines the performance of public services and risks causing harm by missing opportunities to help those most in need. In the same way, unnecessary barriers to technological innovation could drive inefficiencies and slow down growth. So it is vital that we take decisive and evidence-led steps to make the most of data’s potential.\n",
      "We have identified five concrete and significant opportunities for data to positively transform the UK:\n",
      "Achieving these opportunities will not be easy. While they are already being realised in some contexts, the means to do so are missing in many others. There is also a great deal of competition in the data space internationally, combined with differing global views on data and its use. As a digital leader, the UK is well placed to overcome this challenge.\n",
      "In realising these opportunities, the UK can further enhance its world-class status in science and technology, and its reputation for finding pragmatic and innovative solutions to difficult problems. In the years to come, it can use these strengths as a springboard to become a global leader in data as well.\n",
      "2.1 Boosting productivity and trade\n",
      "Data is knowledge. By having access to more of it, combined with the ability to analyse it through modern techniques, we get greater insight into what works and what does not – both in terms of selling products and services, and in terms of making our own processes and practices more efficient. Data therefore has the potential to significantly enhance economic competitiveness and productivity across the UK economy, through new data-enabled business models, as well as through the adoption of data-driven processes by existing businesses.\n",
      "There are various ways of defining and measuring the data market and the data economy. [footnote 4] A range of estimates suggest that the data economy grew about twice as quickly as the rest of the economy during the 2010s, making up about 4% of UK GDP in 2020 . Beyond the impacts of data-driven products and services (i.e. the direct ‘data economy’), the use of data has a more general role in underpinning digitally delivered trade. ONS estimates show that in 2018 the UK exported £190 billion in digitally delivered services (67% of total UK services exports) and imported £90 billion digitally delivered services (52% of UK services imports) . Enabling and growing this data-driven trade will be a priority in our approach to free-trade negotiations.\n",
      "While research into the business impact of increasing and improved data use is in its infancy and methodologically challenging, the existing evidence suggests wide-ranging economic benefits arising from better data use, in particular an association between efficiency, productivity and data-driven business practices. [footnote 5] There are also significant economic advantages from individual companies increasing data access and sharing. For example, Transport for London ( TfL )’s opening up of its data sets to travellers and third-party providers contributed up to £130 million per year to the London economy through time saved by travellers .\n",
      "In recent years, the UK government has taken significant and unprecedented steps to position the UK as a world leader in data-driven innovation. This includes committing to raising investment in (often data-heavy) research and development by 2.4% of GDP by 2027, establishing institutions such as the Centre for Data Ethics and Innovation ( CDEI ) and the Alan Turing Institute, launching brand new conversion courses in data science and AI, and conducting pioneering work on ‘data trusts’ – a novel data-sharing framework. [footnote 6] The importance of continuing and furthering this work will be even more vital in ensuring the UK’s recovery from the coronavirus pandemic, and its economic success in the years beyond.\n",
      "Case study: Sharing data to drive open innovation\n",
      "Open innovation allows companies to apply externally-developed data, ideas and technology to help address challenges. Sharing operational data provides the necessary insights into business challenges, allowing collaborators to analyse and use that data to deliver better insights and demonstrate the value of new technologies.\n",
      "The Open Data Institute documented how an open innovation programme, Data Pitch, has allowed Greiner Packaging International GmbH, a company producing rigid plastic packaging, to share data with logistics intelligence company, Obuu, to help monitor the resilience and efficiency of its supply chain. To do this, Obuu used data to map out shipping, storage and manufacturing flows so they could investigate three key performance indicators for efficiency in the supply chain: whether spare parts were available when needed; the average time the system is down when a part is not available; and the overall investment in the accessible stock. From this, Obuu was able to identify reductions in fixed asset investment, resulting in a significant cost saving .\n",
      "2.2 Supporting new businesses and jobs\n",
      "Data skills – like many digital skills – are increasingly important for all aspects of life, but especially for the working environment. Increasing numbers of jobs require technical data skills. One estimate suggests there was over a 50% increase in data professionals between 2013 and 2020 – increasing from 1.1m to 1.7m employees. However, there are also over 100,000 unfilled data professional posts in 2020. [footnote 7]\n",
      "But beyond these jobs requiring technical data skills, most jobs require some data and digital skills, and they are only set to increase in importance. Estimates from a DCMS -commissioned module of the February 2020 ONS ‘Opinions and Lifestyle’ survey found that nearly half (48%) of the working population use ‘basic’ data skills at work a lot, and just under a quarter (24%) use more advanced skills, such as data analysis and making graphs, a lot in their current job. [footnote 8]\n",
      "We know that data is a basis for the creation of new jobs that use both general and more technical data skills. TfL ’s work on open data (below) is estimated to have created 500 jobs directly, and another 230 indirectly. [footnote 9] More widely, the European Data Portal predicts a baseline scenario of 1.12m open data employees and an optimistic scenario of 1.97m open data employees in the EU by 2025 . And over 6% of new companies in the UK in 2019 were tech start-ups , with data and the aligned need for increasingly sophisticated data skills underpinning their business models.\n",
      "By encouraging and supporting the use of data in the UK, we can ensure that the coming waves of technological innovation do not just drive new services but also foster the creation of new businesses and new jobs for the UK.\n",
      "Case study: Using data in the transport sector\n",
      "‘Intelligent Mobility’, the moving of people and goods in an easier and more efficient way, provides an intersection between traditional transport and exciting new products and services relating to mobile devices, open data, wireless communication or the Internet of Things. According to research commissioned by the Transport Systems Catapult, the international Intelligent Mobility market is estimated to grow to just over £900bn a year by 2025, with data comprising an estimated £32bn of this per year by 2025 .\n",
      "TfL already uses technology and data to make journeys easier for their customers. By making live travel information available, developers can create software and services, such as online maps and journey planners. The data is provided via an API, which unifies the data for different modes of transport into a common format and structure, where historically they have differed and been hard to layer onto each other.\n",
      "This data has been used to develop applications used by the public to plan journeys and check for disruptions and has helped create hundreds of jobs. Recent research by Tech City noted London’s digital economy was worth £30bn in GVA supporting over 300,000 jobs , and the government’s Future of Mobility strategy describes some of the actions being taken to maximise the benefits of data use in the UK’s transport sector.\n",
      "2.3 Increasing the efficiency and scope of scientific research\n",
      "The UK is a leader in science and research, and data is at the heart of it. New scientific developments driven by data have potentially game-changing applications across the economy, such as tracking public health risks and aiding decarbonisation through smarter energy grids, predictive maintenance of infrastructure or better traffic management.\n",
      "While data is critical in all research, some of the clearest examples of the benefits to society are in the life sciences. For example, data has been crucial in recognising and understanding the side effects of drugs, identifying the benefits of surgery for patients with Inflammatory Bowel Disease and demonstrating the impact of anti-smoking laws on the number of babies born prematurely in Scotland . The UK has also published five principles which will underpin the government’s policy framework to govern fair, ethical and appropriate use of health data, while also supporting innovation. More advanced applications of data-driven technology have also provided responses to the coronavirus pandemic, with AI-driven systems being used to predict the virus’s protein structure and determine which drugs may be effective for treatment, helping prioritise promising candidates for real-world trials .\n",
      "However, barriers to accessing data represent a significant limitation on research; these range from legal barriers (real and perceived) through to cultural blockers and risk aversion. These barriers must be addressed if the UK is to remain at the forefront of science and research. For example, recent research into data use by the pharmaceutical and life sciences industry identified a number of systemic barriers that limit access to data. Most companies surveyed noted having experienced delays and uncertainties. These include time taken to access data, access constraints for commercial users, the effort to identify and assess the quality of data sets and, most notably, the cost of the data itself.\n",
      "When dealing with sensitive data, the way forward must be considered and appropriate. For example, NHSX is developing a Data Strategy for Health and Social Care in Autumn 2020, which will aim to build on the permissive approach to data sharing for care seen during the coronavirus response, while still protecting the absolute need for patient confidentiality. A similar balance is needed in other instances where risks must be managed while unlocking the significant opportunities from data use at the forefront of science, research and technological development.\n",
      "Case study: Data-driven clinical trials to tackle coronavirus\n",
      "Conducting clinical trials that are better, faster and more efficient is a UK priority.\n",
      "Electronic Health Records (EHR) play a key role in identifying and enabling individuals from across the UK to take part in clinical trials. The UK is well-placed to take advantage of EHRs due to the unified nature of the NHS within the four nations and because a large portion of NHS records are electronic. Using securely stored data, we can find patients from across the UK that might benefit from a particular trial, rather than just a single NHS Trust or geographical location. All while protecting patients’ data.\n",
      "This approach was tested during the response to coronavirus and the RECOVERY trial for potential treatments. These trials normally take months to set up but, designated as an Urgent Public Health Research study, RECOVERY was set up in a matter of days. 12,000 patients from 176 NHS hospital organisations were enrolled in the trial with data being tracked and analysed.\n",
      "Within 100 days, the trial identified the world’s first coronavirus treatment proven to save lives – Dexamethasone. The results were announced on 16 June 2020, adopted into UK practice later the same day and included in new US guidelines within 2 weeks.\n",
      "RECOVERY was supported by NHS DigiTrials , the Health Data Research UK hub for clinical trials, which provided centrally collected and curated data on a weekly basis so that progress and outcomes of trial participants could be closely tracked. The data was drawn from extracts from routine EHR data, and supported rapid access to new data assets collected in response to the coronavirus pandemic, such as test data, intensive care data, and GP data. The approach not only removed the burden on the NHS ‘front line’ to field additional data requests, but enabled the RECOVERY trial team to make rapid decisions.\n",
      "2.4 Driving better delivery of policy and public services\n",
      "Data can revolutionise the public sector, creating better, cheaper and more responsive services. Public services are complex to deliver, with services such as the pensions system, the benefits system, the NHS, tax and the courts each engaging with millions of people across the UK every year. Likewise, keeping people safe requires access to the right information. These services and capabilities rely heavily on data, but the systems that handle this data have grown iteratively and independently, increasing in complexity over time. Many legacy systems are out of date, costly to operate and incapable of exchanging data with one another, presenting challenges in a world where public services are increasingly interconnected – be that between health and social care provision, tax and benefits or across policing, courts, prisons and probation.\n",
      "Our experience responding to the coronavirus pandemic has demonstrated that when we treat data as a strategic asset and improve coordination between organisations, the delivery of services can be more agile, more innovative, more effective and more cost-effective. Indeed, it has underlined the need for the public sector to move away from a culture of risk aversion towards a joined-up approach, where the presumption is that, with appropriate safeguards, data should be shared to drive better outcomes. The rollout of the Coronavirus (COVID-19) Shielded Patients List showed how much can be achieved through appropriate data sharing across central and local government and the private sector, with over four million support packages distributed to some of the most vulnerable people in society.\n",
      "For central government, better data also means better decision-making. It means policies that can be tailored and delivered more efficiently and significant savings for the public purse. Better evidence on whether policies are delivering their intended effects in different areas and for different groups means interventions can be far more effectively designed. This aligns with the public’s new expectations in our increasingly digital context. As highlighted by the CDEI ’s public attitudes research , for example, ‘there is an expectation that the public sector should use online targeting to ensure that advice and services are delivered as effectively as possible.\n",
      "The problem and the opportunity are not limited to central government. Some of the biggest benefits can be realised by better, more coordinated use of data across the wider public sector – in education, the justice system, health and within local government. As we move to implementation, we will work with partners to better understand the needs and barriers faced by local government in utilising data to its fullest potential. We will cut down on bureaucratic burdens, tackle risk aversion and strengthen the incentives to share data across the public sector. Non-standardisation and a lack of coordination on data mean that data collected by one organisation cannot easily be used by another. This results in duplication of effort and wasted resources. Treating data in the public sector as a strategic asset, with appropriate governance, will save time and money and drive better outcomes for us all.\n",
      "Case study: Data First\n",
      "Data First is an ambitious, pioneering data-linking programme led by the Ministry of Justice (MoJ) and funded by UK Research and Innovation through Administrative Data Research UK. The projects it supports will enable researchers in government, universities and other institutions to securely access anonymised extracts of linked administrative datasets held by MoJ and its executive agencies. This can then be linked with data held by other government departments, such as the Department for Education.\n",
      "Data First will allow researchers to understand how people interact with courts over time and analyse which characteristics influence patterns of frequent use, all to build a much better understanding of what MoJ policies and services are most effective. Researchers will be able to explore how users of the justice system interact with other government services. This will enable a deeper understanding of how the economic, social and educational backgrounds of people who use the justice system influence their needs, the pathways they follow through the system (for example, between the civil and criminal courts) and the outcomes they experience. Such understanding will enable more evidence-informed, targeted support and lead to lower cost, higher-quality public services for everyone in the UK.\n",
      "Data access will be facilitated by the controlled circumstances of the ONS Secure Research Server, an accredited processor under the Digital Economy Act 2017, which complies with the highest standards of data security and protection outlined by the principles of the Five Safes .\n",
      "2.5 Creating a fairer society for all\n",
      "Data holds great potential to empower people and civil society, delivering benefits that reach beyond the economy. Powered by better data, civil society organisations can be better equipped to reach the people most in need, at the time they most need it. Better data use could also significantly decrease operating costs, allowing charities to focus resources on protecting the most vulnerable parts of our society. Charities and other non-profits, and particularly smaller organisations, rarely have access to large enough datasets to be able to prove, to very high levels of certainty, the effectiveness of different interventions. Better coordination, re-use and sharing of data between civil society organisations can also lead to better understanding of societal issues, and of what interventions are effective in supporting those most at need. Data can drive applications that make our digital lives better. Artificial intelligence is increasingly being used to drive automated content moderation online, particularly in social media contexts, where it can help tackle misinformation . Data-driven online profiling technologies can help identify potentially vulnerable web users (such as people suffering from gambling addiction), and target support or prevent them from seeing potentially harmful content.\n",
      "We can harness data as part of struggles to tackle bias and exclusion. Data can be used to hold a mirror up to society – to understand how different groups are faring, and to ensure that government and private sector actions treat people fairly, and are not unintentionally discriminating against protected groups. [footnote 10] We must further ensure that data-driven technologies and AI are a force for good. Biases arising from data or algorithm use will need to be addressed to ensure that data’s potential is harnessed to drive a better, more inclusive and less biased society rather than entrenching existing problems. This is part of using data in an ethical and responsible way.\n",
      "Case study: Domestic abuse statistics tool\n",
      "The Office for National Statistics (ONS) domestic abuse publication brings together data from a range of sources, including the Crime Survey for England and Wales, police recorded crime, other government organisations and domestic abuse services. When taken in isolation, these data sources may not provide the context required to understand the national and local picture of domestic abuse. However, bringing this data together enables appropriate action to be taken to improve victims’ experiences and to help provide a clearer understanding of the criminal justice system’s response to perpetrators of abuse.\n",
      "Alongside the publication, there is also an interactive data tool for domestic abuse statistics. This allows users to explore data for their police force area in more detail and compare this with data from other areas. The tool is intended to help shape the questions that need to be answered by police forces and other agencies working with victims and responding to perpetrators of domestic abuse.\n",
      "The statistics are used to monitor the UK’s progress toward the 17 Sustainable Development Goals adopted by all UN Member States in 2015, as part of the 2030 Agenda for Sustainable Development.\n",
      "2.6 Realising the Data Opportunity\n",
      "While the preceding examples show the significant promise of better data use, there are considerable challenges preventing us from realising this more broadly and consistently across our economy and society. Organisations do not always make the best use of the data they hold, whether due to a lack of skills, a lack of leadership or a lack of resources – government and the wider public sector provide examples of this. Many organisations are limited in their access to data, much of which is controlled by a small number of key players. When data is available, it may be in formats that are unhelpful or of undetermined accuracy. And while the UK does have a wealth of data skills, these are concentrated in areas of UK expertise like science and technology; we have identified an overall lack of data skills across the workforce as a whole.\n",
      "To harness the opportunities and realise our vision, we need to drive improvement across the entire data landscape. Through thematic analysis of the responses to our call for evidence, stakeholder engagement and reviewing the wider evidence base, we have organised this into four highly interconnected pillars that describe the basis for better data use. These are:\n",
      "These are:\n",
      "Data foundations : The true value of data can only be fully realised when it is fit for purpose, recorded in standardised formats on modern, future-proof systems and held in a condition that means it is findable, accessible, interoperable and reusable. By improving the quality of the data, we can use it more effectively and drive better insights and outcomes from its use.\n",
      "Data skills : To make the best use of data, we must have a wealth of data skills to draw on. That means delivering the right skills through our education system, but also ensuring that people can continue to develop the data skills they need throughout their lives.\n",
      "Data availability : For data to have the most effective impact, it needs to be appropriately accessible, mobile and re-usable. That means encouraging better coordination, access to and sharing of data of appropriate quality between organisations in the public sector, private sector and third sector, and ensuring appropriate protections for the flow of data internationally.\n",
      "Responsible data : As we drive increased use of data, we must ensure it is used responsibly, in a way that is lawful, secure, fair and ethical, sustainable and accountable, while supporting innovation and research.\n",
      "To ensure that we drive focused change, we have identified five priority missions (outlined in the next section) where the government will emphasise activity across these pillars to begin realising the data opportunity set out above.\n",
      "Questions on the framing of the strategy\n",
      "We want to ensure that we produce a forward-looking strategy that takes into account public opinion and delivers real change. These questions will help to inform future work that the government will take in this space. It will provide evidence for the government to target areas for intervention in future policy.\n",
      "Q1 . To what extent do you agree with the following statement: Taken as a whole, the missions and pillars of the National Data Strategy focus on the right priorities. Please explain your answer here, including any areas you think the government should explore in further depth.\n",
      "NB: For question 2, we are only looking for examples outside health and social care data. Health and social care data will be covered in the upcoming Data Strategy for Health and Social Care.\n",
      "Q2 . We are interested in examples of how data was or should have been used to deliver public benefits during the coronavirus (COVID-19) crisis, beyond its use directly in health and social care. Please give any examples that you can, including what, if anything, central government could do to build or develop them further.\n",
      "Q3 . If applicable, please provide any comments about the potential impact the proposals outlined in this consultation may have on individuals with a protected characteristic under the Equality Act 2010?\n",
      "Q4 . We welcome any comments about the potential impact the proposals outlined in this consultation may have across the UK, and any steps the government should take to ensure that they take account of regional inequalities and support the whole of the UK.\n",
      "3. Missions\n",
      "This strategy sets out five priority areas of action for the government. By delivering against these missions, we will create the optimal environment for data to drive growth and productivity in the UK for the benefit of all, while helping to solve a number of societal and global issues.\n",
      "Mission one: Unlocking the value of data across the economy\n",
      "Data is an incredibly valuable resource for businesses and other organisations, helping them to deliver better services and operations for their users and beneficiaries. However, there is increasing evidence to suggest the full value of data is not being realised because vital information is not getting to where it needs to be.\n",
      "For example, the Digital Competition Expert Panel’s review of competition in Digital Markets and the Competition Market Authority ( CMA )’s report into online platforms and digital advertising highlighted that smaller companies often do not have the same access to data as tech giants, potentially limiting their participation and innovation in digital markets. Improved public sector access to data can also lead to better decision-making at scale. For example, if the government had better data about infrastructure, it could reduce the disruption caused when underground pipes and cables are struck by mistake, or drive more informed choices about where to build new housing.\n",
      "Our first mission is to create an environment where data is appropriately usable, accessible and available across the economy – fuelling growth in organisations large and small.\n",
      "Much of the transformative potential of data lies in the potential for linkage and re-use of datasets across organisations, domains and sectors. We must ensure that the right conditions and incentives are in place to encourage organisations to work together across the economy, ensuring appropriate and timely access to data that is of sufficient quality. This can aid innovation, ensure the benefits of data can be realised by the maximum possible people in society and aid scientific research.\n",
      "This is not simply a case of opening up every dataset. We must take a considered, evidence-based approach: government interventions to increase or decrease access to data are likely to have myriad consequences, intended and not. There is a balance to be struck between maintaining incentives to collect and curate data, and ensuring that data access is broad enough to maximise its value across the economy. For personal data, we must also take account of the balance between individual rights and public benefit.\n",
      "This is a new and complex issue for all digital economies, one that has come to the fore as data has become a significant modern, economic asset. The first step must therefore be the development of a clearer policy framework to identify where greater data access and availability across and with the economy can and should support growth and innovation, in what form, and what government’s role should be, in the UK and globally. We will move quickly to build that framework in the coming months by: undertaking research to further develop our evidence base on the timely availability of appropriate quality data, set out the economic case and understand the opportunities and rationale for intervention drawing on that work, and our existing evidence base, to scope out government’s potential role, both with respect to short-term quick wins and longer-term projects piloting the most promising interventions, working closely with industry and expert groups working closely with the CDEI , the Open Data Institute ( ODI ) and others to leverage their expertise and capability to support delivery of this agenda\n",
      "We are proposing the creation of a framework to identify where we can and should make data available in the wider economy. There are a number of ways the government can intervene to achieve this goal – including as a collaborator, steward, customer, provider, funder, regulator and legislator. Using Policy Lab’s Style of Government Action could be helpful in thinking about the next few questions.\n",
      "These questions will provide an opportunity for the government to scope out areas of focus for the data availability framework.\n",
      "Data availability : For data to have the most effective impact, it needs to be appropriately collected, accessible, mobile and re-usable. That means encouraging better coordination, access to and sharing of data of appropriate quality between organisations in the public sector, private sector and third sector, and ensuring appropriate protections for the flow of data internationally.\n",
      "Q5 . Which sectors have the most to gain from better data availability? Please select all relevant options listed below, which are drawn from the Standardised Industry Classification (SIC) codes. Accommodation and Food Service Activities Administrative and Support Service Activities Agriculture, Forestry and Fishing Arts, Entertainment and Recreation Central/ Local Government inc. Defence Charity or Non Profit Construction Education Electricity, Gas, Steam and Air Conditioning Supply Financial and Insurance Activities Human Health and Social Work Activities Information and Communication Manufacturing Mining and Quarrying Transportation and Storage Water Supply; Sewerage, Waste Management and Remediation Activities Wholesale and Retail Trade; Repair Of Motor Vehicles and Motorcycles Professional, Scientific and Technical Activities Real Estate Activities Other\n",
      "Q6 . What role do you think central government should have in enabling better availability of data across the wider economy?\n",
      "Q6a . If yes, what is it? If not, why not? How does this vary across sectors and applications?\n",
      "Data foundations : The true value of data can only be fully realised when it is fit for purpose, recorded in standardised formats on modern, future-proof systems and held in a condition that means it is findable, accessible, interoperable and reusable. By improving the quality of the data we are using, we can use it more effectively, and drive better insights and outcomes from its use.\n",
      "Q7 . To what extent do you agree with the following statement: The government has a role in supporting data foundations in the wider economy. Please explain your answer. If applicable, please indicate what you think the government’s enhanced role should be.\n",
      "Q8 . What could central government do beyond existing schemes to tackle the particular barriers that small and medium-sized enterprises (SMEs) face in using data effectively?\n",
      "The Smart Data Review in 2019 consulted on ways to make evolving schemes more coordinated across banking, finance, telecoms and energy. The focus of Smart Data is customers asking their providers to share information about them with third parties who then use this data to offer innovative services to consumers and SMEs.\n",
      "Q9 . Beyond existing Smart Data plans, what, if any, further work do you think should be done to ensure that consumers’ data is put to work for them?\n",
      "Mission two: Securing a pro-growth and trusted data regime\n",
      "As the world becomes increasingly digitised, data has become a central driving force of the modern economy. So it is vital that the UK has a data regime that promotes growth and innovation for businesses of every size, while maintaining public trust.\n",
      "The UK is already a world leader in technological innovation and robust data protection standards: two areas required to build and maintain privacy, security and public confidence.\n",
      "We will build on these strengths to maintain a data regime that supports the future objectives of the UK outside of the EU . A pro-growth legal regime must include consideration of both regulation in the wider digital and technology landscape, which will be addressed in the government’s forthcoming Digital Strategy, as well as our data protection laws.\n",
      "As with all policy areas, the UK will control its own data protection laws and regulations in line with its interests after the end of the transition period. We want our data protection laws to remain fit for purpose amid rapid technological change. Far from being a barrier to innovation or trade, we know that regulatory certainty and high data protection standards allow businesses and consumers to thrive. We will seek EU ‘data adequacy’ to maintain the free flow of personal data from the EEA , and we will pursue UK ‘data adequacy’ with global partners to promote the free flow of data to and from the UK and ensure that it will be properly protected.\n",
      "But data is now a far more influential force in our economy than ever before – with the potential to affect the structure and competitiveness of entire markets. This has serious implications for innovators, not least in the way our approach to data affects the ease, costs and risks of developing new technologies and services. The government needs to create the conditions to support vibrant competition and innovation, which will in turn drive future growth.\n",
      "To build a world-leading data economy, we must maintain and bolster a data regime that is not too burdensome for the average company – one that helps innovators and entrepreneurs to use data legitimately to build and expand their businesses, without undue regulatory uncertainty or risk in the UK and globally.\n",
      "Given the rapid innovation of data-intensive technologies, we also need a data regime that is neither unnecessarily complex nor vague. Businesses need certainty to thrive, and the government will work with regulators to prioritise timely, simple and practical guidance, especially for emerging technologies, and create more opportunities to experiment safely.\n",
      "We want to encourage the widespread uptake of digital technologies more broadly – both for the benefit of the economy and wider society. We will work with regulators to provide more support and advice to small and medium-sized businesses to help them expand online, lifting compliance burdens where possible. We will also prioritise the development of sector-specific guidance and co-regulatory tools to accelerate digitisation across the UK economy.\n",
      "Amid all this technological change, we want people to be active agents in the digital revolution. This is a shared responsibility of both businesses and individuals.\n",
      "Businesses and other data-using organisations should be clear and transparent about how they collect and use data responsibly. Working with the CDEI , the government will partner with industry to identify and incentivise best-practice.\n",
      "People should be empowered to choose whether and how to share data in both the public and private sectors, including where the use of their data can help others. In turn, the government will remain committed to high data protection standards so that data processing is fair and does not result in discriminatory outcomes.\n",
      "People, businesses and other data-using organisations need the right data skills to participate actively. Individuals should have the basic data skills to be able to engage with and understand what is happening to their data. UK organisations will need access to top talent in data science, data engineering and related fields, as well as data-literate workforce.\n",
      "Q10 . How can the UK’s data protection framework remain fit for purpose in an increasingly digital and data driven age?\n",
      "In section 7.1.2 we lay out the functions of the Centre for Data Ethics and Innovation (CDEI), set up in 2018 to advise the Government on the use of data-driven technologies and AI.\n",
      "Q11 . To what extent do you agree with the following statement: the functions for the Centre for Data Ethics and Innovation (CDEI) should be Artificial Intelligence (AI) monitoring, partnership working and piloting and testing potential interventions in the tech landscape? Strongly disagree Somewhat disagree Neither agree nor disagree Somewhat agree Strongly agree\n",
      "Q11a . How would a change to statutory status support the CDEI to deliver its remit?\n",
      "Mission three: Transforming government’s use of data to drive efficiency and improve public services\n",
      "There is massive untapped potential in the way the government uses data. The coronavirus pandemic showed how much can be achieved when government departments and the wider public sector share vital information to solve problems quickly. We have a duty to maintain that high watermark after the pandemic, and will implement major and radical changes in the way the government uses data to drive innovation and productivity across the UK. In doing so, we will improve the delivery of public services, as well as our ability to measure the impact of policies and programmes, and to ensure resources are used effectively.\n",
      "There is already consensus amongst experts both inside and outside government – including academics, civil society and parliamentarians – on the need to address this challenge and to capitalise on the opportunities.\n",
      "However, there are numerous obstacles to achieving our ambitions, many of which are long-term and systemic. These include: real and perceived legal and security risks of sharing data; a lack of incentives, skills or investment to drive effective governance and overhaul data infrastructure; and a lack of consistency in the standards and systems used across the government, making it hard to share data efficiently.\n",
      "These obstacles are not insurmountable, and we have both the ambition and the commitment to tackle them.\n",
      "To succeed, we need a whole-government approach led by a Government Chief Data Officer from the centre in strong partnership with organisations. We need to transform the way data is collected, managed, used and shared across government, including with the wider public sector, and create joined-up and interoperable data infrastructure. We need the right skills and leadership to understand and unlock the potential of data – and we need to do so in a way that both incentivises organisations to do the right thing, as well as build in the right controls to drive standardisation, consistency and appropriate data use.\n",
      "To achieve this objective, we will need to drive change across five key areas:\n",
      "Quality, availability and access : striving towards improved data quality that is consistent, a clear understanding of what data is held and where, better data collection, and efficient data-sharing between organisations. All should be the norm, rather than the exception.\n",
      "Standards and assurance : setting and driving the adoption of standards for data, leading to greater consistency, integrity and interoperability, and enabling data to be used widely and effectively across government.\n",
      "Capability, leadership and culture : developing world-leading capability in data and data science across central and local government, so that leaders understand its role, expert resource is widely available, staff at all levels have the skills they need, and a ‘data-sharing by default’ approach across government tackles the culture of risk aversion around data use and sharing.\n",
      "Accountability and productivity : opening government up to greater scrutiny and increasing accountability, ensuring that this drives improvements in productivity, policy and services for people, while also ensuring data security; and using procurement to drive innovation and better outcomes.\n",
      "Ethics and public trust : this transformation will only be possible and sustainable if it is developed within a robust ethical framework of transparency, safeguards and assurance which builds and maintains public trust in the government’s use of data.\n",
      "The government is going to set an ambitious package of work in this space and wants to understand where we can have the biggest impact.\n",
      "Q12 . We have identified five broad areas of work as part of our mission for enabling better use of data across government: Quality, availability and access Standards and assurance Capability, leadership and culture Accountability and productivity Ethics and public trust\n",
      "We want to hear your views on which of these actions will have the biggest impact for transforming government’s use of data.\n",
      "Q13 . The Data Standards Authority is working with a range of public sector and external organisations to coordinate or create data standards and standard practices.\n",
      "We welcome your views on which if any should be prioritised.\n",
      "Mission four: Ensuring the security and resilience of the infrastructure on which data relies\n",
      "With data now a critical part of modern life, we need to ensure the infrastructure underpinning it is safe, secure and resilient. The infrastructure on which data relies is a vital national asset – one that supports our economy, delivers public services and drives growth – and we need to protect it appropriately from security risks and other potential service disruption.\n",
      "In the UK, the government already imposes safeguards and enforcement regimes to ensure that our data is handled responsibly. But we will also take a greater responsibility in ensuring that data is sufficiently protected when in transit, or when stored in external data centres.\n",
      "The government will determine the scale and nature of risks and the appropriate response, accounting for emerging trends.\n",
      "We will tackle the cyber threats that arise from those seeking to harm the UK head on. We will shape a more secure technology environment, and improve cyber risk management in the economy to make the UK resilient to cyber threats. The increasingly international nature of data collection, storage and transfer can present data security risks. We will determine whether current arrangements for managing data security risks are sufficient to protect the UK from threats that counter our missions for data to be a force for good.\n",
      "Data use creates other risks. Better use of data has the potential to help solve wider climate change problems and help the UK meet its net zero 2050 target, but we will also consider government’s and businesses’ responsibility for the environmental impact of increased data use. We will look to understand inefficiencies in stored and processed data, and other carbon-inefficient processes.\n",
      "The infrastructure on which data relies is the virtualised or physical data infrastructure, systems and services that store, process and transfer data. This includes data centres (that provide the physical space to store data), peering and transit infrastructure (that enable the exchange of data), and cloud computing that provides virtualised computing resources (for example servers, software, databases, data analytics) that are accessed remotely.\n",
      "Q14 . What responsibilities and requirements should be placed on virtualised or physical data infrastructure service providers to provide data security, continuity and resilience of service supply?\n",
      "Q14a . How do clients assess the robustness of security protocols when choosing data infrastructure services? How do they ensure that providers are keeping up with those protocols during their contract?\n",
      "Q15 . Demand for external data storage and processing services is growing. In order to maintain high standards of security and resilience for the infrastructure on which data use relies, what should be the respective roles of government, data service providers, their supply chain and their clients of such services?\n",
      "Q16 . What are the most important risk factors in managing the security and resilience of the infrastructure on which data relies? For example, the physical security of sites, the geographic location where data is stored, the diversity and actors in the market and supply chains, or other factors.\n",
      "Q17 . To what extent do you agree with the following statement: The government should play a greater role in ensuring that data use does not negatively contribute to carbon usage?  Strongly disagree Somewhat disagree Neither agree nor disagree Somewhat agree Strongly agree\n",
      "Mission five: Championing the international flow of data\n",
      "In our hyper-connected world, the ability to exchange data securely across borders is essential. Economically, it drives global business, supply chains, trade and development; it will also be critical in enabling the global recovery after coronavirus. On a personal level, people rely on the flow of personal data to ensure their salaries are paid and to connect with loved ones from afar; this data in particular needs to be suitably protected. Finally, it has a huge impact on international cooperation between countries, including for law enforcement and national security, keeping the public safe.\n",
      "Having left the European Union, the UK now has a unique opportunity – as a world leader in digital and as a champion of free trade and the rules-based international system – to be a force for good in the world, shaping global thinking and promoting the benefits that data can deliver while managing malign influences.\n",
      "Using our international engagement and influence, we will:\n",
      "Build trust in the use of data : We will create the regimes, approaches and tools to ensure personal data is appropriately safeguarded as it moves across borders. This will include looking to secure positive adequacy decisions from the EU to allow personal data to continue to flow freely from the EU/EEA to the UK, implementing an independent UK Government capability to conduct data adequacy assessments for transfers of personal data from the UK, and working with the Information Commissioner’s Office (ICO) to build cooperation between national data authorities. The importance of data to our daily lives has made it a geostrategic tool. We will establish clear expectations of accountability when processing data – to protect personal data when it moves across the globe. These criteria will align with the UK’s stance on promoting its wider values, ethics and national interests.\n",
      "Facilitate cross-border data flows : We will work globally to remove unnecessary barriers to international data flows. We will agree ambitious data provisions in our trade negotiations and use our newly independent seat in the World Trade Organisation to influence trade rules for data for the better. We will remove obstacles to international data transfers which support growth and innovation, including by developing a new UK capability that delivers new and innovative mechanisms for international data transfers. We will also work with partners in the G20 to create interoperability between national data regimes to minimise friction when transferring data between different countries.\n",
      "Drive data standards and interoperability internationally : We will cooperate with nations to develop shared standards that align with the UK’s national interests and objectives. In a global arena, technical standards are increasingly expressions of ethical and societal values, as well as industry best practice. Recognising this, the UK will support global work on interoperability, which will facilitate the combination and cross-referencing of different data sources. This will include support to the collaborative on interoperability, an outcome of the UN World Data Forum in January 2017. The UK Government will also work with like-minded states to seek to ensure our values are considered and incorporated into the standards for new technologies which substantially impact data and their data trail.\n",
      "Drive UK values internationally : The UK will be a champion of good-quality, available data across the globe. We want to ensure that UK values of openness, transparency and innovation are adopted worldwide. Now the UK has left the EU, we have an opportunity to set the UK apart and take an independent, individual approach that extols UK values. National competitiveness and the balance of power internationally are increasingly based on technology and the data that drives it. We want to ensure that UK values of openness, transparency and innovation, as well as the protection of security and ethical values, are adopted and observed globally. The UK will continue to play a leadership role to meet the urgent need for open, inclusive data, and its commitments under the International Aid Transparency Initiative. And we will continue to support the work of the Open Government Partnership to open up governments across the globe.\n",
      "As the UK leaves the EU, we have the opportunity to develop a new UK capability that delivers new and innovative mechanisms for international data transfers.\n",
      "Q18 . How can the UK improve on current international transfer mechanisms, while ensuring that the personal data of UK citizens is appropriately safeguarded?\n",
      "We will seek EU ‘data adequacy’ to maintain free flow of personal data from the EEA and we will pursue UK ‘data adequacy’ with global partners to promote the free flow of data to and from the UK and ensure it will be properly protected.\n",
      "Q19 . What are your views on future UK data adequacy arrangements (e.g. which countries are priorities) and how can the UK work with stakeholders to ensure the best possible outcome for the UK?\n",
      "4. Data foundations: ensuring data is fit for purpose\n",
      "If the UK is to fully realise the benefits of our ongoing technological transformation, we must start by getting the basics right. This means being more effective in how we collect, curate, store, manage and delete data. Left alone, data does not sort itself out. If it is to become a powerful tool that can transform organisations and society, data requires effective governance, management and stewardship. It also requires modern infrastructure, allowing data to be shared across systems that can interact with one another.\n",
      "Data Foundations\n",
      "In this strategy we are using the term ‘data foundations’ to mean data that is fit for purpose, recorded in standardised formats on modern, future-proof systems and held in a condition that means it is findable, accessible, interoperable and reusable.\n",
      "The case for change in public sector data use is clear. Presently, data is not consistently managed, used or shared in a way that facilitates informed decision-making or joint working across government and the wider public sector . Data remains undervalued and underexploited .\n",
      "Modernising the way we manage and share data across government will generate significant efficiency savings and improve services. To succeed, we must expand work to treat data as a strategic asset, and create a whole-government, collectively responsible approach to investing in data foundations, so that everyone can benefit from the improved outcomes data can offer.\n",
      "The picture is more varied across the private and third sectors. While the UK is home to many world-leaders in data use, driving innovation and better services for consumers, this is not universally the case. Responses to our call for evidence and the wider existing evidence base suggest that across all sectors of the economy – perhaps particularly for SMEs and the third sector – issues include a lack of understanding about how data can be used, and used well; these issues are felt at most levels of organisations. [footnote 11]\n",
      "The lack of basic coordination and interoperability both within and between organisations can drive inefficiency, a lack of accountability and an inability to thoroughly evaluate or plan. Data that is not usable, linkable or comparable between organisations means that, nationally, we lose out on the ‘positive externalities of data’ – on the ability to pool data from multiple sources and sectors to create new economic opportunities, or to save lives.\n",
      "Indeed, even those working on advanced technologies report that poor data foundations can be a real blocker for driving the transformative power of data. For example, when the source data needed to power AI or machine learning is not fit for purpose, it leads to poor or inaccurate results, and to delays in realising the benefits of innovation .\n",
      "With better data, we can unlock new opportunities for businesses to grow and innovate. We can vastly improve and streamline public service delivery and offer consumers greater power and choice in the market.\n",
      "4.1 Data foundations in the wider economy and society\n",
      "Poor data quality and, relatedly, a lack of agreed standards are clear barriers to the effective use of data, from basic record-keeping to cutting-edge applications of data-driven technology. Responses to our call for evidence and stakeholder engagements highlighted common issues around: a lack of (central) ownership of data standards/ metadata/ APIs a lack of skills in managing data the pace of change leading to a fragmentation in the systems used to manage data, with ongoing resourcing issues linked to set up and maintenance costs\n",
      "Some respondents suggested these costs could be especially burdensome for smaller organisations, or for organisations who make data as a bi-product of their operations rather than as a discrete business product. There was anecdotal evidence from our call for evidence that in general SMEs find it more difficult than large companies to invest in and maintain high quality data. There are pockets of stronger evidence for particular types of business. For example, the construction industry has well recognised Building Information Management ( BIM ) standards. [footnote 12] However, a range of academic studies find SMEs in construction generally do not use BIM . The issues identified by SMEs include: perception that BIM is only of benefit for larger construction projects high set-up costs of software licensing of software lack of in-house skills and/or cost of training. information retention across platforms (interoperability) – despite the industrial strategy supporting BIM lack of demand from clients (so no push to adopt the greater functionality) [footnote 13]\n",
      "Pulling together the pockets of evidence with the wider anecdotal points, it is likely these issues do act beyond just the construction industry. The wider evidence base on the impacts of improving data ‘foundations’, and conversely the effect of not investing in them, is not especially strong. There appears to be little robust and independent research into the case for – and means of – implementing data quality, standards and management-improvement measures. This could be for a number of reasons: definitions vary, with a lack of consensus on how to measure the constituent elements of ‘data foundations’ most organisations would not want independent analysts studying their proprietary data, because of the risks to reputation or intellectual property\n",
      "This is an apparent evidence gap that the government intends to address. Taking the limited research that does exist, there appears to be widespread concerns for ‘data foundations’ in the private and third sectors, with some consensus that small businesses, charities and SMEs are particularly affected. These problems are seen to result in loss of time and risk lower quality business decisions and operation.\n",
      "Any way forward must be carefully considered, and one size will not fit all. Some barriers to better data use, even when substantiated by evidence, will not necessarily warrant government intervention. Poor data quality within organisations, for instance, is unlikely to warrant government intervention, unless it stops them carrying out a legal or statutory requirement. Even then, it is more likely to be a question for enforcers or regulators (for example, if poor records lead to poor or negligent care, or if an organisation’s poor data management procedures lead to a data breach that warrants ICO involvement).\n",
      "What could warrant intervention is the need to drive better quality, more standardised and interoperable data to help drive economic growth or enable a public good outcome, especially where the value of the data sits beyond its immediate use. The government has taken decisive action to unlock the power of location data and data about the built environment – as exemplified by the Geospatial Commission and through our world-leading National Digital Twin programme . The government has acted to further ADR-UK’s work to transform the way researchers are able to access public sector administrative data by using the Research Powers of the Digital Economy Act (2017). There may be a case for extending this approach to other areas of the economy. In the first instance, we will take the actions outlined in the section below, and are seeking your views where stakeholder input and debate might push the agenda forward.\n",
      "4.1.1 Consolidating a clear framework for government action in the wider economy\n",
      "The government is committed to tackling market failures that mean the foundations of data use in the wider economy are missing or misaligned. Interoperable and consistent data can bring wide economic benefits. However, data of sufficient quality comes with a cost, and businesses can lack the information they need to make resourcing choices. Organisations report being tied into contracts on legacy systems that make collecting and maintaining data in interoperable formats harder. We know that a lack of coordination can act as a barrier to interoperability. And where businesses require data in certain formats for their business practices, such data may be undersupplied by the current market.\n",
      "This is a new and complex issue that has come to the fore as data has become a more significant modern, economic asset. We are committed to addressing these issues, as highlighted in Mission 1 (Unlocking the value of data across the economy) . This is a complex area, with many actors and the potential for unintended consequences. Before moving to consolidate this framework, we are keen to work in partnership with stakeholders as highlighted by our consultation questions and plans for future engagement. We are further working to build on the evidence base by commissioning research that deepens how we understand the government’s role in driving data availability in the economy.\n",
      "The steps we will take to consolidate the framework and approach, working with regulators and the Better Regulation Executive where appropriate, will include the coordination and alignment of existing work to build on data foundations across the economy, such as: working across the physical environment, including the standardisation of data about location, the built and natural environment, and transport and other infrastructures. This will include further developing the Information Management Framework , which will seek to establish a common language by which digital twins can communicate securely and effectively, part of the Centre for Digital Built Britain’s work towards developing a National Digital Twin. maximising the use of trusted data in innovation at the national and local level through supporting the new UK R&D roadmap . This will include taking decisive action to drive the standardisation and interoperability of data for research, science and innovation, and improving the access to trusted data resources at local and regional levels, with further actions to be confirmed as joint work on R&D progresses. bolstering efforts to ensure that consumers’ data is put to work for them\n",
      "Case study: the Food Hygiene Ratings Scheme\n",
      "The Food Hygiene Rating Scheme (FHRS) is an established, government-led open data service. Diners across the UK will recognise the scheme from the green stickers on restaurant windows. The scheme was set up in 2010, with a web service and an associated API created in 2012. The scheme serves in the region of 120 million API calls a year and covers over 400 local authorities in the UK.\n",
      "The Food Standards Agency makes these ratings available as open data to platforms such as Just Eat, Uber Eats and Deliveroo. This data can then be combined with other data sources to support consumers and platforms in making more informed choices about where to eat and which businesses to feature.\n",
      "Behind the FHRS is a complete and rigorous method of food safety and hygiene assurance based on best practice developed over many years. Due to the open availability of its data, the scheme has also been used for purposes beyond its original intention. For example, the FHRS was used by the Department for the Economy in Northern Ireland to help close the business rates gap .\n",
      "The usual method of targeting missing rates is to conduct manual inspection on high value properties, but the Department for Economy used FHRS data, along with other datasets, to work out which properties were likely to be occupied. This targeted approach increased the success rate of inspections in a two-week period and enabled the pilot to identify £350,000 in business rates that were not being collected. In another example, FHRS data was correlated with other datasets allowing it to be used to help identify risks as diverse as the location of fatbergs in the sewers. The power of the FHRS as a tool for consumers lies in its simplicity, which is achieved without compromising its underlying fidelity.\n",
      "4.2 Data foundations across government and the wider public sector\n",
      "In our call for evidence, issues related to ‘data foundations’ were particularly highlighted across government and the public sector. Key issues included: data quality issues, and different standards for data used at all stages of the data lifecycle from collection to publicly available datasets, and the (in)consistent use of metadata – where it was provided at all issues with legacy systems and different, often incompatible systems for inputting and recording data at different stages of the data journey a lack of resources for local authorities to deal with data issues a lack of senior buy-in and leadership on data a lack of alignment across government\n",
      "The case for change for government and the wider public sector’s use of data is well established. The Chancellor of the Duchy of Lancaster stressed the importance of using data more effectively to measure the impact of policies and make the best value-for-money decisions, for the greatest benefit, while increasing accountability. As highlighted by the National Audit Office ( NAO ) report – Challenges in using data across government – too often data is not seen as a key priority, the quality of data is not well understood and there is a culture of tolerating and working around poor-quality data. Inefficiency and cost can arise not just from poor-quality data, but also from a lack of coordination in data systems.\n",
      "The NAO report referenced above also found that a lack of standards across government has led to inconsistencies in the way data is recorded between departments, including identifying numerous methods of capturing and storing data on individuals and businesses. This makes it extremely challenging for the government to get a holistic view of problems and limits the ability of departments to benefit from new technologies and tools.\n",
      "These problems extend into the wider public sector. For example, effective electronic health recording systems play an important role in direct care, service delivery and research. While many of these systems are likely working to the same data standards and many neighbouring Trusts have interoperable systems, a recent study indicated that, of the 117 NHS trusts using electronic health recording systems, 92 of them were using at least 21 different medical records systems, making it harder to coordinate and effectively share information. Ensuring these systems are fully functional and interoperable is vital if we want to continue to realise their benefits. The government took decisive steps at the March 2020 Budget, announcing the establishment of the Data Standards Authority, the Government Data Quality Hub and the development of an integrated platform for data across government. In July 2020, the Cabinet Office also assumed responsibility for government use of data to drive coordinated improvements in the use of data in policy making and service delivery. More remains to be done.\n",
      "4.2.1 Data quality and technical barriers to data use and re-use across government\n",
      "We will improve data quality across the public sector, ensuring that this data is not fragmented, siloed or duplicated across different organisations, and is deleted appropriately. Even the best-quality data cannot be maximised if it is placed on ageing, non-interoperable systems. The government is committed to removing the barriers to data interoperability presented by variations in the hardware and software used across government, [footnote 14] including by using processing techniques that make data ‘independent’ of the infrastructure that contains it.\n",
      "We will: launch a programme of work to tackle the cultural and coordination barriers to good quality data, including: creating a central team of experts able to ensure a consistent interpretation of the legal regime around data sharing launching the Data Quality Framework creating a Data Maturity Model for government building a data management community of good practice learning and setting best practice and guidance through a series of flagship demonstration – or ‘lighthouse’ – projects implement the recommendations of the ‘Joined-up data in government: the future of data linking methods’ report to improve data linkage methods, application and skill sets across government commit to resolving the long-running problems of legacy IT and broader data infrastructure drive data discoverability across government through: developing an Integrated Data Platform for government, which will be a safe, secure and trusted infrastructure for government’s own data. It will be a digital collaborative environment that will support government in unlocking the potential of linked data, building up data standards, tools and approaches that enable policymakers to draw on the most up-to-date evidence and analysis to support policy development, improving public services and improving people’s lives. creating an audit of data inventories work to better support local government in maximising the benefits of data\n",
      "4.2.2. Standards and assurance\n",
      "To ensure that data is reusable and interoperable across government, [footnote 15] we have established a Data Standards Authority, with ongoing work to identify and agree a prioritised list of data standards to adopt across government. In the past, standards around data have been seen as voluntary. The result has been inconsistent adoption and a failure to realise the benefits – we will tackle this through a prioritised approach to mandating certain standards and using spend controls to drive others.\n",
      "We will: develop and validate a set of data principles to be applied across government set out a strategy for standards, to include: clarity on where the Data Standards Authority will mandate some standards use of the DDaT spend controls process a parallel controls process for APIs and Technology Code of Practice to ensure consistent adoption of data standards across government\n",
      "4.2.3. Productivity and accountability\n",
      "To ensure that these changes are effective, we will tackle data governance across government, challenging risk aversion and data-hoarding, driving consistent levels of data maturity and ensuring a joined-up approach to establishing appropriate safeguards. A whole-government approach on data requires oversight and accountability from the top and centre of government, and through each department; we will ensure aligned accountability mechanisms, as well as a set of senior data leaders with the relevant expertise and backing and support from across government.\n",
      "We will: recruit senior cross-government data leadership, including a Chief Data Officer for government establish a cross-departmental governance mechanism with the authority to enforce standards across government drive aligned governance structures across government by: undertaking a review of governance structures for data within departments ensuring central government departments include data management plans in their Single Departmental Plans\n",
      "Case Study: Harnessing the power of administrative data to transform statistical systems for the public good\n",
      "Timely population, migration, social and economic statistics reflecting regional variations better enable the government to proactively respond to trends, and to react more effectively to crises.\n",
      "ONS is using administrative data to transform its statistical systems, efficiently providing more timely, flexible statistics and analysis to decision-makers. These transformed systems will, in turn, provide greater insight across key aspects of our society and the economy, ultimately leading to better outcomes for the public.\n",
      "4.3 Supporting data foundations internationally\n",
      "Beyond our borders, and as brought to the fore most recently, a lack of basic data maturity, standardisation and interoperability on the international stage can mean that it is difficult to thoroughly understand issues that affect us globally, such as the difficulty in comparing transmission or mortality figures in the early stages of the coronavirus pandemic.\n",
      "In the global arena, technical standards are increasingly expressions of ethical and societal values, as well as industry best practice. As we set in place our domestic data strategy, we must engage our global partners to adopt complementary measures so they can fully embrace and harness the innovations that data can bring. We must also work with like-minded states to ensure our values are considered and incorporated into the standards of new technologies which substantially impact data and their data trail.\n",
      "We want to be a data champion across the world.\n",
      "We will: support the global effort on data interoperability, which will facilitate the combination and cross-referencing of different data sources collaborate with our international partners to build strong national statistical systems to drive economic growth and help to deliver inclusive, effective services\n",
      "Case study: Ordnance Survey and Singapore’s exploration of a 3D geospatial data model\n",
      "In 2016-19, following prior joint research programmes, Singapore turned to the UK’s Ordnance Survey (OS) for expertise in geospatial data standards and interoperability to help pave the way and enable enhancements to their 3D ‘digital twin’ of Singapore.\n",
      "OS carried out two projects involving a variety of stakeholders and discovered data requirements. An efficient data-capture process for a variety of city ‘themes’ (e.g. buildings, vegetation, street furniture and transport) was identified and set up. A key part of the challenge was to select relevant information ‘locked away’ in building information management (BIM) models, and to make it accessible through an open standards data model to wider stakeholders, such as city planners or regulatory bodies, who would benefit from the data to test new developments or city planning initiatives.\n",
      "Based on these findings, joint collaboration between OS and the National University of Singapore led to the development of an ‘IFC2CityGML transformation engine’ – a software tool capable of automating the transfer of detailed building model information for different geospatial use cases. The project also helped to improve engagement, understanding and collaboration across the BIM and geographic information systems communities in Singapore, creating a new building theme with enhanced semantic representations for mobility, energy and urban planning. In the UK, OS is continuing to innovate with spatial-visualisation and data-integration techniques, combining them with other technologies such as Artificial Intelligence and machine learning. By applying common standards to practical solutions, OS is ensuring that the right information can get to the right people at the right time – and, crucially, in the right format.\n",
      "5. Skills: Data skills for a data-driven economy and data-rich lives\n",
      "Data skills deliver benefits across the board. Businesses are more likely to be competitive in today’s digital-driven economy if they can use data effectively. Likewise, data-literate individuals are more likely to benefit from and contribute to the increasingly data-rich environments they live and work in, while data-driven companies can deliver significant productivity benefits to their own business and the wider economy.\n",
      "The need for data skills continues to grow across the economy. The Royal Society reports that demand for specialist data skills has more than tripled since 2013, while DCMS -commissioned analysis of 9.4 million online job adverts predicts that data analysis skills will be the fastest growing digital skills cluster over the next five years. This characterises the exponential growth in the demand for advanced applications of data science and machine learning across all sectors of the economy, from cyber to construction. The growth in AI and cyber specialisms also drives the demand for broader supply of data skills at foundational level, to feed the pipeline of advanced skills and to provide business with the foundational skills they need to work with data. Notably, these scarce skills have been critical in the deployment of research capabilities to the coronavirus response. The portion of UK R&D that they support is significant and growing rapidly.\n",
      "Data skills\n",
      "There is no widely agreed definition of data skills. In this document we use the term broadly to cover the full range of basic, technical, governance and other skills – including project management, governance and problem solving – needed by practitioners to maximise the usefulness of data.\n",
      "The required technical skills range from programming, data visualisation, analysis and database management, to core skills such as problem solving, project management and communication.\n",
      "Planning for and delivering data at the right quality requires a wide range of skills that are sometimes underappreciated. Assurance of data requires people familiar with data laws and ethical oversight. Data processing and analysis skills, used to turn data into useful information, span a wide range of capabilities comprising both technical and soft skills.\n",
      "Basic data literacy requires some knowledge of data uses, some ability to assess the quality of data and its application, and the skills to conduct basic analysis.\n",
      "Consultation with data experts, the responses to the call for evidence and a review of existing research identifies a number of challenges involved in helping both individuals and companies develop the data skills they need. These include: Lack of coordinated vision and leadership across multiple industry interests . Many of the issues with data skills lie parallel to the issues with AI and Cyber skills needs. For example, almost half of UK businesses are facing a cyber skills gap . A coherent approach across all skills stakeholders and landscapes will be required, as will enhanced efforts to drive diversity in skills provision. Greater clarity needed in describing data skills required by industry , which will help assessment of individual skill sets and will ensure a better match of new recruits to company requirements. The need for the formal and vocational education system to better prepare those leaving school, further education and university for increasingly data-rich lives and careers . Foundational data literacy will be required by all. Working with industry will be necessary to help ensure that the supply of specialist data skills meets and responds to companies’ changing requirements. Industries needing to develop their understanding of their own data skills needs , including how to define and source these requirements, and how to develop or source employees with the right mix of sector and specialist knowledge. Companies that are able to meet these challenges, particularly through senior buy in and advocacy, will likely thrive in a data-driven economy . A limited pool of data-skilled individuals nationally , with the cost of hiring and retaining such staff preventing the government from accessing the data skills it needs. The NAO ’s ‘Challenges in using data across government’ report also highlights the current gap in skill sets “at several levels”: legal and ethical data use; data storage, management and architecture; and planning and data governance. These points were reiterated in responses to the NDS call for evidence and roundtable discussions.\n",
      "The government is committed to working with the devolved administrations to align activity on advanced data, digital and R&D skills to support vibrant career pathways and to attract talent. Further actions will be laid out through the government’s upcoming Digital Strategy and through the next steps of the R&D Roadmap.\n",
      "5.1 Driving clarity and coordination\n",
      "5.1.1 Definition of data skills and role descriptors\n",
      "Research commissioned by DCMS and the Royal Society , and responses to our call for evidence, all indicate that there is an inconsistent use of data role descriptors – an employee in a data scientist role at one company may have a distinctly different skill profile than a data scientist at another firm. There is often a lack of clear distinction between data skills, digital skills, AI skills and similar terms. This inconsistent use of role descriptors and the lack of clear distinctions between skills complicates skills assessment and makes it difficult to recruit staff with the specific skills required.\n",
      "As highlighted in responses to our call for evidence, there is a need to develop and promote clearer career pathways for individuals looking to work in data roles. This is also true of related roles in AI and cyber security. [footnote 16] Importantly, many of these roles are underpinned by common skills, whether that be specific technical skills or more general aptitude.\n",
      "We will: publish a working definition of data skills for the wider economy, set out a clear distinction between data skills, digital skills and AI skills, and consider the benefits of providing information on pathways into data related careers\n",
      "This will build on industry initiatives, such as the Royal Statistical Society-led project to establish industry-wide professional standards for data science, and existing government initiatives to fully describe data skills within the existing DDaT framework.\n",
      "5.1.2 National leadership in data skills\n",
      "There are a number of national institutions that are involved in data skills related work. These include the Alan Turing Institute (National Institute for Data Science and Artificial Intelligence), the National Innovation Centre for Data, and the ODI . However, their respective roles in addressing the UK’s data skills challenges are not clearly understood by all, which can lead to coordination issues, confusion among industry and the need for a more unified voice.\n",
      "The Data Skills Taskforce was set up – partly in response to a recommendation in Nesta’s Analytic Britain report – to act as a knowledge and best practice-sharing forum across key participants from industry and higher education, and to promote data skills and analytics. It has a wide range of members from industry, academia, Royal Societies and government. [footnote 17]\n",
      "In related areas, the government has created the AI Council and UK Cyber Security Council. As independent bodies capable of providing authoritative advice and representing their communities, these are intended to serve as national leadership in AI and cyber security, respectively. The Data Lab, partly funded by the Scottish government, aims to help Scotland maximise value from data and plays a key role in helping to develop data skills in Scotland.\n",
      "We will: consider the roles of the Alan Turing Institute, the National Innovation Centre for Data, the ODI, the Data Skills Taskforce, the AI Council, the UK Cyber Security Council and others in the data skills ecosystem in order to improve the leadership and facilitation of new and better collaborations between industry, the public sector, universities and institutes\n",
      "5.2 Ensuring formal and vocational education rises to the challenge\n",
      "5.2.1 Schools\n",
      "This data revolution has implications not only for experts with advanced analytical skills, but for the entire UK workforce. While we do not all need to become data scientists, everyone needs some level of data literacy in order to operate successfully in increasingly data–rich environments. [footnote 18]\n",
      "To prepare the workforce of tomorrow to contribute to – and benefit from – a data-rich environment, it is important for everyone leaving our schools and universities to be better prepared for data-rich lives and careers. This has been emphasised by Royal Society and Nesta publications, which call for data science to be integrated across a wider range of subjects. It is similarly important that data science is accessible as an education pathway and to ensure that information about relevant qualifications and skills is widely available, as well as about career opportunities.\n",
      "Post 16 T Levels are being developed to give young people a high quality technical option that delivers on the skills needs of employers. All T Levels technical qualifications will include the digital skills, and the Digital Production T Level will include content on data, digital analysis and software development.\n",
      "Outside the formal curriculum, the education system offers opportunities to capitalise on students’ interest in technical fields, enabling them to develop their technical skills and learn about options for further study and future careers.\n",
      "Further measures will be announced as part of the digital strategy and through the National Skills Fund.\n",
      "5.2.2 Universities and vocational education\n",
      "The government’s Higher Technical Education reforms will establish prestigious qualifications that meet employer needs, promote high quality courses and provision, and encourage HTE to be a more popular choice for learners and employers, starting with the Digital route from September 2022. The Department for Education is also working across government and with the organisations driving innovation, providers and industry on the adoption of emerging skills, and considering how links between universities and regional businesses could be more effectively coordinated.\n",
      "In June 2020, DCMS and the Office for AI announced £13m for the Office for Students to support degree conversion courses in data science and AI , including £10m for up to 1,000 scholarships for people from diverse backgrounds, matched by an additional £11m from universities and industry partners. At least 2,500 graduate places will be created through the programme, with the first courses starting in Autumn 2020. This programme builds on a highly successful pilot of degree conversion courses in data science.\n",
      "The Royal Statistical Society has highlighted the opportunity for UK Research and Innovation ( UKRI ) to further develop and strengthen data skills across the research landscape. There is interest from Research England, Economic and Social Research Council (ESRC) and others to address these gaps across the UK through the potential establishment of Data and Public Policy Centres for Doctoral Training (CDTs).\n",
      "We will: work with the appropriate bodies to understand how data science is integrated into relevant technical qualifications, ensure that good quality data science courses are offered and that data related skills are given due consideration in their work to support emerging skills test the most effective ways to teach foundational data skills to undergraduates in two ways – through offering modules including wider subjects such as AI, cyber and digital skills, and by integrating data skills in other subject areas. Universities will take part in the pilot on a voluntary basis. examine ways of expanding the supply of advanced data skills across research engineers and professionals to help maximise R&D investments and to increase mobility across business and academia and to foster the links between industry and universities at the regional level; this work will build on the interim observations of the UKRI AI Review, which highlights the critical shortage of data capabilities in research professionals across all disciplines.\n",
      "5.2.3 Labour market and industry\n",
      "It is important for the UK to have data skills capabilities in companies, from basic data literacy to advanced technical skills. Those with both advanced data skills and sector knowledge will be in particular demand throughout the UK and internationally, meaning that companies – especially SMEs – will need to have access to viable training options. We will also need to build on the diversity and mobility initiatives in the workplace, and integrate the provision of data skills with the development of business skills at all levels to help develop data-driven companies. We will look to build on the recent government announcement about the establishment of a new Office for Talent that will make it easier for top science, research, digital and technology talent to come to the UK.\n",
      "The Data Lab’s engagement with industry over the past five years points to a need for an improved understanding of: how specific roles and skills are best suited to deliver company requirements. This will help ensure first hires are more successful. the opportunities for data to drive productivity improvements and innovation at leadership and board level the need for all workers at board level to have basic data skills\n",
      "With further measures to be announced as part of the Digital Strategy and through the National Skills Fund, we will: launch an online portal to support businesses’ access to data skills training, helping signpost SMEs to good-quality online training material matched to their technical data science capabilities and ambitions\n",
      "Case Study: Airbus Internal Training Programme – data culture as data capabilities for all\n",
      "Airbus has developed an internal training programme to help equip itself with the data capabilities needed to become a data-driven company. The programme was open to all employees subject to passing an entry test to identify the basic skills required to complete the course. Trainees from all core Airbus countries, including the UK, took part, including subsidiary SMEs.\n",
      "The 9-month part-time programme paired existing business knowledge and analytics skills (acquired during the programme) and was part of the Airbus digital transformation. The learning framework relied on both digital and social/community learning, allowing enough flexibility for each participant to learn and leverage knowledge and experience from other trainees.\n",
      "5.3 Driving data skills across the public sector: capability, leadership and culture\n",
      "Despite the many benefits that can be derived from data, institutionalised data culture – where data is seen as everyone’s job and where data is seen to support outcomes at all levels – is lacking in many organisations across government and the wider public sector. There is not yet a consistent and mature approach to working with data based on knowledge, experience and best practice: we must strive to change this in the same way that the government has worked to instill a culture that emphasises value for money. In the future, the use of data in our work must become the norm, rather than the exclusive domain of specialists.\n",
      "The lack of a mature data culture across government and the wider public sector stems from a fragmentation of leadership and a lack of depth in data skills at all levels. The resulting overemphasis on the challenges and risks of misusing data has driven a chronic underuse of data and a woeful lack of understanding of its value.\n",
      "Senior leadership and effective governance will be key to establishing a data culture across government, but everyone, regardless of seniority or profession, should see data as a priority in their role – with data supporting each step of policy and delivery, from scoping to ongoing performance tracking, evaluation and improvement.\n",
      "Work is underway to drive the importance of improving data culture across government and the wider public sector. At more senior levels, an innovation fellowship scheme, sponsored by No.10, will bring top data skills from the digital and tech sector into government. Fellows will support senior leaders on transformation projects of national importance.\n",
      "With further measures to be announced as part of the Digital Strategy and through the National Skills Fund, we will: prioritise bringing in and building the right skills across government recruit leaders with data and digital skills across government to build a strong cadre of technical, policy, legal and analytical data experts in the centre of government train 500 analysts across the public sector in data science by 2021, through the Data Science Campus at the ONS, the Government Analysis Function and the Government Digital Service. This will be reviewed in 2021 with a new capacity building strategy meeting the emerging needs of government up to 2025. deliver the range of actions to be outlined in the Public Sector Data Science Capability Audit review data training available to all civil servants and develop proposals to enhance and extend this offering design a career pathway for data expertise in government agree a shared definition of data expertise across central government review the needs of local government in having the capabilities to manage, use and disseminate data\n",
      "Case Study: The ONS Data Science Campus\n",
      "In 2017, the Office for National Statistics (ONS) established the Data Science Campus in 2017 with a core of qualified data professionals. The Data Science Campus was established to build skills across the UK and internationally. It accomplished this by building public sector capacity and capability, strengthening the evidence base around data skills, investigating new sources of data and enhancing analytical methods and approaches to data policy.\n",
      "The campus has set up a series of data projects to provide insight into key policy themes, creating new learning and development pathways in data science at a range of different levels from Level 4 Apprenticeships to providing support for PhDs and post-doctoral projects. Courses on offer include: The Data Science Accelerator: A 12-week mentoring programme for public sector analysts delivered up to 3 times a year by the ONS and Government Digital Service on behalf of the Government Data Science Partnership Direct Training: The Data Science Campus and the Government Statistical Service deliver a range of training courses in data science and advanced analytical techniques to individuals and organisations across the public sector MSc in Data Analytics for Government: This part-time MSc programme is aimed at existing public sector analysts seeking to upskill in data science, and is delivered by Cardiff, Glasgow, Oxford Brookes, Southampton, and UCL on behalf of the Data Science Campus\n",
      "6. Availability: ensuring data is appropriately accessible\n",
      "With the increasing ascendance of data, [footnote 19] it has become ever-more important that the government removes the unnecessary barriers that prevent businesses and organisations from accessing such information.\n",
      "Data availability\n",
      "The terms data sharing, data discoverability, data access, data availability, data portability and data mobility are often used in combination and interchangeably.\n",
      "In this strategy we use ‘data availability’ to mean an environment which facilitates appropriate data access, mobility and re-use both across and between the private, third and public sectors in order to generate maximal economic and/or societal benefit for the UK.\n",
      "The advent of new technologies models (e.g. Cloud, edge, secure) are making it increasingly feasible to allow multi-party access to data in secure and privacy-enhancing ways, for example through secure research environments, or through attribute exchange models. As we move to implementation, we will consider the government’s role in supporting technologies that allow for this access, as well as governance and organisational models for data sharing or stewardship.\n",
      "The importance of data sharing was demonstrated during the first few months of the coronavirus pandemic, when government departments, local authorities, charities and the private sector came together to provide essential services. One notable example is the Vulnerable Person Service, which in a very short space of time enabled secure data-sharing across the public and private sectors to provide millions of food deliveries and access to priority supermarket delivery slots for clinically extremely vulnerable people.\n",
      "Aggregation of data from different sources can also lead to new insights that otherwise would not have been possible. For example, the Connected Health Cities project anonymises and links data from different health and social care services, providing new insights into the way services are used.\n",
      "Vitally, data sharing can also fuel growth and innovation. [footnote 20] For new and innovating organisations, increasing data availability will mean that they, too, will be able to gain better insights from their work and access new markets – from charities able to pool beneficiary data to better evaluate the effectiveness of interventions, to new entrants able to access new markets. Often this happens as part of commercial arrangements; in other instances government has sought to intervene where there are clear consumer benefits, such as in relation to Open Banking and Smart Data. Government has also invested in the research and development of new mechanisms for better data sharing, such as the Office for AI and Innovate UK’s partnership with the Open Data Institute to explore data trusts . [footnote 21]\n",
      "However, our call for evidence, along with engagement with stakeholders, has identified a range of barriers to data availability, including: a culture of risk aversion issues with current licensing regulations market barriers to greater re-use, including data hoarding and differential market power inconsistent formatting of public sector data issues pertaining to the discoverability of data privacy and security concerns the benefits relating to increased data sharing not always being felt by the organisation incurring the cost of collection and maintenance\n",
      "This is a complex environment, and heavy-handed intervention may have the unwanted effect of reducing incentives to collect, maintain and share data for the benefit of the UK. It is clear that any way forward must be carefully considered to avoid unintended negative consequences. There is a balance to be struck between maintaining appropriate commercial incentives to collect data, while ensuring that data can be used widely for the benefit of the UK. For personal data, we must also take account of the balance between individual rights and public benefit.\n",
      "This is a new issue for all digital economies that has come to the fore as data has become a significant modern, economic asset. Our approach will take account of those incentives, and consider how innovation can overcome perceived barriers to availability. For example, it can be limited to users with specific characteristics, by licence or regulator accreditation; it can be shared within a collaborating group of organisations; there may also be value in creating and sharing synthetic data to support research and innovation, as well as other privacy-enhancing technologies and techniques.\n",
      "6.1 Data availability for the economy and society\n",
      "There is increasing evidence suggesting that the full value of data is not being realised in the economy, and that government intervention is necessary to address specific market failures in this area. The report of the Digital Competition Expert Panel and the CMA ’s market study into online platforms and digital advertising highlight data concentration and lack of interoperability as a critical factor to suboptimal competition and innovation in digital markets. Alongside this, research by the Bennett Institute and ODI argues that government intervention is necessary to realise the full value of data in the UK. Economic analysis by Ctrl-Shift uses the observed productivity and efficiency increases seen following the introduction of Open Banking to the financial sector, and looks at how equivalent changes would impact a variety of other sectors given differing levels of data use across them. This analysis suggests an average 1.4% increase to UK GDP, which in 2017 would have constituted a £27.8bn increase. The contribution to the economy that digital innovation represents is likely to be significantly greater, given multiplier effects.\n",
      "Given this evidence, we anticipate that in certain circumstances increasing data availability across the wider economy and society has the potential to support greater innovation and drive economic growth. This would ensure that the benefits of data are realised by the maximum possible number of people in society and further aid scientific research.\n",
      "These issues are addressed in Mission 1 (Unlocking the value of data across the economy) . In addition to this, the government will continue to take action in a number of more specific areas, set out below.\n",
      "6.1.1 Ensuring consumer’s data works for them: Smart Data\n",
      "Smart Data enables consumers and SMEs to simply and securely share data that firms hold about them with authorised third parties. The first and most advanced Smart Data initiative, Open Banking, has over one million users and an estimated gross annual benefit of £12bn for consumers and £6bn for SMEs . Following the issuing of the Retail Banking Market Investigation Order 2017 by the CMA , the advent of interoperable formats and new data flows have enabled new, innovative services to develop while increasing competition in the banking market and beyond, as different providers are able to offer services built on this newly available data.\n",
      "For too long it has been unnecessarily difficult and time consuming for consumers to access and use the data that suppliers hold about them, or to access innovative new services that use this data. The government is committed to an economy where consumers’ data works for them, and innovative businesses thrive. We expect that, in time, the extension of Smart Data will deliver new and innovative services, stronger competition in the affected markets, and better prices and choice for consumers and small businesses, including through reduced bureaucracy. Competitive data-driven markets can reduce friction for business and drive start-ups, investment and job creation.\n",
      "In the 2019 Smart Data Review, the government committed to supporting existing initiatives in other regulated sectors such as finance, energy, telecoms and pensions. We believe that such measures will allow us to solve this problem, and that government intervention is needed to coordinate and incentivise action across sectors.\n",
      "We are committed to an economy where consumers’ data works for them, and innovative businesses thrive. As announced in the parallel Smart Data Review response, we continue to advance the Smart Data agenda. The extension of Smart Data will deliver new innovative services, stronger competition in markets and better prices and choice for consumers and small businesses.\n",
      "We will: establish a cross-sector Smart Data working group, which will coordinate and accelerate existing Smart Data initiatives in communications, finance, energy and pensions, while providing recommendations to support the development of high-quality standards and systems across sectors introduce primary legislation, when parliamentary time allows, to improve our ability to mandate participation in Smart Data initiatives and provide a legislative footing for all initiatives\n",
      "6.1.2 Ensuring digital markets work effectively\n",
      "Data lies at the heart of dynamic and competitive digital markets; it is central to the future of the UK’s economy. We need the right incentives and structures to create, share and use data safely, and with consumer confidence, to drive the provision of new, improved and innovative digital products and services.\n",
      "However, the economic characteristics of data may mean that it is not always allocated efficiently. The value of large, aggregated datasets set against the high costs required to build them leads to economies of scale, with the potential to inhibit market entry, undermine effective competition, reduce data sharing and result in unequal access to consumer data. A poorly designed regulatory regime can also reduce access to data. A pro-growth data regime must not drive concentration of data or limit data sharing at the expense of those individuals it seeks to empower.\n",
      "The government has accepted in principle all six of the strategic recommendations made by the Digital Competition Expert Panel . The government has also established a cross-regulator Digital Markets Taskforce to consider the functions, processes and powers which may be needed to promote competition. The Taskforce is based in the Competition and Markets Authority ( CMA ), and draws on the expertise of ICO and Ofcom. The government is committed to ensuring that our overall approach to digital regulation is proportionate and supportive of innovation.\n",
      "We will: ensure that the findings of the CMA’s online platforms and digital advertising report inform our development of a clear policy framework; the framework will identify where greater data access and availability across and with the economy can and should support growth and innovation, in what form, and what government’s role should be in supporting the market\n",
      "6.1.3 Open data\n",
      "Since its introduction in the 2012 White Paper, ‘Unleashing the Potential’ , the government has adopted the policy of ‘Open by Default’ for public sector data across all departments. This approach promotes the concept of open data release for a number of desired outcomes: accountability (by openly publishing data and the evidence base behind policy, government will drive trust in decision making) efficiency (publishing data to identify duplication,waste and other systemic issues that can be reviewed and remedied) economic outcomes (catalysing the growth of innovative companies using data as the basis of new products and services)\n",
      "By improving access to government-owned datasets – for example, by making them open – we unlock an abundance of value that can create and improve marketplaces to better meet people’s needs. By moving away from document-based processes and systems, to one that is based on standardised data, we better support burgeoning industries like ‘PropTech’ (Property Technology). These industries are then equipped to drive innovation and improve people’s engagement and experience with different services.\n",
      "For open data to flourish, a number of underlying policies and mechanisms have been created. These include the technical frameworks for data use, governance forums and international commitments to transparency. Over time, the data landscape has evolved, and issues of impact measurement, effective governance and awareness for public officials have been raised. The government seeks to address these issues, and to drive the agenda to ensure that public sector open data is the backbone of innovation, efficiency and growth.\n",
      "We will: review open data publication and decision-making processes to ensure their consistency; and support development of interoperable metrics to measure the impact of published data continue work to implement the recommendations of the Energy Data Taskforce and drive forward the Modernising Energy Data Access programme\n",
      "Case Study: modernising the energy sector and regulatory frameworks\n",
      "The Energy Data Taskforce, commissioned by government, Ofgem and Innovate UK, outlined a series of recommendations with the goal of digitalising the energy sector . Broadly, these recommendations advocated a ‘presumed open’ data triaging model, centred around the sector making its data more visible and accessible through better data management and cataloguing. This mission is already essential, but will become even more critical when the energy system includes large amounts of low carbon energy demand, such as electric vehicles and heat pumps, and distributed generation such as solar panels. System planners and operators will need this data to make optimal decisions. Government is working closely with the Ofgem, Innovate UK and other industry stakeholders to implement this vision through the Modernising Energy Data work.\n",
      "Regulators, such as Ofgem, need to keep pace with technological change to enable better outcomes for society and the environment, at least cost to business. When regulators adopt agile regulatory approaches, the outputs are often decisive and impactful. The Better Regulation Executive is exploring how the Taskforce’s recommendations might be adopted by other regulators and sectors to ensure best use of data in supporting the economy, enabling innovation and increasing transparency as part of the UK having a world-class regulatory system.\n",
      "6.1.4 Shared models for deriving value from public and private data assets\n",
      "Beyond the commitment to open data, the government has long recognised that new models and approaches are needed to drive value from data and data systems that span the private and public sector – this is particularly important in cases where the data itself is not appropriate to be shared as open data, be it for privacy, national security or commercial reasons.\n",
      "This is well illustrated by the work of the National Infrastructure Commission and the Geospatial Commission, as further explored in the case studies below. To ensure that our focus on data availability supports both growth and the public good, the government will ensure an aligned approach to deriving value from these assets, supporting the future infrastructure required for emerging technologies such as driverless cars and smart cities.\n",
      "This shared approach will be increasingly needed across the economy and society. For example, to improve systems for detecting and addressing online harms, the government is launching a £2.6m programme that will help companies to develop AI-based solutions to tackle these issues ever more effectively.\n",
      "We will: review and upgrade the data infrastructure that underpins the monitoring and reporting of online harms such as child sexual abuse, hate speech and self harm and suicide ideation\n",
      "Case study: Data for the built environment\n",
      "The National Infrastructure Commission (NIC)’s ‘Data for the Public Good’ report made the compelling case that the structured use of data originating from emergent digital twin technology of individual or connected assets would bring significant operational efficiencies, improve public services and significantly contribute to the UK’s net zero carbon commitments. In infrastructure alone, the NIC estimates that a saving of £7bn could be achieved through better use of data. The report also highlighted that the value and usefulness of this data would significantly increase if aggregated and shared effectively between organisations for both business and public good.\n",
      "In response to the recommendations from the report, the ‘Centre for Digital Built Britain’ (a partnership between the government and the University of Cambridge) established the ‘National Digital Twin Programme’ (NDTP) to provide a national focus to efforts to deliver this vision. The NDTP has already produced the ‘Gemini Principles’ which provide the values to guide the development of individual and connected digital twins, to ensure these deliver public benefit in perpetuity.\n",
      "The NDTP has also established an expert technical group drawn from government, industry and academia, who are now laying down the foundation for the ‘Information Management Framework’ (IMF). The IMF will create and enable the adoption of the common information management components to enable the integration of data in a consistent, resilient and secure manner across organisations and sectors. As the IMF is developed and approved, its parts will be made available nationally to progressively enable a UK system of trusted, decentralised and interoperable information exchange. This will create a data infrastructure for the built environment, and pave the way for the National Digital Twin.\n",
      "Image source: Project Iceberg, FutureCitiesCatapult © 2017\n",
      "Case Study: National Underground Assets Register\n",
      "In 2019-20, the Geospatial Commission launched two pilots in London and the North East of England to test the feasibility of creating a national underground asset register (NUAR), validate assumptions about the value of such a register, and seek feedback from planners and excavators. Findings from these pilots indicate significant economic and social value would be unlocked by creating a national register and ensuring field operatives have accessible data to carry out their work safely and effectively.\n",
      "Given the significant economic value a national register will have to the UK economy (estimated at at least £245m per annum) and the pressing need to improve worker safety, the Geospatial Commission is preparing for a national rollout of NUAR which will be used by all asset owners to share data for the purposes of safe digging. To make this possible, they are addressing common barriers related to infrastructure data sharing. However, it is possible that an asset owner may still refuse to participate, even after reasonable requirements have been accommodated. Were this to happen, the asset owner would be limiting the value of NUAR to other participating organisations and the effectiveness of the platform in improving worker safety, especially in emergency situations where immediate data access is essential.\n",
      "To maximise the benefit for the National Platform to all end users, the Geospatial Commission has been asked by asset owners and other stakeholders to mandate the sharing of data through the platform and are considering options, including the need to legislate.\n",
      "6.2 Data availability within government and the public sector\n",
      "6.2.1 Frameworks to enable public sector data sharing\n",
      "Barriers to the sharing, linking and reuse of data across government and the wider public sector are well understood. As outlined in Mission 3: Transforming government’s use of data and the Data Foundations pillar, there is work underway to ensure that data is reusable across government, and that systems are in place to allow for that re-use.\n",
      "Legislative barriers – perceived and genuine – have also historically prevented greater data sharing in the public sector. Historically, a proliferation of powers to share data for specific purposes has made it difficult for public authorities to understand what data can be shared, and, where powers did not exist, it could take years to establish legislation to introduce new data sharing powers.\n",
      "The government has already taken measures to address these issues and simplify public sector data sharing. The Digital Economy Act 2017 sought to reduce these legal barriers, with the introduction of legal powers for the sharing of publicly held information for a specific purpose.\n",
      "Research into the use of the public service delivery power within the Digital Economy Act (2017) identified additional barriers, ranging from resource constraints and a lack of awareness of the powers, to cultural barriers centred around a nervousness around data sharing.\n",
      "These issues are addressed in Mission 3: Transforming government’s use of data) .\n",
      "In addition, we will: drive use of the Digital Economy Act (2017) powers, as well as addressing barriers to data sharing more widely\n",
      "Case study: Better data sharing to improve the lives of children and families\n",
      "The Troubled Families Programme , administered by the Ministry of Housing, Communities and Local Government (MHCLG), is designed to help local services improve the way they support families experiencing problems such as poor health, domestic abuse, addiction, poor school attendance and unemployment.\n",
      "Since 2015, the second round of the programme has helped 350,105 families, including supporting 30,000 adults into sustained employment. The programme encourages services to: identify family problems early; work together to understand the needs of the whole family instead of responding separately to individual problems; coordinate support; track whether the support has improved outcomes or not; and re-design and transform services through digital tools to support better outcomes. None of this is possible without effective data sharing.\n",
      "Dorset County Council and Bristol City Council have both set up needs analysis systems to identify vulnerable children and families at an early stage. Using legal gateways like the Digital Economy Act’s power to reduce multiple disadvantages, these systems bring together relevant data from local public service partners such as attendance, employment, anti-social behaviour and crime, to identify which families are experiencing multiple problems and should be prioritised for targeted and tailored support from family support workers, schools, health or another local service to stop problems escalating further.\n",
      "The benefits of this approach for families and the public purse have been shown in the programme’s national evaluation of impact , which is itself one of the biggest data-linking exercises in government, bringing together administrative data on over one million individuals and around three hundred and fifty thousand families, from local government and four central government departments. MHCLG worked with the Office for National Statistics (ONS), acting as a trusted third party, to match the data together from the local authorities and government departments to create an individual and family level dataset for analysis. The results showed positive impacts on a number of outcome measures and a good fiscal and economic case for the programme. The evaluation won the 2019 Civil Service Award for Innovation and Science, beating over 1200 other nominations. It was described as ‘one of the most complex ever attempted in social policy’ and a ‘landmark study in terms of its methodological sophistication’.\n",
      "6.3 International data availability\n",
      "It is hard to overstate the importance of flows of data across borders to support economic development and global cooperation. Accurate, available data of appropriate quality can help to improve transparency, accountability and economic activity, which are all critical in creating more stable and prosperous countries across the globe. International data flows drive global business operations, e-commerce, supply chains and trade in goods and services. They also support cooperation between policymakers, law enforcement, regulators and academics – the need for this cooperation has been clearly demonstrated by the coronavirus pandemic.\n",
      "The proposition to our international partners is that data can be used to drive innovation, the economy, governmental cooperation and trade without compromising safety, security or privacy. We will take a holistic approach to enabling global data, through the removal of unjustified barriers, the development of frameworks for the transfer of personal data, and, where appropriate, by helping our international partners to increase data availability in their own countries.\n",
      "6.3.1 Removing barriers to data flows\n",
      "Unjustified barriers to cross border data flows, such as measures that require the use of local computing facilities as a condition for conducting business in that country, can be a barrier to innovation, market access, and trade. The UK will take a leading role in encouraging the removal of such barriers to unlock the growth potential of global digital trade.\n",
      "We will: seek provisions with trade partners – including current negotiations with the EU, US, Japan, Australia and New Zealand – that remove unnecessary barriers to cross border data flows, with specific commitments to prevent the use of unjustified data localisation measures advocate for the importance of global data flows in the World Trade Organisation (WTO), G7, G20 and Organisation for Economic Co-operation and Development (OECD) draw upon the expertise of the UK co-chaired Data Governance Working Group under the Global Partnership on AI to work with international partners and explore approaches to international data access and sharing\n",
      "6.3.2 Personal data transfers\n",
      "Digitally-delivered trade, which requires and generates data flows, has expanded rapidly in recent years, creating great opportunities for businesses and consumers. However, such trade faces significant challenges, such as fragmented transfer mechanisms and rising restrictions on cross-border data flows .\n",
      "The importance of data to the daily lives of modern citizens has made it a geostrategic tool. The government is committed to supporting international data flows while ensuring that transfers of personal data from the UK uphold high data protection standards. The UK must take responsibility for the different means by which personal data may be lawfully transferred to countries outside of the UK. In doing this we will ensure that UK businesses, charities and public sector organisations have effective and efficient mechanisms to transfer personal data from the UK, while safeguarding people’s data. We will establish clear expectations of accountability to protect personal data when it moves across the globe. These criteria will align with the UK’s stance on promoting its wider values, ethics and national interests.\n",
      "We will: establish an independent HMG capability to conduct the UK’s own data adequacy assessments for transfers of personal data from the UK review the transitional arrangements for international data transfers review the use of alternative transfer mechanisms which ensure that transfers of personal data outside the UK are appropriately protected seek positive adequacy decisions from the EU, under both the General Data Protection Regulation (GDPR) and the Law Enforcement Directive (LED), before the end of the transition period\n",
      "6.3.3 Supporting availability in other countries\n",
      "Available data is essential to understand and tackle global issues. The fight against climate change, international crime and the coronavirus pandemic are not confined to the borders of one country. Addressing them head on is made easier if data is available.\n",
      "The UK has a strong track record here. For example, in 2019 cyclone Idai caused catastrophic damage over 17 days in Mozambique. The UK’s response included collating evidence from a range of science and data providers. Using available and standardised data, we developed flood forecast and population exposure maps allowing response teams on the ground to prioritise where immediate action was needed. The UK has also helped to strengthen the capacity of the UK’s international partner’s National Statistics Systems to make data available, including setting up an Open Sustainable Development Goals platform.\n",
      "We will: support countries to take a more open approach to their data and will continue to play a leadership role on the open data agenda internationally develop methods to use big data and modelling analyses to support a greater resilience of vulnerable countries to extreme weather events and disease outbreaks, as part of our Official Development Assistance support the implementation of standards such as the International Aid Transparency Initiative open data standard, Extractive Industries Transparency Initiative and Infrastructure Transparency Initiative work with international agencies such as the Red Cross and the UN to ensure data on crisis affected areas is handled safely, legally and ethically\n",
      "7. Responsibility: driving safe and trusted use of data\n",
      "The UK is already a major data user. This strategy sets out our ambition to make even greater use of data, recognising the benefits this will bring to all.\n",
      "In order to reap the benefits of greater data use, we must maintain a fit-for-purpose legal and regulatory regime capable of keeping pace with, and responding to, the increasing importance of data in our economy, society and lives. A regime that reflects what people really care about and preserves their trust, while also enabling the opportunity that responsible data use creates.\n",
      "The ever-growing importance of data will also increase our dependence on the infrastructure on which data relies, as well as on the systems and services that keep data both secure and accessible.\n",
      "Responsible data\n",
      "In this strategy, we use ‘responsible data’ to mean data that is handled in a way that is lawful, secure, fair, ethical, sustainable and accountable, while also supporting innovation and research.\n",
      "Getting safe and trusted use of data right requires action at all levels of society: Government has a responsibility to ensure that there is a clear and predictable legal framework for data use that can both spur the innovative use of data, especially for purposes in the public interest, and earn people’s trust. A pro-growth legal regime requires the consideration of regulation in the wider digital and technology landscape, which will be addressed in the government’s forthcoming Digital Strategy, as well as in our data rights regime, explored in chapter 7.1. The government has a further responsibility to ensure that the infrastructure on which data relies is secure, sustainable and resilient enough to support ongoing digitalisation, economic growth and changes to the way that we live and work. The government must also be transparent and prepared to open itself up to scrutiny over its own use of data. Organisations have responsibilities to upskill themselves so that they can both manage and use data efficiently as a strategic resource, and ensure such use is lawful, secure, unbiased and explainable. We want businesses and other organisations to place a greater value on ensuring that they have the right skills to collect, organise and manage data. This will bring collective benefits to the wider economy and to society. There is also a growing need to ensure that security is incorporated as part of product and system design. Currently, almost half of UK businesses have identified a cyber security breach or attack in the previous 12 months . To be effective, organisations must also ensure that they account for biases arising from data or algorithm use, as identified in the CDEI ’s interim report . Individuals should be empowered to control how their data is used, and supported to have the necessary skills and confidence to take active decisions around the use of their data, in order to contribute to the wider societal benefit data can offer. Recent Ofcom research into consumer experiences online found that over 80% of those surveyed had concerns about using the internet, with 37% having specific concerns relating to data or privacy. Nonetheless, there is public benefit in the collective use of data that may derive from the activity of individuals. For example, our individual medical data has been critical in tracking the coronavirus pandemic and in making collective decisions about how quickly and in what areas it is safe for the UK to ease restrictions. Data about individuals is also critical to understanding threats to our collective security. We want people to recognise their responsibility to consider how their data – used responsibly and fairly – can create a better society for all. In particular, we want to strengthen the existing understanding that aggregated data about people – used responsibly and fairly – can have public benefits for all. Where people understand why data about them might be required, they tend to support it being used for the broader good. For example, a recent government survey revealed that 79% of british adults said that they would share some of their medical data if it helped develop new medicines or treatments. Enhancing individual awareness about the public benefits of data use requires transparency about such benefits, as well as a commitment to ensuring that people have trust and confidence in the use of their data and that it is adequately safeguarded.\n",
      "7.1 A pro-growth data rights regime\n",
      "As highlighted in Mission 2: Maintaining a pro-growth and trusted data regime , we will work to maintain a pro-growth data regime that the public trusts. We will focus on the key areas set out below.\n",
      "7.1.1 Helping organisations to comply with the law\n",
      "Responses to the NDS call for evidence highlighted a lack of clarity about certain aspects of data protection rules and regulations, which cause particular difficulty for SMEs. Businesses should not be driven to costly over-compliance or high risk aversion with respect to data sharing by unnecessary complexity or vagueness in the regulatory environment. This limits the societal benefits of responsible data sharing.\n",
      "We will: work in partnership with the ICO and other bodies to clarify aspects of the UK’s existing data regime that generate confusion or inertia, including by fast-tracking guidance and the use of co-regulatory tools work in partnership with the ICO to lift compliance burdens wherever possible on businesses, especially SMEs boost proactive advice and support for innovators, including via world-leading interventions such as the ICO’s regulatory sandbox\n",
      "7.1.2 Fairness, transparency and trust\n",
      "Our data regime should empower individuals and groups to control and understand how their data is used. It should also instil confidence in individuals, increasing their comfort with the use of data, including their personal data, to deliver benefits for the whole of society. Principles of fairness and transparency are central to the UK’s data regime, which also safeguards people’s access to their data and to information about how their data is collected, shared, analysed and stored. These principles require active interpretation and application to new and emerging technologies, such as big data techniques and machine learning. The use of algorithms has the potential to improve the quality and speed of decision-making, but there are also risks of human-introduced bias, discriminatory outcomes or unsafe applications, which must be mitigated if we are to harness their benefits.\n",
      "In particular, the transformation of government data, and the data-driven transformation of government, will only be possible and sustainable if it is built upon a sound ethical and legal framework which engenders public trust. People need to have confidence that the government is collecting, storing and using their data safely and securely, in accordance with the highest standards of ethics, privacy and security.\n",
      "A wide range of research suggests transparency around how data is used is important for building public trust , and the importance of trust as an enabler for public sector data sharing . Evidence on existing levels of public trust in government data use is mixed; estimates from a DCMS commissioned module of the February 2020 ONS ‘Opinions and Lifestyle’ survey found that nearly half of adults (49%) trust central government with data about them [footnote 22] (comparable to the estimate of those who would trust families and friends at 50%). In contrast, a 2019 study by the ODI found that only 30% of people trust central government to use their personal data ethically. These differences may stem from different methodologies.\n",
      "Initiatives such as Project ExplAIn, a collaboration between the ICO and the Alan Turing Institute, are creating practical guidance to assist organisations with explaining artificial intelligence (AI) decisions to the individuals affected. The ICO has also recently published an AI Auditing Framework , focusing on best practices for data protection compliance. Nonetheless, a number of expert institutions – including the Alan Turing Institute, the Ada Lovelace Institute, the Oxford Internet Institute and AI Now – have emphasised the need for greater algorithmic transparency, particularly within the public sector.\n",
      "We recognise and commit to addressing the need to develop appropriate mechanisms for increasing transparency and accountability in decisions made or supported by algorithmic systems, and for monitoring their impact. We will therefore collaborate with the leading organisations and academic bodies in the field to scope and pilot methods to enhance algorithmic transparency.\n",
      "We will: run a national engagement campaign on the societal benefits of the use of government data explore appropriate and effective mechanisms to deliver more transparency on the use of algorithmic assisted decision making within the public sector work in partnership with the Centre for Data Ethics and Innovation (CDEI) and other leading organisations in the field of data and AI ethics to pilot approaches to algorithmic transparency this year, and consider what would be needed to roll them out across the public sector explore the role of privacy enhancing technologies to enhance consumer control and confidence explore further measures to ensure appropriate fairness, transparency and trustworthiness in private and third sector data use leverage our position as a founding member of the newly established Global Partnership on AI, collaborating with our international partners and drawing upon the expertise and, in particular, recommendations on this agenda from the Responsible AI and the (UK co-chaired) Data Governance Working Group\n",
      "Finally, new technologies may help to create safe and secure environments for sharing data, including personal data. Privacy-enhancing technologies facilitate data sharing in ways that can improve privacy and in so doing build trust, while personal data stores could help people to exercise more control over their data. Nevertheless, ethical and legal questions remain .\n",
      "The government will only be able to build and maintain public trust by ensuring and clearly demonstrating that its approach to data is rooted in appropriate levels of transparency, robust safeguards and credible assurances. To support this, the government must be willing to open itself up to scrutiny, increase public engagement and improve the publishing of data by which progress can be measured. The recently refreshed Data Ethics Framework guides appropriate and responsible data use in government and the wider public sector. [footnote 23] In the research and statistics community, the UK Statistics Authority has established the National Statistician’s Data Ethics Advisory Committee and developed a self-assessment tool to help researchers and statisticians consider the ethics of their use of data.\n",
      "We will: promote the use of the government’s Data Ethics Framework across the wider public sector; support data scientists and data policymakers to build lasting capability for ethical data use; and disseminate knowledge, resources and case studies through the data ethics community work with the CDEI to understand how to ensure public sector use of data is trustworthy, by exploring the potential for technical innovations, such as privacy enhancing technologies, and through research into public attitudes\n",
      "Case Study: Establishing the National Data Guardian\n",
      "In 2014, the National Data Guardian (NDG) for Health and Social Care was created to help build trust in the use of data across the sector. Given the sensitivities around this type of data, its purpose is to ensure that people’s information is kept safe and confidential, and that it is only shared when appropriate to achieve better outcomes for patients. The NDG also acts as an independent champion for the public when it comes to matters of their confidential health and care information.\n",
      "Since 2019, the NDG has had the power to issue official guidance about the processing of health and adult social care data in England. All public bodies – including hospitals, general practices, care homes, planners and commissioners of services – must take note of guidance that is relevant to them. The guidance also extends to any organisation, public or private, delivering services for the NHS or publicly funded adult social care.\n",
      "The NDG’s work has involved the examination of data sharing in line with people’s reasonable expectations , in addition to a review of data security, consent and opt-outs . The recommendations of the latter resulted in the national data opt-out , a service providing individuals with the choice to opt-out of their confidential information being used for research and planning, and the redesign of the Data Security and Protection Toolkit .\n",
      "The Centre for Data Ethics and Innovation (CDEI) , founded in 2018 to advise on the use of data-driven technologies and AI, is the world’s first body of its kind. In its first year, the government commissioned the CDEI to conduct two policy reviews on online targeting and algorithmic bias. The CDEI also set up an ‘Analyse and Anticipate’ function providing expert-horizon scanning to identify the barriers to ethical innovation and to monitor public attitudes. It has produced the AI Barometer Report, as well as a series of thematic ‘snapshot’ reports on high profile technologies such as facial recognition, and reviewed data sharing in the public sector.\n",
      "The CDEI has developed a partnership working approach, working with a range of public and private sector organisations to address specific barriers to responsible innovation at an operational level, and to scale these tools and methodologies to other organisations. In addition, we will ask the CDEI to build on its existing independent status to provide more practical support for the technical development of potential interventions in the tech space. The government will consider whether putting the CDEI on a statutory footing would enhance these functions.\n",
      "7.2 Data use that is secure and sustainable\n",
      "7.2.1 Security and resilience of UK infrastructure on which data relies\n",
      "As our economy and public services become increasingly dependent on data, the security and resilience of the infrastructure on which data relies will also become more important.\n",
      "The need to store and process data externally – for example, in data centres – will also become even more of a critical operating function. OECD figures show that the number of businesses in the UK purchasing cloud computing systems nearly doubled from 2014 to 2018. As data centres underpin an increasing amount of business and societal activity, having confidence in the security and resilience of the UK’s infrastructure on which data relies is a key aspect of protecting individuals’ rights, service delivery across private and public sector organisations and national interests.\n",
      "Significant progress has already been achieved in managing the risks from threats and hazards to the infrastructure on which data relies, with the establishment of the National Cyber Security Centre (NCSC), as well as the GDPR and the Network and Information Systems Regulations. However, due to our dependence on the infrastructure on which data relies, the government must provide constant risk assurance in this area, taking into account the global and dynamic nature of the data storage and processing market. Accordingly, as outlined in Mission 4: Ensuring the security and resilience of the infrastructure on which data relies , we will keep apace with the risks that come with increasing reliance on data, taking steps as needed to build confidence in the security and resilience of the infrastructure on which data relies. We will also determine whether current arrangements for managing data security risks are suitable for protecting the UK from threats that counter our mission for data to be a force for good.\n",
      "7.2.2 Sustainable data use\n",
      "The trajectory for global warming is well documented, with a recent special report by the Intergovernmental Panel on Climate Change (IPCC) projecting a stark increase in climate-related risks to health, livelihoods, food security, water supply, human security and economic growth. Given such projections, business as usual is no longer an option.\n",
      "Data and its supporting infrastructure are increasingly championed as key components of any solution to the global climate crisis and associated targets and goals. However, the true impact of data and digital on sustainability is not yet fully understood. Recent research has shown that while demand for data centre services increased by more than 500% between 2010 and 2018, the amount of energy consumed by data centers only increased by 6% over the same period. This can be attributed to improvements in energy efficiency. But issues remain around a lack of transparency from providers, in particular sustainability reporting related to specific services. Procurement teams and service designers could benefit from improved knowledge of the use of sustainability criteria in commissioning. End users are largely unaware of the impacts of digital consumption on energy use and wider sustainability issues. Poor data management and culture can also lead to a vast amount of data duplication, unnecessary data retention, migration and processing that contributes to carbon use.\n",
      "We will: publish the Greening Government: ICT & Digital Services Strategy 2020-2025, that will look to address transparency, accountability, responsibility and resilience to reduce carbon and cost related to government procurement. Alongside this we will commit to producing a Data Sustainability Charter that will inform how government works with its suppliers to manage and use data sustainably.\n",
      "Case Study: The hidden impact of the legacy estate\n",
      "The Ministry of Defence (MoD) had traditionally reported the energy use by its corporate IT systems and associated large data centres. In 2019, additional resources were expended in approaching programme teams and departmental bodies across Defence for a more complete view of their assets. Not only did this expose a considerably larger estate than had been previously identified, but energy-intensive elements of the estate were incorporated for the first time. This may have been because they were previously considered ‘enabling’ rather than ‘delivery’ components, as was the case with a large number of network components whose energy consumption represented a significant increase over previous years. This ‘hidden’ infrastructure has not yet been fully documented, so there may be further additions in future years.\n",
      "The work has been extremely positive: it presents a clearer picture of MoD’s true estate and energy consumption figures. MoD’s data centres represented about 12% of Defence’s ICT energy consumption against 40% from end user devices and 48% from networks. While the review is ongoing – to better understand the true consumption figures of the estate – this is a good step in indicating how we can measure consumption and work to reduce it.\n",
      "8. Next steps\n",
      "This strategy proposes five priority missions where we can take action now to have the biggest impact. This document also captures further actions that will support the delivery of the National Data Strategy. A key area of focus will be to ensure that we put the right structures and mechanisms in place to monitor and assess progress against each of these actions.\n",
      "It is equally important to recognise that the government cannot – and must not – deliver these actions alone. Given the cross-cutting nature of data, collaboration across a wide range of sectors and organisations will be essential. We are therefore seeking to consult further with stakeholders to sense-check our proposed actions, and make use of their expertise on how we can best deliver them and ensure that the strategy meets its overarching objective of unlocking the power of data across the UK.\n",
      "Monitoring and evaluation\n",
      "Each proposed priority mission and action will be delivered by an accountable owner across government (set out in Annex A). Annex A groups these actions by pillar to show alignment with the Strategy’s framework.\n",
      "In order to drive successful implementation, we will develop a monitoring and evaluation process for the strategy. This will monitor the Strategy’s delivery and help ensure that it is achieving its intended outcomes, in line with the vision and opportunities set out in Chapter 2. We will provide further details of this monitoring and evaluation process in a future publication.\n",
      "A public consultation on this document was held in late 2020, and the government formally responded to the consultation in May 2021.\n",
      "Term Description Adequacy ( EU ) Adequacy decisions are how the EU determines if a non- EU country has an adequate level of data protection. They are unilateral decisions taken by the European Commission after an assessment of a country’s data protection framework. ‘Data adequacy’ is the status granted by the European Commission to countries outside the EEA whose level of personal data protection is judged to be essentially equivalent to the EU ’s. Once a third country has received a positive adequacy decision, personal data can flow from the EEA to that country without any further safeguards. Adequacy (UK) Data adequacy is a status granted by the Secretary of State to countries outside the UK that provide high standards of personal data protection. When a country has been designated ‘adequate’, personal data can be transferred from the UK without further safeguards being required. In addition to countries, specified territories within countries or sectors of an economy or international organisations can be ‘adequate’. Data Availability The terms data sharing, data discoverability, data access, data availability, data portability and data mobility are often used in combination and interchangeably. In this strategy we use ‘data availability’ to mean an environment which facilitates appropriate data access, mobility and re-use both across and between the private, third and public sectors in order to generate maximal economic and/or societal benefit for the UK. Data Economy See Digital Economy, below. Data Foundations In this strategy we are using the term ‘data foundations’ to mean data that is fit for purpose, recorded in standardised formats on modern, future-proof systems and held in a condition that means it is findable, accessible, interoperable and reusable. Data Infrastructure See Infrastructure below Data Mobility Data mobility refers to the efficient and safe flow of data between individuals and organisations, including but not limited to personal data. Data Portability The right to allow individuals to obtain and reuse their personal data for their own purposes across different services, as defined in the GDPR (see below). Data Protection Act 2018 Act supplementing the General Data Protection Regulation ( GDPR ). The Data Protection Act is a complete data protection system, so as well as governing the processing of personal data covered by the GDPR , it also covers all other processing of personal data for UK law enforcement and national security. It makes a number of agreed modifications to the GDPR to make it work for the benefit of the UK, in areas like academic research, financial services and child protection. Digital Economy Economic activity featuring digital technologies, and changes to market activities based on the influence and changes digitalisation brings. The term Data Economy, while more specific, is often used interchangeably, and covers the direct, indirect, and induced effects that the use and selling of data has on the economy as a whole. It involves the generation, collection, storage, processing, distribution, analysis elaboration, delivery, and exploitation of data enabled by digital technologies. Digital Economy Act References to this act in the National Data Strategy refer to part 5 of the Digital Economy Act 2017, which gives government powers to share information across organisational boundaries for a number of purposes including improving public service delivery, reducing fraud against the public sector and improving the production of national and official statistics. Five Safes A framework for helping to make decisions about effective use, regulation and access to potentially disclosive information, developed by the ONS in 2003 and widely used across the public and private sectors. The ‘five safes’ are: Safe people; Safe projects; Safe settings; Safe outputs; Safe data. General Data Protection Regulation ( GDPR ) The GDPR is an EU regulation that applies directly across all EU Member States, including the UK, until Exit Day. It regulates the processing of personal data by organisations established in the EU /UK and those outside the EU who are processing the personal data of individuals in the EU /UK to provide them with goods and services or to monitor their behaviour. Geospatial Data Often used interchangeably with ‘location data’ or ‘geographical information’. Geographic data is an umbrella term for any type of data with a location element. Fundamentally related to what we do, and where we do it. It tells us where people and objects are in relation to a particular geographic location. Infrastructure Data infrastructure – Data infrastructure is a broad concept that indicates the data assets and processes that are significant to acquire knowledge and take action about a specific context. This consists of data assets, such as datasets, identifiers and registers, the processes to acquire these assets, and the support process, including the people, standards, and technologies used, which can be both digital and non-digital. This also includes the policies that guide curation, access, management, and use of the data infrastructure. The infrastructure on which data relies – The virtualised or physical data infrastructure, software, systems and services that store, process and transfer data. This includes data centres (that provide the physical space to store data), peering and transit infrastructure (that enable the exchange of data), and cloud computing that provides virtualised computing resources (for example servers, software, databases, data analytics) that are accessed remotely. Interoperability Data Interoperability – The ability of data services and products to interact and share data. The term tends to cover two main aspects: the digital protocols that allow for data exchange, and the data standards used to preserve compatibility while processing data. It is enabled through open or common technical standards, which create a shared protocol for the exchange of information. In the context of data protection regimes – Increasing interoperability between data protection regimes usually indicates a willingness to remove barriers to data flows by, for example: increased alignment of rules, negotiated codes of conduct or similar schemes. Open Banking Open Banking is a banking technology that enables people and businesses to benefit from a range of new payment solutions and data-based products and services from regulated third-party providers, through secure connections to their customers’ payment accounts. Following an investigation into the retail banking market, the Competition & Markets Authority ( CMA ) issued an Order requiring the nine largest banks in the UK to provide these third parties with API access to payment account data and payments initiation, in order to stimulate innovation and competition. The UK’s Open Banking Standard guarantees interoperability between banks and third party providers, making it easier for new third parties to come to market and offer services to people and businesses. Linked Data Structured data that uses common standards and identifiers to allow multiple datasets to be used in conjunction with each other. It requires a common data model, and a technological and data standards that enable the use and interoperability of the data model. Z Metadata A set of data that describes contextual information on another set of data. It helps to organise, find, understand and manage data. Open Data Data that can be freely used, re-used and redistributed by anyone, subject only, at most, to the requirement to attribute and sharealike. Personal Data Store A means to manage and share personal information in a structured and secure manner with trusted parties. Privacy Enhancing Technologies A range of technical solutions to support the protection of personal data. This is done through either reducing personal data, or preventing undesired personal data processing, without losing the functionality of the information system. Smart Data A BEIS -led initiative that involves the secure and consented sharing of customer data with authorised third-party providers. These providers then use this data to provide innovative services for the customer, such as automatic switching and account management. This aims to save time, money and effort for consumers and SMEs, and increase competition in the sectors involved. Responsible Data In this strategy, we use ‘responsible data’ to mean data that is handled in a way that is lawful, secure, fair, ethical, sustainable and accountable, while also supporting innovation and research.\n",
      "Annex A - List of actions and owners\n",
      "This table reflects UK Government owners for each respective action:\n",
      "Pillar Action Owner Missions Mission 1: Unlocking the value of data across the economy DCMS Missions Mission 2: Securing a pro-growth and trusted data regime DCMS Missions Mission 3: Transforming government’s use of data to drive efficiency and improve public services Cabinet Office Missions Mission 4: Ensuring the security and resilience of the infrastructure on which data relies DCMS Missions Mission 5: Championing the international flow of data DCMS Data Foundations We will launch a programme of work to tackle the cultural and coordination barriers to good quality data, including: – creating a central team of experts able to ensure a consistent interpretation of the legal regime around data sharing (Cabinet Office) – launching the Data Quality Framework ( ONS ) – creating a Data Maturity Model for government ( ONS /Cabinet Office) – building a data management community of good practice – learning and setting best practice and guidance through a series of demonstration – or ‘lighthouse’ – projects (Cabinet Office/ ONS ) Cabinet Office / ONS Data Foundations We will implement the recommendations of the ‘Joined-up data in government’ report to improve data linkage methods, application and skill sets across government. Cabinet Office / ONS Data Foundations We will commit to resolving the long-running problems of legacy IT and broader data infrastructure. Cabinet Office Data Foundations We will drive data discoverability across government through: – developing an Integrated Data Platform for government, which will be a safe, secure and trusted infrastructure for government’s own data. It will be a digital collaborative environment that will support government to unlock the potential of linked data and build up data standards, tools and approaches enabling policy makers to draw on the most up to date evidence and analysis to support policy development, improving public services and improving people’s lives. – creating an audit of data inventories ONS / Cabinet Office (audit of data inventories) Data Foundations We will work to better support local government in maximising the benefits of data MHCLG Data Foundations We will develop and validate a set of data principles to be applied across government. Cabinet Office / ONS Data Foundations We will set out a strategy for data standards across government to include: – clarity on where the DSA will mandate some standards – use of the DDaT spend controls process –a parallel controls process for APIs and Technology Code of Practice to ensure consistent adoption of data standards across government Cabinet Office Data Foundations We will recruit senior cross-government data leadership, including a Chief Data Officer for government. Cabinet Office Data Foundations We will establish a cross-departmental governance mechanism with the authority to enforce standards across government. Cabinet Office Data Foundations We will drive aligned governance structures across government through: – undertaking a review of governance structures for data within departments – ensuring central government departments include data management plans in their Single Departmental Plans Cabinet Office Data Foundations We will support the global effort on interoperability, which will facilitate the combination and cross-referencing of different data sources. FCDO / ONS Data Foundations We will collaborate with our international partners to build strong national statistical systems to drive economic growth and help to deliver inclusive, effective services. FCDO Skills We will publish a working definition of data skills in the wider economy that sets out clear distinctions between data skills, digital skills and AI skills, and consider the benefits of providing information on pathways into data related careers. DCMS Skills We will consider the roles of the Alan Turing Institute, the National Innovation Centre for Data, the Open Data Institute, the Data Skills Taskforce, the AI Council, the UK Cyber Security Council, the Data Lab, and others in the data skills ecosystem for ways to improve the leadership and facilitation of new and better collaborations between industry, the public sector, universities and institutes. DCMS Skills We will work with the appropriate bodies to understand how data science is integrated into relevant technical qualifications, ensure that good quality data science courses are offered and that data related skills are given due consideration in their work to support emerging skills. DfE Skills We will test the most effective ways to teach foundational data skills to undergraduates in two ways – through offering modules including wider subjects such as AI, cyber and digital skills, and by integrating data skills in other subject areas. Universities will take part in the pilot on a voluntary basis. DCMS Skills We will examine ways of expanding the supply of advanced data skills across research engineers and professionals to help maximise R&D investments and to increase mobility across business and academia, and to foster the links between industry and universities at the regional level. UKRI Skills We will launch an online portal that will support businesses’ access to data skills training, helping signpost SMEs to good quality online training material matched to their technical data science capabilities and ambitions. DCMS Skills We will recruit leaders with data and digital skills across government to build a strong cadre of technical, policy, legal and analytical data experts in the centre of government. Cabinet Office Skills We will train 500 analysts across the public sector in data science by 2021, through the Data Science Campus at the ONS , the Government Analysis Function, and GDS . This will be reviewed in 2021 with a new capacity building strategy meeting the emerging needs of government up to 2025. Cabinet Office/ ONS Skills We will deliver the range of actions to be outlined in the Public Sector Data Science Capability Audit. Cabinet Office/ ONS Skills We will review the data training available to all civil servants and develop proposals to enhance and extend this offering. Cabinet Office Skills We will design a career pathway for data expertise in government. Cabinet Office Skills We will agree a shared definition of data expertise across central government. Cabinet Office Skills We will review the needs of local government in having the capabilities to manage, use and disseminate data. MHCLG Data Availability We will: – establish a cross-sector Smart Data working group, which will coordinate and accelerate existing Smart Data initiatives in communications, finance, energy and pensions, while providing recommendations to support the development of high-quality standards and systems across sectors –introduce primary legislation, when parliamentary time allows, to improve our ability to mandate participation in Smart Data initiatives and provide a legislative footing for all initiatives BEIS Data Availability We will respond to the Competition and Market Authority’s online platforms and digital advertising report and consider how its findings inform the establishment of a pro-competition digital markets unit. DCMS / BEIS Data Availability We will continue work to implement the recommendations of the Energy Data Taskforce and drive forward the Modernising Energy Data Access programme. BEIS Data Availability We will develop a clearer policy framework to identify where greater data access and availability across and with the economy can and should support growth and innovation, in what form, and what government’s role should be in supporting the market. DCMS Data Availability We will review open data publication and decision making processes to ensure their consistency; and support development of interoperable metrics to measure the impact of published data. Cabinet Office Data Availability We will review and upgrade the data standards and systems that underpin the monitoring and reporting of online harms such as child sexual abuse, hate speech and self harm and suicide ideation. DCMS Data Availability We will drive use of the Digital Economy Act (2017) powers, as well as addressing barriers to data sharing more widely. Cabinet Office Data Availability We will seek provisions with trade partners – including current negotiations with the EU , US, Japan, Australia and New Zealand – that remove unnecessary barriers to cross border data flows, with specific commitments to prevent the use of unjustified data localisation measures. DCMS Data Availability We will advocate for the importance of global data flows in the World Trade Organisation (WTO), G7, G20 and OECD . DCMS Data Availability We will draw upon the expertise of the UK Co-Chaired Data Governance Working Group under the Global Partnership on AI, work with international partners and explore approaches to international data access and sharing. DCMS / BEIS Data Availability We will establish an independent HMG capability to conduct the UK’s own data adequacy assessments for transfers of personal data from the UK. DCMS Data Availability We will review the transitional arrangements for international data transfers. DCMS Data Availability We will review the use of alternative transfer mechanisms which ensure that transfers of personal data outside the UK are appropriately protected. DCMS Data Availability We will seek positive adequacy decisions from the EU , under both the General Data Protection Regulation ( GDPR ) and the Law Enforcement Directive (LED), before the end of the transition period. DCMS ( GDPR ) / HO (LED) Data Availability We will support countries to take a more open approach to their data and will continue to play a leadership role on the open data agenda internationally. FCDO / Cabinet Office Data Availability We will develop methods to use big data and modelling analyses to support a greater resilience of vulnerable countries to extreme weather events and disease outbreaks, as part of our Official Development Assistance. FCDO Data Availability We will support the implementation of standards such as the International Aid Transparency Initiative open data standard, Extractive Industries Transparency Initiative and Infrastructure Transparency initiative. FCDO / Cabinet Office Data Availability We will work with international agencies such as the Red Cross and the UN to ensure data on crisis affected areas is handled safely, legally and ethically. FCDO Responsibility We will: – work in partnership with the ICO and other bodies to clarify aspects of the UK’s existing data regime that generate confusion or inertia, including by fast-tracking guidance and the use of co-regulatory tools – work in partnership with the ICO to lift compliance burdens wherever possible on businesses, especially SMEs – boost proactive advice and support for innovators, including via world-leading interventions such as the ICO ’s regulatory sandbox DCMS Responsibility We will run a national engagement campaign on the societal benefits of the use of government data. Cabinet Office Responsibility We will explore appropriate and effective mechanisms to deliver more transparency on the use of algorithmic assisted decision making within the public sector. Cabinet Office Responsibility We will work in partnership with the CDEI and other leading organisations in the field of data and AI ethics to pilot the proposed approach to algorithmic transparency this year, and consider what would be needed to roll it out across the public sector. Cabinet Office Responsibility We will explore the role of privacy enhancing technologies to enhance consumer control and confidence. DCMS Responsibility We will explore further measures to ensure appropriate fairness, transparency and trustworthiness in private and third sector data use. DCMS Responsibility We will leverage our position as a founding member of the newly established Global Partnership on AI, collaborating with our international partners and drawing upon the expertise and recommendations on this agenda from the Responsible AI and (the UK Co-Chaired) Data Governance Working Group in particular. DCMS / BEIS Responsibility We will promote the use of the government’s Data Ethics Framework across the wider public sector, support data scientists and data policymakers to build lasting capability for ethical data use; and disseminate knowledge, resources and case studies through the data ethics community. Cabinet Office Responsibility We will work with the CDEI to understand how to ensure public sector use of data is trustworthy, by exploring the potential for technical innovations, such as privacy enhancing technologies, and through research into public attitudes. Cabinet Office Responsibility We will publish the Greening Government: ICT & Digital Services Strategy 2020-2025, that will look to address transparency, accountability, responsibility and resilience to reduce carbon and cost related to government procurement. Alongside this we will commit to producing a Data Sustainability Charter that will inform how government works with its suppliers to manage and use data sustainably. DEFRA\n",
      "DCMS analysis using the UNCTAD definition of digitally deliverable services. Services that are principally or largely enabled by information and communication technologies ( ICT ) are defined as digitally deliverable services, which are used within the data-enabled estimations.  ↩\n",
      "Several sector estimates suggest exponential growth in data use – for example, The Digitization of the World from Edge to Core, IDC (2018)   ↩\n",
      "As highlighted in the 2019 Tech Nation report   ↩\n",
      "For example, the European Data Market Monitoring Tool (EDMMT) defines Data Markets as ‘the marketplace where digital data is exchanged as “products” or “services” as a result of the elaboration of raw data.’ In contrast, the Data Economy is defined as ‘the overall impacts of the Data Market on the economy as a whole. It involves the generation, collection, storage, processing, distribution, analysis elaboration, delivery, and exploitation of data enabled by digital technologies. The Data Economy includes the direct, indirect, and induced effects of the Data Market on the economy.’ European Data Market Monitoring Tool, IDC (2020)   ↩\n",
      "For example, The Rapid Adoption of Data-Driven Decision-Making, Enterprise Research Centre. In construction, data sharing as part of Building Information Modelling ( BIM ) was associated with cost savings of up to a third in some areas – Building Information Modelling ( BIM ) Task Group . A 2019 McKinsey report found that internationally, a larger proportion of fast-growing companies use data-driven practices compared to slower-growing companies – Catch them if you can: How leaders in data and analytics have pulled ahead, McKinsey & Co (2019)   ↩\n",
      "The AI Review recognised that to grow the AI industry in the UK, organisations required better access to data, with its key recommendation being the development of data trusts for data sharing.  ↩\n",
      "EDMMT, EU (2020) . This is also an increasing share of total employment, from 3.9% of the workforce in 2013 to 5.4% of the workforce in 2020. Varying definitions and methodologies are likely the main reason for discrepancies between different sources in how many are employed in data professional roles. For example, Royal Society research found demand (based on internet listed job adverts) for data professionals was in line with increasing demand for all jobs between the years to December 2013 and July 2018, but demand for data scientists and advanced analysts rose much faster. Also see UK Consumer Digital Index, Lloyds Bank (2019) and No Longer Optional, DCMS (2018) .  ↩\n",
      "Opinions and lifestyle survey data module, ONS (2020) . This survey used specific definitions of data skills (see survey for details). The question was asked to respondents currently in employment or who had undertaken casual or unpaid voluntary work in the last week in February 2020.  ↩\n",
      "Assessing the value of TfL ’s open data and digital partnerships, Deloitte (2017). The international trend is also likely to see increasing demand and job creation for both high-skilled jobs, and also basic data skills for the entire workforce. For example, the growth in AI could create 58 million new (high-skilled) jobs internationally over the coming years, improving the quality of work by replacing manually laborious professions with more creative and analytical roles – World Economic Forum: The Future of Jobs, WEF (2018)   ↩\n",
      "Existing work in the space includes the Race Disparity Unit . who collect, analyse and publish government data on the experiences of people from different ethnic backgrounds, supporting government departments in driving change where disparities are found.  ↩\n",
      "For a summary of findings from our call for evidence, see the accompanying publication . Evidence on the scope and scale of data skills in the third sector can be found in the Skills Platform, Charity Digital Skills Report, (2019)   ↩\n",
      "These standards involve common file formats, integrated time and budget management, and can facilitate sharing of information, off-site construction, automation of supply chains, and a range of other benefits. HM Government, 2013. Construction 2025 , Industrial Strategy: Government and Industry in Partnership   ↩\n",
      "This paragraph summarises points from the following sources: A BIM Readiness & Implementation Strategy for SME Construction Companies in the UK (Ghaffarianhoseini, et al., 2016); BIM adoption and implementation: Focusing on SMEs (Vidalakis, et al., 2019); An overview of benefits and challenges of Building Information Modelling ( BIM ) adoption in UK residential projects (Georgiadou, 2019); Critical Success Competencies for the BIM Implementation Process: UK Construction Clients (Dakhil, et al., 2019)  ↩\n",
      "Some work has been done in parts of government to attempt to improve data quality. The Government Statistical Service 2019-21 Quality Strategy aims to ‘improve statistical quality across the Government Statistical Service ( GSS ). The forthcoming Data Quality Framework for government will set out key principles and provide guidance and tools for organisations to identify and take action to ensure data is fit for its intended purpose.  ↩\n",
      "Some work has taken place to try to implement a consistent approach to standards across government. In June 2019, the Office for National Statistics developed the GSS Harmonisation Strategy , which sets out realistic actions to improve comparability and coherence across official statistics. In August 2019, the Government Digital Service developed a set of API technical and data standards . The INSPIRE Regulations 2009 and INSPIRE Regulations (Scotland) 2009 established a UK Spatial Data Infrastructure with common standards for spatial data and spatial data services.  ↩\n",
      "To address the lack of career pathways in cyber security, DCMS is working with the Institute of Engineering and Technology to establish the UK Cyber Security Council – UK Cyber Security Council Formation Project, The IET   ↩\n",
      "Members include companies such as Accenture, GSK and Nationwide, organisations such as the CBI , FSB , ONS , the Turing Institute, The National Innovation Centre for Data, the Data Lab several Royal Societies, academia, the Nuffield Foundation and DCMS .  ↩\n",
      "Opinions and Lifestyle survey data module, ONS (2020) . Of respondents working in the UK in February 2020, 72% use some data skills occasionally or a lot.  ↩\n",
      "By one estimate, there was a five-fold increase in the amount of new data created and used internationally per year between 2018 and 2025 ( The Digitization of the World from Edge to Core, IDC (2018) ). Beyond just volume, the UK data economy more than doubled from 2013 to 2020 ( The European data market monitoring tool, EU (2020) ). The size of the UK data economy increased from €43.8bn in 2013 to €89.7bn in 2020.  ↩\n",
      "For example, a sample of larger businesses that used Companies House data directly attributed £23m/year of revenue to it, although this is an average and may not reflect changes over time. Companies House data: valuing the user benefits, BEIS   ↩\n",
      "Opinions and lifestyle survey data module, ONS (2020). Of the organisation types asked about, the least trusted were marketing and advertising companies (3%) and the highest were NHS and healthcare providers (73%).  ↩\n",
      "This guidance is aimed at anyone working directly or indirectly with data in the public sector, including data practitioners (such as statisticians, analysts and data scientists), policymakers and operational staff. The Framework teams and departments a template for the development of their own guidelines, such as the Office for AI’s Procurement Guidelines and the Department for Health and Social Care’s Code of Conduct for Data-Driven Health and Care Technology.  ↩\n"
     ]
    }
   ],
   "source": [
    "print(texts[filename][\"dragnet\"][\"doc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f300e49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ministerial foreword\n",
      "\n",
      "When I became Digital Secretary, I vowed to be unashamedly pro-tech. This has to begin with data. Data is now the driving force of the world’s modern economies. It fuels innovation in businesses large and small, and has been a lifeline during the global coronavirus pandemic. The fact that governments, businesses, organisations and public services were able to share vital information quickly, efficiently and ethically during the pandemic has not only saved countless lives, but has enabled us to work from home, keep the economy running and stay connected with loved ones during a period of unprecedented disruption. As we enter into recovery, it is vital that we make the most of what we have learnt.\n",
      "\n",
      "This National Data Strategy aims to do exactly that, building on our manifesto pledge to improve data use in government, and going further. It seeks to maintain the high watermark of data use set during the pandemic, and to free up businesses and organisations to keep using data to innovate, experiment and drive a new era of growth. It seeks to harness the power of data to boost productivity, create new businesses and jobs, improve public services and position the UK as the forerunner of the next wave of innovation.\n",
      "\n",
      "Under this strategy, data and data use are seen as opportunities to be embraced, rather than threats against which to be guarded.\n",
      "\n",
      "This means asking fundamental questions about what data should and should not be made available across the UK. It means maintaining a regulatory regime that is not overly burdensome for smaller businesses and that supports responsible innovation. It means driving a radical transformation of how the government understands and unlocks the value of its own data to improve a range of public services and inform decisions at scale, through a whole-government approach driven from the centre. It means taking the risks of increased data use seriously. And it means positioning the UK as a global champion of data use, and encouraging the international flow of information across borders.\n",
      "\n",
      "The strategy is a central part of the government’s wider ambition for a thriving, fast-growing digital sector in the UK, underpinned by public trust. We want the UK to be a nation of digital entrepreneurs, innovators and investors, the best place in the world to start and grow a digital business, as well as the safest place in the world to go online. We will set out more on how we propose to support a digital drive for growth in our Digital Strategy, which we will be publishing in the Autumn.\n",
      "\n",
      "This document is a framework for the action this government will take on data. It is not the final answer, but part of a conversation about the way that we support the use of data in the UK. We lay out the opportunities that we want to realise, the pillars that we have identified as core to unlocking the power of data for the UK, and the missions that we must prioritise now.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Rt Hon. Oliver Dowden CBE MP\n",
      "\n",
      "Secretary of State for Digital, Culture, Media and Sport\n",
      "\n",
      "Executive summary\n",
      "\n",
      "The opportunity\n",
      "\n",
      "Better use of data can help organisations of every kind succeed – across the public, private and third sectors. It can support the delivery of existing services, from manufacturing to logistics, and it can be used to create entirely new products. It is a driver of scientific and technological innovation, and central to the delivery of a whole range of vital public services and societal goals, from tackling climate change to supporting the National Health Service. As businesses embrace technology, data creates jobs, opens up whole new markets and drives demand for a highly skilled workforce.\n",
      "\n",
      "On an individual level, the use of data benefits us every day – from the lives saved due to data-driven medical discoveries, to personal budgeting, understanding how much we have exercised and identifying better transport routes.\n",
      "\n",
      "The UK is already a leading digital nation. The data market in the UK (i.e. money made from products or services derived from digitised data) is the largest in Europe. UK tech grew dramatically in 2019, with the UK securing 33% of European tech investment.[footnote 1] Globally, the UK now sits behind only the US and China in terms of venture capital investment.\n",
      "\n",
      "But the last five years have seen huge technological changes, and national governments need to respond accordingly. We need a data strategy that reflects the opportunities and challenges of our new hyper-digital world, one that ensures we weigh the priorities and potential trade-offs of data in a deliberate and evidence-based way, and, above all, one that drives growth in the UK economy and powers our recovery from the coronavirus pandemic.\n",
      "\n",
      "This strategy looks at how we can leverage existing UK strengths to boost the better use of data across businesses, government, civil society and individuals. Having left the European Union, we will take advantage of being an independent, sovereign nation to maximise those strengths domestically, and position ourselves internationally to influence the global approach to data sharing and use. We will act ambitiously at home and on the international stage, aligning our history of problem-solving in science and technology with progressive values and the competence and pragmatism of our regulatory institutions.\n",
      "\n",
      "The UK response to the global coronavirus pandemic has powerfully illustrated the potential benefits of data. Our understanding of this disease, our ability to support people and our cooperation across borders have all relied on the responsible and effective use and sharing of data.\n",
      "\n",
      "But we have a duty to do more – especially with the data that the government itself holds, which can be used and shared for the benefit of society. Individual transactions, from applying for social security benefits to buying a house, are more resilient when personal information can be shared from trusted sources – for example, in the form of a digital identity.\n",
      "\n",
      "Data is a non-depletable resource in theory, but its use is limited by barriers to its access – such as when data is hoarded, when access rights are unclear or when organisations do not make good use of the data they already have. These barriers undermine the performance of public services and our economy, risking poorer outcomes for citizens. We will ensure that data can be leveraged to deliver new and innovative services, promote stronger competition, and better prices and choice for consumers and small businesses. We will drive an approach to data that holds that all can benefit when data is used responsibly, and that withholding data can negatively impact society.\n",
      "\n",
      "This strategy\n",
      "\n",
      "As part of this document, we are asking for your views. Consultation questions are included throughout the text and in an accompanying document. In future updates, we will lay out the steps that we will take to implement the strategy, and the way that your responses and the evidence you have provided have shaped our approach.\n",
      "\n",
      "The pillars A number of interconnected issues currently prevent the best use of data in the UK. These are reflected in the core pillars of this strategy: Data foundations: The true value of data can only be fully realised when it is fit for purpose, recorded in standardised formats on modern, future-proof systems and held in a condition that means it is findable, accessible, interoperable and reusable. By improving the quality of the data, we can use it more effectively, and drive better insights and outcomes from its use. Data skills: To make the best use of data, we must have a wealth of data skills to draw on. That means delivering the right skills through our education system, but also ensuring that people can continue to develop the data skills they need throughout their lives. Data availability: For data to have the most effective impact, it needs to be appropriately accessible, mobile and re-usable. That means encouraging better coordination, access to and sharing of data of appropriate quality between organisations in the public, private and third sectors, and ensuring appropriate protections for the flow of data internationally. Responsible data: As we drive increased use of data, we must ensure that it is used responsibly, in a way that is lawful, secure, fair, ethical, sustainable and accountable, while also supporting innovation and research.\n",
      "\n",
      "The missions From these pillars, we have identified five priority areas of action. These missions address key challenges that can prevent us from taking advantage of the opportunities that data offers. Unlocking the value of data across the economy. Data is an incredibly valuable resource for businesses and other organisations. However, there is increasing evidence to suggest its full value is not being realised because vital information is not getting to where it needs to be. To ensure the UK is a world leader in data, our first mission will be to set the correct conditions to make data usable, accessible and available across the economy, while protecting people’s data rights and private enterprises’ intellectual property. Using a considered and evidence-based approach, we will develop a clear policy framework to determine what government interventions are needed to do so. Securing a pro-growth and trusted data regime. We want the data revolution to benefit businesses large and small. That means maintaining a data regime in the UK that is not too burdensome for the average company; one that helps innovators and entrepreneurs to use data responsibly and securely, without undue regulatory uncertainty or risk, to drive growth across the economy. But we also want the public to be active agents in the thriving digital economy and to have confidence and trust in how data, including personal data, is used. The UK’s data regime will support vibrant competition and innovation, building trust and maintaining high data protection standards without creating unnecessary barriers to data use. Transforming government’s use of data to drive efficiency and improve public services. The coronavirus pandemic showed that there is massive untapped potential in the way government and public services use and share data to help and protect people. To sustain the high watermark set by the pandemic, the government will undertake an ambitious and radical transformation of its own approach, driving major improvements in the way information is efficiently managed, used and shared across government. To succeed, we need a whole-government approach that ensures alignment around the best practice and standards needed to drive value and insights from data; and the creation of an appropriately safeguarded, joined-up and interoperable data infrastructure to support this. We also need the right skills and leadership within the public sector to understand and unlock the potential of data. Ensuring the security and resilience of the infrastructure on which data relies. The use of data is now a central part of modern life, so we need to make sure that the infrastructure underpinning it is safe and secure. The infrastructure on which data relies is a vital national asset that needs to be protected from security risks and other concerns, such as service disruption. Interruption to data-driven services and activities can cause disruption to businesses, organisations and public services. While these are also commercial risks to manage, the government has a responsibility to ensure that data and its supporting infrastructure is resilient in the face of established, new and emerging risks, protecting the economy as it grows. Championing the international flow of data. The flow of information across borders fuels global business operations, supply chains and trade, powering growth across the world. It also plays a wider societal role. The transfer of personal data ensures people’s salaries are paid, and helps them connect with loved ones from afar. And, as the coronavirus pandemic has demonstrated, sharing health data can aid vital scientific research into diseases while uniting countries in their response to global health emergencies. Having left the European Union, the UK will champion the benefits that data can deliver. We will promote domestic best practice and work with international partners to ensure data is not inappropriately constrained by national borders and fragmented regulatory regimes so that it can be used to its full potential.\n",
      "\n",
      "Together, the steps identified in this strategy build on UK strengths to drive better use of data – data use that is more secure, more innovative and more widely recognised as a force for good. Better use of data will drive growth and productivity, improve our society and public services and position the UK as a leader of the next wave of data-driven innovation. We have an obligation to realise this ambition.\n",
      "\n",
      "1. About the National Data Strategy\n",
      "\n",
      "This strategy sets out how best to unlock the power of data for the UK. It builds upon initiatives such as the Industrial Strategy, the AI Review, the AI Sector Deal and the Research and Development Roadmap – setting out a framework for how we approach and invest in data to strengthen our economy and create big opportunities for us in the future. The government believes that unlocking the value of data is key to driving growth both within the digital sector and across the economy. This will be part of our Digital Strategy, which will be published in the Autumn and will consider more broadly how we can support a digital drive for growth.\n",
      "\n",
      "In this publication, we set out the framework for the approach this government will take, the improvements we seek to deliver and the priority missions we will focus on now to realise that change. The increasing importance of data raises novel and complex policy questions. Some of these need further consideration before the UK government confirms its direction. As such, we are also asking for your views in relation to our general framing, along with some of the actions we are considering. These questions are included throughout the document and also collated in an accompanying publication.\n",
      "\n",
      "This consultation is on a UK-wide basis: we welcome responses from organisations and individuals across the UK. The strategy covers both reserved and devolved areas: where the strategy covers reserved areas (and, in respect of Northern Ireland, excepted areas), it does so for the whole of the UK, and where it covers devolved or transferred areas, it applies to England only. We will publish a response to this consultation in early 2021.\n",
      "\n",
      "A National Data Strategy will require activity and focus beyond government. In this framework strategy we have focused on the government’s role in harnessing data. Following this consultation period, and as we move to implementation, we will work with stakeholders to set out how we will work with business and actors across the wider data landscape to land a strategy for the whole of the UK.\n",
      "\n",
      "What we mean by data Data is notoriously hard to define – and it means different things to different people. For an application developer, data is what enables the creation of rich and complex digital services. For a scientist, it is what is collected as part of experiments or surveys. For a data protection practitioner, it is the names and addresses of staff organised in a spreadsheet. For a personal trainer, it is the information in an app recording our heart rate during a workout. And for every one of us, it is the tool that powers our online maps, helps us book supermarket delivery slots, and allows us to check tomorrow’s weather forecast. When we refer to data, we mean information about people, things and systems. While the legal definition of data covers paper and digital records, the focus of this strategy is on digital information. Data about people can include personal data, such as basic contact details, records generated through interaction with services or the web, or information about their physical characteristics (biometrics) – and it can also extend to population-level data, such as demographics. Data can also be about systems and infrastructure, such as administrative records about businesses and public services. Data is increasingly used to describe location, such as geospatial reference details, and the environment we live in, such as data about biodiversity or the weather. It can also refer to the information generated by the burgeoning web of sensors that make up the Internet of Things. When thinking about the government’s own data use, the strategy covers administrative, operational and transactional data – that is, data collected in the process of running services or businesses – as well as analytical and statistical data. To ensure that data dependency risks are well managed, we are also interested in the infrastructure underpinning the storage of data, such as physical and virtualised data centres/the Cloud.\n",
      "\n",
      "1.1 Evidence and working with stakeholders\n",
      "\n",
      "This strategy, and the structure and substance of the pillars on which it is built, is drawn from a range of evidence sources, including desk research covering both case studies and published academic/sector research.\n",
      "\n",
      "In June 2019, the government launched a call for evidence on the proposed framework of the strategy, receiving over 100 responses. Alongside this, a programme of stakeholder engagement was undertaken, including the hosting of 20 roundtables and workshops, with representatives from over 250 organisations across business, the third sector and local government. Through our call for evidence, roundtables and workshops held across the country in 2019, we consulted on our parameters and objectives, and gathered evidence that has underpinned this framework National Data Strategy.\n",
      "\n",
      "We have analysed the call for evidence submissions and the discussions at the roundtables to make sure our evidence base is as wide and inclusive as possible. A summary of our call for evidence and of the stakeholder engagement can be found in the accompanying publication.\n",
      "\n",
      "Data policy is a rapidly evolving area globally, and for many of the issues highlighted in this strategy a number of questions remain unanswered, with further research and analysis required. As the government takes this strategy forward, we will continue to draw upon available evidence to inform our actions. We will develop a monitoring and evaluation process for the strategy to help ensure it is achieving its intended outcomes, as well as building the evidence base on which we develop and evaluate future policy decisions. As we move to implementation, we will further continue to work with stakeholders to set out how we will engage with business and the wider data economy to land a strategy for the whole of the UK.\n",
      "\n",
      "2. The data opportunity\n",
      "\n",
      "We are currently in the middle of a fourth industrial revolution. Technological innovation has transformed our lives, changing the way we live, work and play. At the same time, this innovation has brought with it an exponential growth in data: in its generation and use, and in the world’s increasing reliance upon it.[footnote 2] By embracing data and the benefits its use brings, the UK now faces tangible opportunities to improve our society and grow our economy. If we get this right, data and data-driven technologies like AI could provide a significant boost to the entire economy. Data can improve productivity and provide better-quality jobs. But it can also transform our public services and dramatically improve health outcomes nationally. It can keep us safe and assist the reduction of crime, speed the journey to decarbonisation, and, used well, drive efforts to create a more inclusive, less biased society.\n",
      "\n",
      "Importantly, data can also be used to harness the potential of regions right across the country,[footnote 3] ensuring that people and organisations from the whole of the UK can benefit from the full value of the digital revolution.\n",
      "\n",
      "Like many things, the use of data also presents risks; those risks need to be fully understood and taken into account. Used badly, data could harm people or communities, or have its overwhelming benefits overshadowed by public mistrust. Equally, misplaced government reluctance to securely share and use data undermines the performance of public services and risks causing harm by missing opportunities to help those most in need. In the same way, unnecessary barriers to technological innovation could drive inefficiencies and slow down growth. So it is vital that we take decisive and evidence-led steps to make the most of data’s potential.\n",
      "\n",
      "We have identified five concrete and significant opportunities for data to positively transform the UK:\n",
      "\n",
      "Achieving these opportunities will not be easy. While they are already being realised in some contexts, the means to do so are missing in many others. There is also a great deal of competition in the data space internationally, combined with differing global views on data and its use. As a digital leader, the UK is well placed to overcome this challenge.\n",
      "\n",
      "In realising these opportunities, the UK can further enhance its world-class status in science and technology, and its reputation for finding pragmatic and innovative solutions to difficult problems. In the years to come, it can use these strengths as a springboard to become a global leader in data as well.\n",
      "\n",
      "2.1 Boosting productivity and trade\n",
      "\n",
      "Data is knowledge. By having access to more of it, combined with the ability to analyse it through modern techniques, we get greater insight into what works and what does not – both in terms of selling products and services, and in terms of making our own processes and practices more efficient. Data therefore has the potential to significantly enhance economic competitiveness and productivity across the UK economy, through new data-enabled business models, as well as through the adoption of data-driven processes by existing businesses.\n",
      "\n",
      "There are various ways of defining and measuring the data market and the data economy.[footnote 4] A range of estimates suggest that the data economy grew about twice as quickly as the rest of the economy during the 2010s, making up about 4% of UK GDP in 2020. Beyond the impacts of data-driven products and services (i.e. the direct ‘data economy’), the use of data has a more general role in underpinning digitally delivered trade. ONS estimates show that in 2018 the UK exported £190 billion in digitally delivered services (67% of total UK services exports) and imported £90 billion digitally delivered services (52% of UK services imports). Enabling and growing this data-driven trade will be a priority in our approach to free-trade negotiations.\n",
      "\n",
      "While research into the business impact of increasing and improved data use is in its infancy and methodologically challenging, the existing evidence suggests wide-ranging economic benefits arising from better data use, in particular an association between efficiency, productivity and data-driven business practices.[footnote 5] There are also significant economic advantages from individual companies increasing data access and sharing. For example, Transport for London ( TfL )’s opening up of its data sets to travellers and third-party providers contributed up to £130 million per year to the London economy through time saved by travellers.\n",
      "\n",
      "In recent years, the UK government has taken significant and unprecedented steps to position the UK as a world leader in data-driven innovation. This includes committing to raising investment in (often data-heavy) research and development by 2.4% of GDP by 2027, establishing institutions such as the Centre for Data Ethics and Innovation ( CDEI ) and the Alan Turing Institute, launching brand new conversion courses in data science and AI, and conducting pioneering work on ‘data trusts’ – a novel data-sharing framework.[footnote 6] The importance of continuing and furthering this work will be even more vital in ensuring the UK’s recovery from the coronavirus pandemic, and its economic success in the years beyond.\n",
      "\n",
      "Case study: Sharing data to drive open innovation Open innovation allows companies to apply externally-developed data, ideas and technology to help address challenges. Sharing operational data provides the necessary insights into business challenges, allowing collaborators to analyse and use that data to deliver better insights and demonstrate the value of new technologies. The Open Data Institute documented how an open innovation programme, Data Pitch, has allowed Greiner Packaging International GmbH, a company producing rigid plastic packaging, to share data with logistics intelligence company, Obuu, to help monitor the resilience and efficiency of its supply chain. To do this, Obuu used data to map out shipping, storage and manufacturing flows so they could investigate three key performance indicators for efficiency in the supply chain: whether spare parts were available when needed; the average time the system is down when a part is not available; and the overall investment in the accessible stock. From this, Obuu was able to identify reductions in fixed asset investment, resulting in a significant cost saving.\n",
      "\n",
      "2.2 Supporting new businesses and jobs\n",
      "\n",
      "Data skills – like many digital skills – are increasingly important for all aspects of life, but especially for the working environment. Increasing numbers of jobs require technical data skills. One estimate suggests there was over a 50% increase in data professionals between 2013 and 2020 – increasing from 1.1m to 1.7m employees. However, there are also over 100,000 unfilled data professional posts in 2020.[footnote 7]\n",
      "\n",
      "But beyond these jobs requiring technical data skills, most jobs require some data and digital skills, and they are only set to increase in importance. Estimates from a DCMS -commissioned module of the February 2020 ONS ‘Opinions and Lifestyle’ survey found that nearly half (48%) of the working population use ‘basic’ data skills at work a lot, and just under a quarter (24%) use more advanced skills, such as data analysis and making graphs, a lot in their current job.[footnote 8]\n",
      "\n",
      "We know that data is a basis for the creation of new jobs that use both general and more technical data skills. TfL ’s work on open data (below) is estimated to have created 500 jobs directly, and another 230 indirectly.[footnote 9] More widely, the European Data Portal predicts a baseline scenario of 1.12m open data employees and an optimistic scenario of 1.97m open data employees in the EU by 2025. And over 6% of new companies in the UK in 2019 were tech start-ups, with data and the aligned need for increasingly sophisticated data skills underpinning their business models.\n",
      "\n",
      "By encouraging and supporting the use of data in the UK, we can ensure that the coming waves of technological innovation do not just drive new services but also foster the creation of new businesses and new jobs for the UK.\n",
      "\n",
      "Case study: Using data in the transport sector ‘Intelligent Mobility’, the moving of people and goods in an easier and more efficient way, provides an intersection between traditional transport and exciting new products and services relating to mobile devices, open data, wireless communication or the Internet of Things. According to research commissioned by the Transport Systems Catapult, the international Intelligent Mobility market is estimated to grow to just over £900bn a year by 2025, with data comprising an estimated £32bn of this per year by 2025. TfL already uses technology and data to make journeys easier for their customers. By making live travel information available, developers can create software and services, such as online maps and journey planners. The data is provided via an API, which unifies the data for different modes of transport into a common format and structure, where historically they have differed and been hard to layer onto each other. This data has been used to develop applications used by the public to plan journeys and check for disruptions and has helped create hundreds of jobs. Recent research by Tech City noted London’s digital economy was worth £30bn in GVA supporting over 300,000 jobs, and the government’s Future of Mobility strategy describes some of the actions being taken to maximise the benefits of data use in the UK’s transport sector.\n",
      "\n",
      "2.3 Increasing the efficiency and scope of scientific research\n",
      "\n",
      "The UK is a leader in science and research, and data is at the heart of it. New scientific developments driven by data have potentially game-changing applications across the economy, such as tracking public health risks and aiding decarbonisation through smarter energy grids, predictive maintenance of infrastructure or better traffic management.\n",
      "\n",
      "While data is critical in all research, some of the clearest examples of the benefits to society are in the life sciences. For example, data has been crucial in recognising and understanding the side effects of drugs, identifying the benefits of surgery for patients with Inflammatory Bowel Disease and demonstrating the impact of anti-smoking laws on the number of babies born prematurely in Scotland. The UK has also published five principles which will underpin the government’s policy framework to govern fair, ethical and appropriate use of health data, while also supporting innovation. More advanced applications of data-driven technology have also provided responses to the coronavirus pandemic, with AI-driven systems being used to predict the virus’s protein structure and determine which drugs may be effective for treatment, helping prioritise promising candidates for real-world trials.\n",
      "\n",
      "However, barriers to accessing data represent a significant limitation on research; these range from legal barriers (real and perceived) through to cultural blockers and risk aversion. These barriers must be addressed if the UK is to remain at the forefront of science and research. For example, recent research into data use by the pharmaceutical and life sciences industry identified a number of systemic barriers that limit access to data. Most companies surveyed noted having experienced delays and uncertainties. These include time taken to access data, access constraints for commercial users, the effort to identify and assess the quality of data sets and, most notably, the cost of the data itself.\n",
      "\n",
      "When dealing with sensitive data, the way forward must be considered and appropriate. For example, NHSX is developing a Data Strategy for Health and Social Care in Autumn 2020, which will aim to build on the permissive approach to data sharing for care seen during the coronavirus response, while still protecting the absolute need for patient confidentiality. A similar balance is needed in other instances where risks must be managed while unlocking the significant opportunities from data use at the forefront of science, research and technological development.\n",
      "\n",
      "Case study: Data-driven clinical trials to tackle coronavirus Conducting clinical trials that are better, faster and more efficient is a UK priority. Electronic Health Records (EHR) play a key role in identifying and enabling individuals from across the UK to take part in clinical trials. The UK is well-placed to take advantage of EHRs due to the unified nature of the NHS within the four nations and because a large portion of NHS records are electronic. Using securely stored data, we can find patients from across the UK that might benefit from a particular trial, rather than just a single NHS Trust or geographical location. All while protecting patients’ data. This approach was tested during the response to coronavirus and the RECOVERY trial for potential treatments. These trials normally take months to set up but, designated as an Urgent Public Health Research study, RECOVERY was set up in a matter of days. 12,000 patients from 176 NHS hospital organisations were enrolled in the trial with data being tracked and analysed. Within 100 days, the trial identified the world’s first coronavirus treatment proven to save lives – Dexamethasone. The results were announced on 16 June 2020, adopted into UK practice later the same day and included in new US guidelines within 2 weeks. RECOVERY was supported by NHS DigiTrials, the Health Data Research UK hub for clinical trials, which provided centrally collected and curated data on a weekly basis so that progress and outcomes of trial participants could be closely tracked. The data was drawn from extracts from routine EHR data, and supported rapid access to new data assets collected in response to the coronavirus pandemic, such as test data, intensive care data, and GP data. The approach not only removed the burden on the NHS ‘front line’ to field additional data requests, but enabled the RECOVERY trial team to make rapid decisions.\n",
      "\n",
      "2.4 Driving better delivery of policy and public services\n",
      "\n",
      "Data can revolutionise the public sector, creating better, cheaper and more responsive services. Public services are complex to deliver, with services such as the pensions system, the benefits system, the NHS, tax and the courts each engaging with millions of people across the UK every year. Likewise, keeping people safe requires access to the right information. These services and capabilities rely heavily on data, but the systems that handle this data have grown iteratively and independently, increasing in complexity over time. Many legacy systems are out of date, costly to operate and incapable of exchanging data with one another, presenting challenges in a world where public services are increasingly interconnected – be that between health and social care provision, tax and benefits or across policing, courts, prisons and probation.\n",
      "\n",
      "Our experience responding to the coronavirus pandemic has demonstrated that when we treat data as a strategic asset and improve coordination between organisations, the delivery of services can be more agile, more innovative, more effective and more cost-effective. Indeed, it has underlined the need for the public sector to move away from a culture of risk aversion towards a joined-up approach, where the presumption is that, with appropriate safeguards, data should be shared to drive better outcomes. The rollout of the Coronavirus (COVID-19) Shielded Patients List showed how much can be achieved through appropriate data sharing across central and local government and the private sector, with over four million support packages distributed to some of the most vulnerable people in society.\n",
      "\n",
      "For central government, better data also means better decision-making. It means policies that can be tailored and delivered more efficiently and significant savings for the public purse. Better evidence on whether policies are delivering their intended effects in different areas and for different groups means interventions can be far more effectively designed. This aligns with the public’s new expectations in our increasingly digital context. As highlighted by the CDEI ’s public attitudes research, for example, ‘there is an expectation that the public sector should use online targeting to ensure that advice and services are delivered as effectively as possible.\n",
      "\n",
      "The problem and the opportunity are not limited to central government. Some of the biggest benefits can be realised by better, more coordinated use of data across the wider public sector – in education, the justice system, health and within local government. As we move to implementation, we will work with partners to better understand the needs and barriers faced by local government in utilising data to its fullest potential. We will cut down on bureaucratic burdens, tackle risk aversion and strengthen the incentives to share data across the public sector. Non-standardisation and a lack of coordination on data mean that data collected by one organisation cannot easily be used by another. This results in duplication of effort and wasted resources. Treating data in the public sector as a strategic asset, with appropriate governance, will save time and money and drive better outcomes for us all.\n",
      "\n",
      "Case study: Data First Data First is an ambitious, pioneering data-linking programme led by the Ministry of Justice (MoJ) and funded by UK Research and Innovation through Administrative Data Research UK. The projects it supports will enable researchers in government, universities and other institutions to securely access anonymised extracts of linked administrative datasets held by MoJ and its executive agencies. This can then be linked with data held by other government departments, such as the Department for Education. Data First will allow researchers to understand how people interact with courts over time and analyse which characteristics influence patterns of frequent use, all to build a much better understanding of what MoJ policies and services are most effective. Researchers will be able to explore how users of the justice system interact with other government services. This will enable a deeper understanding of how the economic, social and educational backgrounds of people who use the justice system influence their needs, the pathways they follow through the system (for example, between the civil and criminal courts) and the outcomes they experience. Such understanding will enable more evidence-informed, targeted support and lead to lower cost, higher-quality public services for everyone in the UK. Data access will be facilitated by the controlled circumstances of the ONS Secure Research Server, an accredited processor under the Digital Economy Act 2017, which complies with the highest standards of data security and protection outlined by the principles of the Five Safes.\n",
      "\n",
      "2.5 Creating a fairer society for all\n",
      "\n",
      "Data holds great potential to empower people and civil society, delivering benefits that reach beyond the economy. Powered by better data, civil society organisations can be better equipped to reach the people most in need, at the time they most need it. Better data use could also significantly decrease operating costs, allowing charities to focus resources on protecting the most vulnerable parts of our society. Charities and other non-profits, and particularly smaller organisations, rarely have access to large enough datasets to be able to prove, to very high levels of certainty, the effectiveness of different interventions. Better coordination, re-use and sharing of data between civil society organisations can also lead to better understanding of societal issues, and of what interventions are effective in supporting those most at need. Data can drive applications that make our digital lives better. Artificial intelligence is increasingly being used to drive automated content moderation online, particularly in social media contexts, where it can help tackle misinformation. Data-driven online profiling technologies can help identify potentially vulnerable web users (such as people suffering from gambling addiction), and target support or prevent them from seeing potentially harmful content.\n",
      "\n",
      "We can harness data as part of struggles to tackle bias and exclusion. Data can be used to hold a mirror up to society – to understand how different groups are faring, and to ensure that government and private sector actions treat people fairly, and are not unintentionally discriminating against protected groups.[footnote 10] We must further ensure that data-driven technologies and AI are a force for good. Biases arising from data or algorithm use will need to be addressed to ensure that data’s potential is harnessed to drive a better, more inclusive and less biased society rather than entrenching existing problems. This is part of using data in an ethical and responsible way.\n",
      "\n",
      "Case study: Domestic abuse statistics tool The Office for National Statistics (ONS) domestic abuse publication brings together data from a range of sources, including the Crime Survey for England and Wales, police recorded crime, other government organisations and domestic abuse services. When taken in isolation, these data sources may not provide the context required to understand the national and local picture of domestic abuse. However, bringing this data together enables appropriate action to be taken to improve victims’ experiences and to help provide a clearer understanding of the criminal justice system’s response to perpetrators of abuse. Alongside the publication, there is also an interactive data tool for domestic abuse statistics. This allows users to explore data for their police force area in more detail and compare this with data from other areas. The tool is intended to help shape the questions that need to be answered by police forces and other agencies working with victims and responding to perpetrators of domestic abuse. The statistics are used to monitor the UK’s progress toward the 17 Sustainable Development Goals adopted by all UN Member States in 2015, as part of the 2030 Agenda for Sustainable Development.\n",
      "\n",
      "2.6 Realising the Data Opportunity\n",
      "\n",
      "While the preceding examples show the significant promise of better data use, there are considerable challenges preventing us from realising this more broadly and consistently across our economy and society. Organisations do not always make the best use of the data they hold, whether due to a lack of skills, a lack of leadership or a lack of resources – government and the wider public sector provide examples of this. Many organisations are limited in their access to data, much of which is controlled by a small number of key players. When data is available, it may be in formats that are unhelpful or of undetermined accuracy. And while the UK does have a wealth of data skills, these are concentrated in areas of UK expertise like science and technology; we have identified an overall lack of data skills across the workforce as a whole.\n",
      "\n",
      "To harness the opportunities and realise our vision, we need to drive improvement across the entire data landscape. Through thematic analysis of the responses to our call for evidence, stakeholder engagement and reviewing the wider evidence base, we have organised this into four highly interconnected pillars that describe the basis for better data use. These are:\n",
      "\n",
      "These are:\n",
      "\n",
      "Data foundations: The true value of data can only be fully realised when it is fit for purpose, recorded in standardised formats on modern, future-proof systems and held in a condition that means it is findable, accessible, interoperable and reusable. By improving the quality of the data, we can use it more effectively and drive better insights and outcomes from its use.\n",
      "\n",
      "Data skills: To make the best use of data, we must have a wealth of data skills to draw on. That means delivering the right skills through our education system, but also ensuring that people can continue to develop the data skills they need throughout their lives.\n",
      "\n",
      "Data availability: For data to have the most effective impact, it needs to be appropriately accessible, mobile and re-usable. That means encouraging better coordination, access to and sharing of data of appropriate quality between organisations in the public sector, private sector and third sector, and ensuring appropriate protections for the flow of data internationally.\n",
      "\n",
      "Responsible data: As we drive increased use of data, we must ensure it is used responsibly, in a way that is lawful, secure, fair and ethical, sustainable and accountable, while supporting innovation and research.\n",
      "\n",
      "To ensure that we drive focused change, we have identified five priority missions (outlined in the next section) where the government will emphasise activity across these pillars to begin realising the data opportunity set out above.\n",
      "\n",
      "Questions on the framing of the strategy We want to ensure that we produce a forward-looking strategy that takes into account public opinion and delivers real change. These questions will help to inform future work that the government will take in this space. It will provide evidence for the government to target areas for intervention in future policy. Q1. To what extent do you agree with the following statement: Taken as a whole, the missions and pillars of the National Data Strategy focus on the right priorities. Please explain your answer here, including any areas you think the government should explore in further depth. NB: For question 2, we are only looking for examples outside health and social care data. Health and social care data will be covered in the upcoming Data Strategy for Health and Social Care. Q2. We are interested in examples of how data was or should have been used to deliver public benefits during the coronavirus (COVID-19) crisis, beyond its use directly in health and social care. Please give any examples that you can, including what, if anything, central government could do to build or develop them further. Q3. If applicable, please provide any comments about the potential impact the proposals outlined in this consultation may have on individuals with a protected characteristic under the Equality Act 2010? Q4. We welcome any comments about the potential impact the proposals outlined in this consultation may have across the UK, and any steps the government should take to ensure that they take account of regional inequalities and support the whole of the UK.\n",
      "\n",
      "3. Missions\n",
      "\n",
      "This strategy sets out five priority areas of action for the government. By delivering against these missions, we will create the optimal environment for data to drive growth and productivity in the UK for the benefit of all, while helping to solve a number of societal and global issues.\n",
      "\n",
      "Mission one: Unlocking the value of data across the economy\n",
      "\n",
      "Data is an incredibly valuable resource for businesses and other organisations, helping them to deliver better services and operations for their users and beneficiaries. However, there is increasing evidence to suggest the full value of data is not being realised because vital information is not getting to where it needs to be.\n",
      "\n",
      "For example, the Digital Competition Expert Panel’s review of competition in Digital Markets and the Competition Market Authority ( CMA )’s report into online platforms and digital advertising highlighted that smaller companies often do not have the same access to data as tech giants, potentially limiting their participation and innovation in digital markets. Improved public sector access to data can also lead to better decision-making at scale. For example, if the government had better data about infrastructure, it could reduce the disruption caused when underground pipes and cables are struck by mistake, or drive more informed choices about where to build new housing.\n",
      "\n",
      "Our first mission is to create an environment where data is appropriately usable, accessible and available across the economy – fuelling growth in organisations large and small.\n",
      "\n",
      "Much of the transformative potential of data lies in the potential for linkage and re-use of datasets across organisations, domains and sectors. We must ensure that the right conditions and incentives are in place to encourage organisations to work together across the economy, ensuring appropriate and timely access to data that is of sufficient quality. This can aid innovation, ensure the benefits of data can be realised by the maximum possible people in society and aid scientific research.\n",
      "\n",
      "This is not simply a case of opening up every dataset. We must take a considered, evidence-based approach: government interventions to increase or decrease access to data are likely to have myriad consequences, intended and not. There is a balance to be struck between maintaining incentives to collect and curate data, and ensuring that data access is broad enough to maximise its value across the economy. For personal data, we must also take account of the balance between individual rights and public benefit.\n",
      "\n",
      "This is a new and complex issue for all digital economies, one that has come to the fore as data has become a significant modern, economic asset. The first step must therefore be the development of a clearer policy framework to identify where greater data access and availability across and with the economy can and should support growth and innovation, in what form, and what government’s role should be, in the UK and globally. We will move quickly to build that framework in the coming months by:\n",
      "\n",
      "undertaking research to further develop our evidence base on the timely availability of appropriate quality data, set out the economic case and understand the opportunities and rationale for intervention\n",
      "\n",
      "drawing on that work, and our existing evidence base, to scope out government’s potential role, both with respect to short-term quick wins and longer-term projects\n",
      "\n",
      "piloting the most promising interventions, working closely with industry and expert groups\n",
      "\n",
      "working closely with the CDEI , the Open Data Institute ( ODI ) and others to leverage their expertise and capability to support delivery of this agenda\n",
      "\n",
      "We are proposing the creation of a framework to identify where we can and should make data available in the wider economy. There are a number of ways the government can intervene to achieve this goal – including as a collaborator, steward, customer, provider, funder, regulator and legislator. Using Policy Lab’s Style of Government Action could be helpful in thinking about the next few questions. These questions will provide an opportunity for the government to scope out areas of focus for the data availability framework. Data availability: For data to have the most effective impact, it needs to be appropriately collected, accessible, mobile and re-usable. That means encouraging better coordination, access to and sharing of data of appropriate quality between organisations in the public sector, private sector and third sector, and ensuring appropriate protections for the flow of data internationally. Q5. Which sectors have the most to gain from better data availability? Please select all relevant options listed below, which are drawn from the Standardised Industry Classification (SIC) codes. Accommodation and Food Service Activities\n",
      "\n",
      "Administrative and Support Service Activities Agriculture, Forestry and Fishing\n",
      "\n",
      "Arts, Entertainment and Recreation\n",
      "\n",
      "Central/ Local Government inc. Defence\n",
      "\n",
      "Charity or Non Profit\n",
      "\n",
      "Construction\n",
      "\n",
      "Education\n",
      "\n",
      "Electricity, Gas, Steam and Air Conditioning Supply\n",
      "\n",
      "Financial and Insurance Activities\n",
      "\n",
      "Human Health and Social Work Activities\n",
      "\n",
      "Information and Communication\n",
      "\n",
      "Manufacturing\n",
      "\n",
      "Mining and Quarrying\n",
      "\n",
      "Transportation and Storage\n",
      "\n",
      "Water Supply; Sewerage, Waste Management and Remediation Activities\n",
      "\n",
      "Wholesale and Retail Trade; Repair Of Motor Vehicles and Motorcycles\n",
      "\n",
      "Professional, Scientific and Technical Activities\n",
      "\n",
      "Real Estate Activities\n",
      "\n",
      "Other Q6. What role do you think central government should have in enabling better availability of data across the wider economy? Q6a. If yes, what is it? If not, why not? How does this vary across sectors and applications? Data foundations: The true value of data can only be fully realised when it is fit for purpose, recorded in standardised formats on modern, future-proof systems and held in a condition that means it is findable, accessible, interoperable and reusable. By improving the quality of the data we are using, we can use it more effectively, and drive better insights and outcomes from its use. Q7. To what extent do you agree with the following statement: The government has a role in supporting data foundations in the wider economy. Please explain your answer. If applicable, please indicate what you think the government’s enhanced role should be. Q8. What could central government do beyond existing schemes to tackle the particular barriers that small and medium-sized enterprises (SMEs) face in using data effectively? The Smart Data Review in 2019 consulted on ways to make evolving schemes more coordinated across banking, finance, telecoms and energy. The focus of Smart Data is customers asking their providers to share information about them with third parties who then use this data to offer innovative services to consumers and SMEs. Q9. Beyond existing Smart Data plans, what, if any, further work do you think should be done to ensure that consumers’ data is put to work for them?\n",
      "\n",
      "Mission two: Securing a pro-growth and trusted data regime\n",
      "\n",
      "As the world becomes increasingly digitised, data has become a central driving force of the modern economy. So it is vital that the UK has a data regime that promotes growth and innovation for businesses of every size, while maintaining public trust.\n",
      "\n",
      "The UK is already a world leader in technological innovation and robust data protection standards: two areas required to build and maintain privacy, security and public confidence.\n",
      "\n",
      "We will build on these strengths to maintain a data regime that supports the future objectives of the UK outside of the EU . A pro-growth legal regime must include consideration of both regulation in the wider digital and technology landscape, which will be addressed in the government’s forthcoming Digital Strategy, as well as our data protection laws.\n",
      "\n",
      "As with all policy areas, the UK will control its own data protection laws and regulations in line with its interests after the end of the transition period. We want our data protection laws to remain fit for purpose amid rapid technological change. Far from being a barrier to innovation or trade, we know that regulatory certainty and high data protection standards allow businesses and consumers to thrive. We will seek EU ‘data adequacy’ to maintain the free flow of personal data from the EEA , and we will pursue UK ‘data adequacy’ with global partners to promote the free flow of data to and from the UK and ensure that it will be properly protected.\n",
      "\n",
      "But data is now a far more influential force in our economy than ever before – with the potential to affect the structure and competitiveness of entire markets. This has serious implications for innovators, not least in the way our approach to data affects the ease, costs and risks of developing new technologies and services. The government needs to create the conditions to support vibrant competition and innovation, which will in turn drive future growth.\n",
      "\n",
      "To build a world-leading data economy, we must maintain and bolster a data regime that is not too burdensome for the average company – one that helps innovators and entrepreneurs to use data legitimately to build and expand their businesses, without undue regulatory uncertainty or risk in the UK and globally.\n",
      "\n",
      "Given the rapid innovation of data-intensive technologies, we also need a data regime that is neither unnecessarily complex nor vague. Businesses need certainty to thrive, and the government will work with regulators to prioritise timely, simple and practical guidance, especially for emerging technologies, and create more opportunities to experiment safely.\n",
      "\n",
      "We want to encourage the widespread uptake of digital technologies more broadly – both for the benefit of the economy and wider society. We will work with regulators to provide more support and advice to small and medium-sized businesses to help them expand online, lifting compliance burdens where possible. We will also prioritise the development of sector-specific guidance and co-regulatory tools to accelerate digitisation across the UK economy.\n",
      "\n",
      "Amid all this technological change, we want people to be active agents in the digital revolution. This is a shared responsibility of both businesses and individuals.\n",
      "\n",
      "Businesses and other data-using organisations should be clear and transparent about how they collect and use data responsibly. Working with the CDEI , the government will partner with industry to identify and incentivise best-practice.\n",
      "\n",
      "People should be empowered to choose whether and how to share data in both the public and private sectors, including where the use of their data can help others. In turn, the government will remain committed to high data protection standards so that data processing is fair and does not result in discriminatory outcomes.\n",
      "\n",
      "People, businesses and other data-using organisations need the right data skills to participate actively. Individuals should have the basic data skills to be able to engage with and understand what is happening to their data. UK organisations will need access to top talent in data science, data engineering and related fields, as well as data-literate workforce.\n",
      "\n",
      "Q10. How can the UK’s data protection framework remain fit for purpose in an increasingly digital and data driven age? In section 7.1.2 we lay out the functions of the Centre for Data Ethics and Innovation (CDEI), set up in 2018 to advise the Government on the use of data-driven technologies and AI. Q11. To what extent do you agree with the following statement: the functions for the Centre for Data Ethics and Innovation (CDEI) should be Artificial Intelligence (AI) monitoring, partnership working and piloting and testing potential interventions in the tech landscape? Strongly disagree\n",
      "\n",
      "Somewhat disagree\n",
      "\n",
      "Neither agree nor disagree\n",
      "\n",
      "Somewhat agree\n",
      "\n",
      "Strongly agree Q11a. How would a change to statutory status support the CDEI to deliver its remit?\n",
      "\n",
      "Mission three: Transforming government’s use of data to drive efficiency and improve public services\n",
      "\n",
      "There is massive untapped potential in the way the government uses data. The coronavirus pandemic showed how much can be achieved when government departments and the wider public sector share vital information to solve problems quickly. We have a duty to maintain that high watermark after the pandemic, and will implement major and radical changes in the way the government uses data to drive innovation and productivity across the UK. In doing so, we will improve the delivery of public services, as well as our ability to measure the impact of policies and programmes, and to ensure resources are used effectively.\n",
      "\n",
      "There is already consensus amongst experts both inside and outside government – including academics, civil society and parliamentarians – on the need to address this challenge and to capitalise on the opportunities.\n",
      "\n",
      "However, there are numerous obstacles to achieving our ambitions, many of which are long-term and systemic. These include: real and perceived legal and security risks of sharing data; a lack of incentives, skills or investment to drive effective governance and overhaul data infrastructure; and a lack of consistency in the standards and systems used across the government, making it hard to share data efficiently.\n",
      "\n",
      "These obstacles are not insurmountable, and we have both the ambition and the commitment to tackle them.\n",
      "\n",
      "To succeed, we need a whole-government approach led by a Government Chief Data Officer from the centre in strong partnership with organisations. We need to transform the way data is collected, managed, used and shared across government, including with the wider public sector, and create joined-up and interoperable data infrastructure. We need the right skills and leadership to understand and unlock the potential of data – and we need to do so in a way that both incentivises organisations to do the right thing, as well as build in the right controls to drive standardisation, consistency and appropriate data use.\n",
      "\n",
      "To achieve this objective, we will need to drive change across five key areas:\n",
      "\n",
      "Quality, availability and access: striving towards improved data quality that is consistent, a clear understanding of what data is held and where, better data collection, and efficient data-sharing between organisations. All should be the norm, rather than the exception. Standards and assurance: setting and driving the adoption of standards for data, leading to greater consistency, integrity and interoperability, and enabling data to be used widely and effectively across government. Capability, leadership and culture: developing world-leading capability in data and data science across central and local government, so that leaders understand its role, expert resource is widely available, staff at all levels have the skills they need, and a ‘data-sharing by default’ approach across government tackles the culture of risk aversion around data use and sharing. Accountability and productivity: opening government up to greater scrutiny and increasing accountability, ensuring that this drives improvements in productivity, policy and services for people, while also ensuring data security; and using procurement to drive innovation and better outcomes. Ethics and public trust: this transformation will only be possible and sustainable if it is developed within a robust ethical framework of transparency, safeguards and assurance which builds and maintains public trust in the government’s use of data.\n",
      "\n",
      "The government is going to set an ambitious package of work in this space and wants to understand where we can have the biggest impact. Q12. We have identified five broad areas of work as part of our mission for enabling better use of data across government: Quality, availability and access\n",
      "\n",
      "Standards and assurance\n",
      "\n",
      "Capability, leadership and culture\n",
      "\n",
      "Accountability and productivity\n",
      "\n",
      "Ethics and public trust We want to hear your views on which of these actions will have the biggest impact for transforming government’s use of data. Q13. The Data Standards Authority is working with a range of public sector and external organisations to coordinate or create data standards and standard practices. We welcome your views on which if any should be prioritised.\n",
      "\n",
      "Mission four: Ensuring the security and resilience of the infrastructure on which data relies\n",
      "\n",
      "With data now a critical part of modern life, we need to ensure the infrastructure underpinning it is safe, secure and resilient. The infrastructure on which data relies is a vital national asset – one that supports our economy, delivers public services and drives growth – and we need to protect it appropriately from security risks and other potential service disruption.\n",
      "\n",
      "In the UK, the government already imposes safeguards and enforcement regimes to ensure that our data is handled responsibly. But we will also take a greater responsibility in ensuring that data is sufficiently protected when in transit, or when stored in external data centres.\n",
      "\n",
      "The government will determine the scale and nature of risks and the appropriate response, accounting for emerging trends.\n",
      "\n",
      "We will tackle the cyber threats that arise from those seeking to harm the UK head on. We will shape a more secure technology environment, and improve cyber risk management in the economy to make the UK resilient to cyber threats. The increasingly international nature of data collection, storage and transfer can present data security risks. We will determine whether current arrangements for managing data security risks are sufficient to protect the UK from threats that counter our missions for data to be a force for good.\n",
      "\n",
      "Data use creates other risks. Better use of data has the potential to help solve wider climate change problems and help the UK meet its net zero 2050 target, but we will also consider government’s and businesses’ responsibility for the environmental impact of increased data use. We will look to understand inefficiencies in stored and processed data, and other carbon-inefficient processes.\n",
      "\n",
      "The infrastructure on which data relies is the virtualised or physical data infrastructure, systems and services that store, process and transfer data. This includes data centres (that provide the physical space to store data), peering and transit infrastructure (that enable the exchange of data), and cloud computing that provides virtualised computing resources (for example servers, software, databases, data analytics) that are accessed remotely. Q14. What responsibilities and requirements should be placed on virtualised or physical data infrastructure service providers to provide data security, continuity and resilience of service supply? Q14a. How do clients assess the robustness of security protocols when choosing data infrastructure services? How do they ensure that providers are keeping up with those protocols during their contract? Q15. Demand for external data storage and processing services is growing. In order to maintain high standards of security and resilience for the infrastructure on which data use relies, what should be the respective roles of government, data service providers, their supply chain and their clients of such services? Q16. What are the most important risk factors in managing the security and resilience of the infrastructure on which data relies? For example, the physical security of sites, the geographic location where data is stored, the diversity and actors in the market and supply chains, or other factors. Q17. To what extent do you agree with the following statement: The government should play a greater role in ensuring that data use does not negatively contribute to carbon usage? Strongly disagree\n",
      "\n",
      "Somewhat disagree\n",
      "\n",
      "Neither agree nor disagree\n",
      "\n",
      "Somewhat agree\n",
      "\n",
      "Strongly agree\n",
      "\n",
      "Mission five: Championing the international flow of data\n",
      "\n",
      "In our hyper-connected world, the ability to exchange data securely across borders is essential. Economically, it drives global business, supply chains, trade and development; it will also be critical in enabling the global recovery after coronavirus. On a personal level, people rely on the flow of personal data to ensure their salaries are paid and to connect with loved ones from afar; this data in particular needs to be suitably protected. Finally, it has a huge impact on international cooperation between countries, including for law enforcement and national security, keeping the public safe.\n",
      "\n",
      "Having left the European Union, the UK now has a unique opportunity – as a world leader in digital and as a champion of free trade and the rules-based international system – to be a force for good in the world, shaping global thinking and promoting the benefits that data can deliver while managing malign influences.\n",
      "\n",
      "Using our international engagement and influence, we will:\n",
      "\n",
      "Build trust in the use of data: We will create the regimes, approaches and tools to ensure personal data is appropriately safeguarded as it moves across borders. This will include looking to secure positive adequacy decisions from the EU to allow personal data to continue to flow freely from the EU/EEA to the UK, implementing an independent UK Government capability to conduct data adequacy assessments for transfers of personal data from the UK, and working with the Information Commissioner’s Office (ICO) to build cooperation between national data authorities. The importance of data to our daily lives has made it a geostrategic tool. We will establish clear expectations of accountability when processing data – to protect personal data when it moves across the globe. These criteria will align with the UK’s stance on promoting its wider values, ethics and national interests. Facilitate cross-border data flows: We will work globally to remove unnecessary barriers to international data flows. We will agree ambitious data provisions in our trade negotiations and use our newly independent seat in the World Trade Organisation to influence trade rules for data for the better. We will remove obstacles to international data transfers which support growth and innovation, including by developing a new UK capability that delivers new and innovative mechanisms for international data transfers. We will also work with partners in the G20 to create interoperability between national data regimes to minimise friction when transferring data between different countries. Drive data standards and interoperability internationally: We will cooperate with nations to develop shared standards that align with the UK’s national interests and objectives. In a global arena, technical standards are increasingly expressions of ethical and societal values, as well as industry best practice. Recognising this, the UK will support global work on interoperability, which will facilitate the combination and cross-referencing of different data sources. This will include support to the collaborative on interoperability, an outcome of the UN World Data Forum in January 2017. The UK Government will also work with like-minded states to seek to ensure our values are considered and incorporated into the standards for new technologies which substantially impact data and their data trail. Drive UK values internationally: The UK will be a champion of good-quality, available data across the globe. We want to ensure that UK values of openness, transparency and innovation are adopted worldwide. Now the UK has left the EU, we have an opportunity to set the UK apart and take an independent, individual approach that extols UK values. National competitiveness and the balance of power internationally are increasingly based on technology and the data that drives it. We want to ensure that UK values of openness, transparency and innovation, as well as the protection of security and ethical values, are adopted and observed globally. The UK will continue to play a leadership role to meet the urgent need for open, inclusive data, and its commitments under the International Aid Transparency Initiative. And we will continue to support the work of the Open Government Partnership to open up governments across the globe.\n",
      "\n",
      "As the UK leaves the EU, we have the opportunity to develop a new UK capability that delivers new and innovative mechanisms for international data transfers. Q18. How can the UK improve on current international transfer mechanisms, while ensuring that the personal data of UK citizens is appropriately safeguarded? We will seek EU ‘data adequacy’ to maintain free flow of personal data from the EEA and we will pursue UK ‘data adequacy’ with global partners to promote the free flow of data to and from the UK and ensure it will be properly protected. Q19. What are your views on future UK data adequacy arrangements (e.g. which countries are priorities) and how can the UK work with stakeholders to ensure the best possible outcome for the UK?\n",
      "\n",
      "4. Data foundations: ensuring data is fit for purpose\n",
      "\n",
      "In this section:\n",
      "\n",
      "If the UK is to fully realise the benefits of our ongoing technological transformation, we must start by getting the basics right. This means being more effective in how we collect, curate, store, manage and delete data. Left alone, data does not sort itself out. If it is to become a powerful tool that can transform organisations and society, data requires effective governance, management and stewardship. It also requires modern infrastructure, allowing data to be shared across systems that can interact with one another.\n",
      "\n",
      "Data Foundations In this strategy we are using the term ‘data foundations’ to mean data that is fit for purpose, recorded in standardised formats on modern, future-proof systems and held in a condition that means it is findable, accessible, interoperable and reusable.\n",
      "\n",
      "The case for change in public sector data use is clear. Presently, data is not consistently managed, used or shared in a way that facilitates informed decision-making or joint working across government and the wider public sector. Data remains undervalued and underexploited.\n",
      "\n",
      "Modernising the way we manage and share data across government will generate significant efficiency savings and improve services. To succeed, we must expand work to treat data as a strategic asset, and create a whole-government, collectively responsible approach to investing in data foundations, so that everyone can benefit from the improved outcomes data can offer.\n",
      "\n",
      "The picture is more varied across the private and third sectors. While the UK is home to many world-leaders in data use, driving innovation and better services for consumers, this is not universally the case. Responses to our call for evidence and the wider existing evidence base suggest that across all sectors of the economy – perhaps particularly for SMEs and the third sector – issues include a lack of understanding about how data can be used, and used well; these issues are felt at most levels of organisations.[footnote 11]\n",
      "\n",
      "The lack of basic coordination and interoperability both within and between organisations can drive inefficiency, a lack of accountability and an inability to thoroughly evaluate or plan. Data that is not usable, linkable or comparable between organisations means that, nationally, we lose out on the ‘positive externalities of data’ – on the ability to pool data from multiple sources and sectors to create new economic opportunities, or to save lives.\n",
      "\n",
      "Indeed, even those working on advanced technologies report that poor data foundations can be a real blocker for driving the transformative power of data. For example, when the source data needed to power AI or machine learning is not fit for purpose, it leads to poor or inaccurate results, and to delays in realising the benefits of innovation.\n",
      "\n",
      "With better data, we can unlock new opportunities for businesses to grow and innovate. We can vastly improve and streamline public service delivery and offer consumers greater power and choice in the market.\n",
      "\n",
      "4.1 Data foundations in the wider economy and society\n",
      "\n",
      "Poor data quality and, relatedly, a lack of agreed standards are clear barriers to the effective use of data, from basic record-keeping to cutting-edge applications of data-driven technology. Responses to our call for evidence and stakeholder engagements highlighted common issues around:\n",
      "\n",
      "a lack of (central) ownership of data standards/ metadata/ APIs\n",
      "\n",
      "a lack of skills in managing data\n",
      "\n",
      "the pace of change leading to a fragmentation in the systems used to manage data, with ongoing resourcing issues linked to set up and maintenance costs\n",
      "\n",
      "Some respondents suggested these costs could be especially burdensome for smaller organisations, or for organisations who make data as a bi-product of their operations rather than as a discrete business product. There was anecdotal evidence from our call for evidence that in general SMEs find it more difficult than large companies to invest in and maintain high quality data. There are pockets of stronger evidence for particular types of business. For example, the construction industry has well recognised Building Information Management ( BIM ) standards.[footnote 12] However, a range of academic studies find SMEs in construction generally do not use BIM . The issues identified by SMEs include:\n",
      "\n",
      "perception that BIM is only of benefit for larger construction projects\n",
      "\n",
      "is only of benefit for larger construction projects high set-up costs of software\n",
      "\n",
      "licensing of software\n",
      "\n",
      "lack of in-house skills and/or cost of training.\n",
      "\n",
      "information retention across platforms (interoperability) – despite the industrial strategy supporting BIM\n",
      "\n",
      "lack of demand from clients (so no push to adopt the greater functionality)[footnote 13]\n",
      "\n",
      "Pulling together the pockets of evidence with the wider anecdotal points, it is likely these issues do act beyond just the construction industry. The wider evidence base on the impacts of improving data ‘foundations’, and conversely the effect of not investing in them, is not especially strong. There appears to be little robust and independent research into the case for – and means of – implementing data quality, standards and management-improvement measures. This could be for a number of reasons:\n",
      "\n",
      "definitions vary, with a lack of consensus on how to measure the constituent elements of ‘data foundations’\n",
      "\n",
      "most organisations would not want independent analysts studying their proprietary data, because of the risks to reputation or intellectual property\n",
      "\n",
      "This is an apparent evidence gap that the government intends to address. Taking the limited research that does exist, there appears to be widespread concerns for ‘data foundations’ in the private and third sectors, with some consensus that small businesses, charities and SMEs are particularly affected. These problems are seen to result in loss of time and risk lower quality business decisions and operation.\n",
      "\n",
      "Any way forward must be carefully considered, and one size will not fit all. Some barriers to better data use, even when substantiated by evidence, will not necessarily warrant government intervention. Poor data quality within organisations, for instance, is unlikely to warrant government intervention, unless it stops them carrying out a legal or statutory requirement. Even then, it is more likely to be a question for enforcers or regulators (for example, if poor records lead to poor or negligent care, or if an organisation’s poor data management procedures lead to a data breach that warrants ICO involvement).\n",
      "\n",
      "What could warrant intervention is the need to drive better quality, more standardised and interoperable data to help drive economic growth or enable a public good outcome, especially where the value of the data sits beyond its immediate use. The government has taken decisive action to unlock the power of location data and data about the built environment – as exemplified by the Geospatial Commission and through our world-leading National Digital Twin programme. The government has acted to further ADR-UK’s work to transform the way researchers are able to access public sector administrative data by using the Research Powers of the Digital Economy Act (2017). There may be a case for extending this approach to other areas of the economy. In the first instance, we will take the actions outlined in the section below, and are seeking your views where stakeholder input and debate might push the agenda forward.\n",
      "\n",
      "4.1.1 Consolidating a clear framework for government action in the wider economy\n",
      "\n",
      "The government is committed to tackling market failures that mean the foundations of data use in the wider economy are missing or misaligned. Interoperable and consistent data can bring wide economic benefits. However, data of sufficient quality comes with a cost, and businesses can lack the information they need to make resourcing choices. Organisations report being tied into contracts on legacy systems that make collecting and maintaining data in interoperable formats harder. We know that a lack of coordination can act as a barrier to interoperability. And where businesses require data in certain formats for their business practices, such data may be undersupplied by the current market.\n",
      "\n",
      "This is a new and complex issue that has come to the fore as data has become a more significant modern, economic asset. We are committed to addressing these issues, as highlighted in Mission 1 (Unlocking the value of data across the economy). This is a complex area, with many actors and the potential for unintended consequences. Before moving to consolidate this framework, we are keen to work in partnership with stakeholders as highlighted by our consultation questions and plans for future engagement. We are further working to build on the evidence base by commissioning research that deepens how we understand the government’s role in driving data availability in the economy.\n",
      "\n",
      "The steps we will take to consolidate the framework and approach, working with regulators and the Better Regulation Executive where appropriate, will include the coordination and alignment of existing work to build on data foundations across the economy, such as:\n",
      "\n",
      "working across the physical environment, including the standardisation of data about location, the built and natural environment, and transport and other infrastructures. This will include further developing the Information Management Framework, which will seek to establish a common language by which digital twins can communicate securely and effectively, part of the Centre for Digital Built Britain’s work towards developing a National Digital Twin.\n",
      "\n",
      "maximising the use of trusted data in innovation at the national and local level through supporting the new UK R&D roadmap. This will include taking decisive action to drive the standardisation and interoperability of data for research, science and innovation, and improving the access to trusted data resources at local and regional levels, with further actions to be confirmed as joint work on R&D progresses.\n",
      "\n",
      "bolstering efforts to ensure that consumers’ data is put to work for them\n",
      "\n",
      "Case study: the Food Hygiene Ratings Scheme The Food Hygiene Rating Scheme (FHRS) is an established, government-led open data service. Diners across the UK will recognise the scheme from the green stickers on restaurant windows. The scheme was set up in 2010, with a web service and an associated API created in 2012. The scheme serves in the region of 120 million API calls a year and covers over 400 local authorities in the UK. The Food Standards Agency makes these ratings available as open data to platforms such as Just Eat, Uber Eats and Deliveroo. This data can then be combined with other data sources to support consumers and platforms in making more informed choices about where to eat and which businesses to feature. Behind the FHRS is a complete and rigorous method of food safety and hygiene assurance based on best practice developed over many years. Due to the open availability of its data, the scheme has also been used for purposes beyond its original intention. For example, the FHRS was used by the Department for the Economy in Northern Ireland to help close the business rates gap. The usual method of targeting missing rates is to conduct manual inspection on high value properties, but the Department for Economy used FHRS data, along with other datasets, to work out which properties were likely to be occupied. This targeted approach increased the success rate of inspections in a two-week period and enabled the pilot to identify £350,000 in business rates that were not being collected. In another example, FHRS data was correlated with other datasets allowing it to be used to help identify risks as diverse as the location of fatbergs in the sewers. The power of the FHRS as a tool for consumers lies in its simplicity, which is achieved without compromising its underlying fidelity.\n",
      "\n",
      "4.2 Data foundations across government and the wider public sector\n",
      "\n",
      "In our call for evidence, issues related to ‘data foundations’ were particularly highlighted across government and the public sector. Key issues included:\n",
      "\n",
      "data quality issues, and different standards for data used at all stages of the data lifecycle from collection to publicly available datasets, and the (in)consistent use of metadata – where it was provided at all\n",
      "\n",
      "issues with legacy systems and different, often incompatible systems for inputting and recording data at different stages of the data journey\n",
      "\n",
      "a lack of resources for local authorities to deal with data issues\n",
      "\n",
      "a lack of senior buy-in and leadership on data\n",
      "\n",
      "a lack of alignment across government\n",
      "\n",
      "The case for change for government and the wider public sector’s use of data is well established. The Chancellor of the Duchy of Lancaster stressed the importance of using data more effectively to measure the impact of policies and make the best value-for-money decisions, for the greatest benefit, while increasing accountability. As highlighted by the National Audit Office ( NAO ) report – Challenges in using data across government – too often data is not seen as a key priority, the quality of data is not well understood and there is a culture of tolerating and working around poor-quality data. Inefficiency and cost can arise not just from poor-quality data, but also from a lack of coordination in data systems.\n",
      "\n",
      "The NAO report referenced above also found that a lack of standards across government has led to inconsistencies in the way data is recorded between departments, including identifying numerous methods of capturing and storing data on individuals and businesses. This makes it extremely challenging for the government to get a holistic view of problems and limits the ability of departments to benefit from new technologies and tools.\n",
      "\n",
      "These problems extend into the wider public sector. For example, effective electronic health recording systems play an important role in direct care, service delivery and research. While many of these systems are likely working to the same data standards and many neighbouring Trusts have interoperable systems, a recent study indicated that, of the 117 NHS trusts using electronic health recording systems, 92 of them were using at least 21 different medical records systems, making it harder to coordinate and effectively share information. Ensuring these systems are fully functional and interoperable is vital if we want to continue to realise their benefits. The government took decisive steps at the March 2020 Budget, announcing the establishment of the Data Standards Authority, the Government Data Quality Hub and the development of an integrated platform for data across government. In July 2020, the Cabinet Office also assumed responsibility for government use of data to drive coordinated improvements in the use of data in policy making and service delivery. More remains to be done.\n",
      "\n",
      "4.2.1 Data quality and technical barriers to data use and re-use across government\n",
      "\n",
      "We will improve data quality across the public sector, ensuring that this data is not fragmented, siloed or duplicated across different organisations, and is deleted appropriately. Even the best-quality data cannot be maximised if it is placed on ageing, non-interoperable systems. The government is committed to removing the barriers to data interoperability presented by variations in the hardware and software used across government,[footnote 14] including by using processing techniques that make data ‘independent’ of the infrastructure that contains it.\n",
      "\n",
      "We will: launch a programme of work to tackle the cultural and coordination barriers to good quality data, including:\n",
      "\n",
      "\n",
      "\n",
      "creating a central team of experts able to ensure a consistent interpretation of the legal regime around data sharing launching the Data Quality Framework creating a Data Maturity Model for government building a data management community of good practice learning and setting best practice and guidance through a series of flagship demonstration – or ‘lighthouse’ – projects\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "implement the recommendations of the ‘Joined-up data in government: the future of data linking methods’ report to improve data linkage methods, application and skill sets across government\n",
      "\n",
      "commit to resolving the long-running problems of legacy IT and broader data infrastructure\n",
      "\n",
      "drive data discoverability across government through:\n",
      "\n",
      "\n",
      "\n",
      "developing an Integrated Data Platform for government, which will be a safe, secure and trusted infrastructure for government’s own data. It will be a digital collaborative environment that will support government in unlocking the potential of linked data, building up data standards, tools and approaches that enable policymakers to draw on the most up-to-date evidence and analysis to support policy development, improving public services and improving people’s lives. creating an audit of data inventories\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "work to better support local government in maximising the benefits of data\n",
      "\n",
      "4.2.2. Standards and assurance\n",
      "\n",
      "To ensure that data is reusable and interoperable across government,[footnote 15] we have established a Data Standards Authority, with ongoing work to identify and agree a prioritised list of data standards to adopt across government. In the past, standards around data have been seen as voluntary. The result has been inconsistent adoption and a failure to realise the benefits – we will tackle this through a prioritised approach to mandating certain standards and using spend controls to drive others.\n",
      "\n",
      "We will: develop and validate a set of data principles to be applied across government\n",
      "\n",
      "set out a strategy for standards, to include:\n",
      "\n",
      "\n",
      "\n",
      "clarity on where the Data Standards Authority will mandate some standards use of the DDaT spend controls process a parallel controls process for APIs and Technology Code of Practice to ensure consistent adoption of data standards across government\n",
      "\n",
      "\n",
      "\n",
      "4.2.3. Productivity and accountability\n",
      "\n",
      "To ensure that these changes are effective, we will tackle data governance across government, challenging risk aversion and data-hoarding, driving consistent levels of data maturity and ensuring a joined-up approach to establishing appropriate safeguards. A whole-government approach on data requires oversight and accountability from the top and centre of government, and through each department; we will ensure aligned accountability mechanisms, as well as a set of senior data leaders with the relevant expertise and backing and support from across government.\n",
      "\n",
      "We will: recruit senior cross-government data leadership, including a Chief Data Officer for government\n",
      "\n",
      "establish a cross-departmental governance mechanism with the authority to enforce standards across government\n",
      "\n",
      "drive aligned governance structures across government by:\n",
      "\n",
      "\n",
      "\n",
      "undertaking a review of governance structures for data within departments ensuring central government departments include data management plans in their Single Departmental Plans\n",
      "\n",
      "\n",
      "\n",
      "Case Study: Harnessing the power of administrative data to transform statistical systems for the public good Timely population, migration, social and economic statistics reflecting regional variations better enable the government to proactively respond to trends, and to react more effectively to crises. ONS is using administrative data to transform its statistical systems, efficiently providing more timely, flexible statistics and analysis to decision-makers. These transformed systems will, in turn, provide greater insight across key aspects of our society and the economy, ultimately leading to better outcomes for the public.\n",
      "\n",
      "4.3 Supporting data foundations internationally\n",
      "\n",
      "Beyond our borders, and as brought to the fore most recently, a lack of basic data maturity, standardisation and interoperability on the international stage can mean that it is difficult to thoroughly understand issues that affect us globally, such as the difficulty in comparing transmission or mortality figures in the early stages of the coronavirus pandemic.\n",
      "\n",
      "In the global arena, technical standards are increasingly expressions of ethical and societal values, as well as industry best practice. As we set in place our domestic data strategy, we must engage our global partners to adopt complementary measures so they can fully embrace and harness the innovations that data can bring. We must also work with like-minded states to ensure our values are considered and incorporated into the standards of new technologies which substantially impact data and their data trail.\n",
      "\n",
      "We want to be a data champion across the world.\n",
      "\n",
      "We will: support the global effort on data interoperability, which will facilitate the combination and cross-referencing of different data sources\n",
      "\n",
      "collaborate with our international partners to build strong national statistical systems to drive economic growth and help to deliver inclusive, effective services\n",
      "\n",
      "Case study: Ordnance Survey and Singapore’s exploration of a 3D geospatial data model In 2016-19, following prior joint research programmes, Singapore turned to the UK’s Ordnance Survey (OS) for expertise in geospatial data standards and interoperability to help pave the way and enable enhancements to their 3D ‘digital twin’ of Singapore. OS carried out two projects involving a variety of stakeholders and discovered data requirements. An efficient data-capture process for a variety of city ‘themes’ (e.g. buildings, vegetation, street furniture and transport) was identified and set up. A key part of the challenge was to select relevant information ‘locked away’ in building information management (BIM) models, and to make it accessible through an open standards data model to wider stakeholders, such as city planners or regulatory bodies, who would benefit from the data to test new developments or city planning initiatives. Based on these findings, joint collaboration between OS and the National University of Singapore led to the development of an ‘IFC2CityGML transformation engine’ – a software tool capable of automating the transfer of detailed building model information for different geospatial use cases. The project also helped to improve engagement, understanding and collaboration across the BIM and geographic information systems communities in Singapore, creating a new building theme with enhanced semantic representations for mobility, energy and urban planning. In the UK, OS is continuing to innovate with spatial-visualisation and data-integration techniques, combining them with other technologies such as Artificial Intelligence and machine learning. By applying common standards to practical solutions, OS is ensuring that the right information can get to the right people at the right time – and, crucially, in the right format.\n",
      "\n",
      "5. Skills: Data skills for a data-driven economy and data-rich lives\n",
      "\n",
      "In this section:\n",
      "\n",
      "Data skills deliver benefits across the board. Businesses are more likely to be competitive in today’s digital-driven economy if they can use data effectively. Likewise, data-literate individuals are more likely to benefit from and contribute to the increasingly data-rich environments they live and work in, while data-driven companies can deliver significant productivity benefits to their own business and the wider economy.\n",
      "\n",
      "The need for data skills continues to grow across the economy. The Royal Society reports that demand for specialist data skills has more than tripled since 2013, while DCMS -commissioned analysis of 9.4 million online job adverts predicts that data analysis skills will be the fastest growing digital skills cluster over the next five years. This characterises the exponential growth in the demand for advanced applications of data science and machine learning across all sectors of the economy, from cyber to construction. The growth in AI and cyber specialisms also drives the demand for broader supply of data skills at foundational level, to feed the pipeline of advanced skills and to provide business with the foundational skills they need to work with data. Notably, these scarce skills have been critical in the deployment of research capabilities to the coronavirus response. The portion of UK R&D that they support is significant and growing rapidly.\n",
      "\n",
      "Data skills There is no widely agreed definition of data skills. In this document we use the term broadly to cover the full range of basic, technical, governance and other skills – including project management, governance and problem solving – needed by practitioners to maximise the usefulness of data. The required technical skills range from programming, data visualisation, analysis and database management, to core skills such as problem solving, project management and communication. Planning for and delivering data at the right quality requires a wide range of skills that are sometimes underappreciated. Assurance of data requires people familiar with data laws and ethical oversight. Data processing and analysis skills, used to turn data into useful information, span a wide range of capabilities comprising both technical and soft skills. Basic data literacy requires some knowledge of data uses, some ability to assess the quality of data and its application, and the skills to conduct basic analysis.\n",
      "\n",
      "Consultation with data experts, the responses to the call for evidence and a review of existing research identifies a number of challenges involved in helping both individuals and companies develop the data skills they need. These include:\n",
      "\n",
      "Lack of coordinated vision and leadership across multiple industry interests . Many of the issues with data skills lie parallel to the issues with AI and Cyber skills needs. For example, almost half of UK businesses are facing a cyber skills gap. A coherent approach across all skills stakeholders and landscapes will be required, as will enhanced efforts to drive diversity in skills provision.\n",
      "\n",
      ". Many of the issues with data skills lie parallel to the issues with AI and Cyber skills needs. For example, almost half of UK businesses are facing a cyber skills gap. A coherent approach across all skills stakeholders and landscapes will be required, as will enhanced efforts to drive diversity in skills provision. Greater clarity needed in describing data skills required by industry , which will help assessment of individual skill sets and will ensure a better match of new recruits to company requirements.\n",
      "\n",
      ", which will help assessment of individual skill sets and will ensure a better match of new recruits to company requirements. The need for the formal and vocational education system to better prepare those leaving school, further education and university for increasingly data-rich lives and careers . Foundational data literacy will be required by all. Working with industry will be necessary to help ensure that the supply of specialist data skills meets and responds to companies’ changing requirements.\n",
      "\n",
      ". Foundational data literacy will be required by all. Working with industry will be necessary to help ensure that the supply of specialist data skills meets and responds to companies’ changing requirements. Industries needing to develop their understanding of their own data skills needs , including how to define and source these requirements, and how to develop or source employees with the right mix of sector and specialist knowledge. Companies that are able to meet these challenges, particularly through senior buy in and advocacy, will likely thrive in a data-driven economy.\n",
      "\n",
      ", including how to define and source these requirements, and how to develop or source employees with the right mix of sector and specialist knowledge. Companies that are able to meet these challenges, particularly through senior buy in and advocacy, will likely thrive in a data-driven economy. A limited pool of data-skilled individuals nationally, with the cost of hiring and retaining such staff preventing the government from accessing the data skills it needs. The NAO ’s ‘Challenges in using data across government’ report also highlights the current gap in skill sets “at several levels”: legal and ethical data use; data storage, management and architecture; and planning and data governance. These points were reiterated in responses to the NDS call for evidence and roundtable discussions.\n",
      "\n",
      "The government is committed to working with the devolved administrations to align activity on advanced data, digital and R&D skills to support vibrant career pathways and to attract talent. Further actions will be laid out through the government’s upcoming Digital Strategy and through the next steps of the R&D Roadmap.\n",
      "\n",
      "5.1 Driving clarity and coordination\n",
      "\n",
      "5.1.1 Definition of data skills and role descriptors\n",
      "\n",
      "Research commissioned by DCMS and the Royal Society, and responses to our call for evidence, all indicate that there is an inconsistent use of data role descriptors – an employee in a data scientist role at one company may have a distinctly different skill profile than a data scientist at another firm. There is often a lack of clear distinction between data skills, digital skills, AI skills and similar terms. This inconsistent use of role descriptors and the lack of clear distinctions between skills complicates skills assessment and makes it difficult to recruit staff with the specific skills required.\n",
      "\n",
      "As highlighted in responses to our call for evidence, there is a need to develop and promote clearer career pathways for individuals looking to work in data roles. This is also true of related roles in AI and cyber security.[footnote 16] Importantly, many of these roles are u\n"
     ]
    }
   ],
   "source": [
    "print(texts[filename][\"newspaper3k\"][\"doc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ad7b6d38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.gov.uk/government/publications/uk-national-data-strategy/national-data-strategy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "title                                     NATIONAL DATA STRATEGY\n",
       "country                                           United Kingdom\n",
       "documentUrl    https://www.gov.uk/government/publications/uk-...\n",
       "startDate                                                 2020.0\n",
       "endDate                                                      NaN\n",
       "oecdId                                                     27018\n",
       "Name: 425, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = oecd.loc[int(filename[:-5])]\n",
    "print(tmp[\"documentUrl\"])\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55a6675",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3662c441",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.html\n",
      "word diff 12228\n",
      "newspaper3k: 26, dragnet 12254\n"
     ]
    }
   ],
   "source": [
    "filename = s[-3][0]\n",
    "print(filename)\n",
    "print(\"word diff %s\" % lens[filename])\n",
    "print(\"newspaper3k: %s, dragnet %s\" %(texts[filename][\"newspaper3k\"][\"len\"], texts[filename][\"dragnet\"][\"len\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1af5ca1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samhället och AI Samverkan mellan människor och AI Tillämpningar av AI\n",
      "This live online executive short course provides you with an understanding of AI technologies and application opportunities in order to enable you to evaluate the needs and demands of AI processes. It explains how to make a profitable start and understand what you as a business manager need to know in order to guide your organisation into new opportunities based on AI technologies. It will also demonstrate how AI is changing the business landscape, giving you the tools to ask the right questions and find out how you can use AI to your advantage. Read more at ROYAL INSTITUTE OF TECHNOLOGY (KTH)\n",
      "Computer Vision Intelligent Agents and Multi-agent Systems Machine Learning Mathematics and statistics\n",
      "In this course, we will introduce you to the fundamentals of sensor fusion for automotive systems. Key concepts involve Bayesian statistics and how to recursively estimate parameters of interest using a range of different sensors. The course is designed for students who seek to gain a solid understanding of Bayesian statistics and how to use it to fuse information from different sensors. We emphasize object positioning problems, but the studied techniques are applicable much more generally. The course contains a series of videos, quizzes and hand-on assignments where you get to implement many of the key techniques and build your own sensor fusion toolbox. Read more at CHALMERS UNIVERSITY OF TECHNOLOGY\n",
      "Samhället och AI Samverkan mellan människor och AI Tillämpningar av AI\n",
      "Enhance the business development capabilities of your organisation and get ahead in the ongoing digital transformation! Developed from four years of successful digital transformation courses, this executive short course helps individuals and organisations to increase the effects and impact of digitalization, multiplying the benefits for customers and the entire business ecosystem. The course not only provides methods and tools to increase business impacts but also presents a framework for “What to do now rather than later?” road map. Its purpose – to enhance business development capabilities within four industry segments: manufacturing, utilities (primarily energy and telecoms), banking and government. Learning objectives Understand new business model opportunities and the need to re-organize Learn about digitalization effects on customers and external and internal processes Find out about speed and digital trends that changes how value can be created Master strategies, methods and approaches that are directly applicable for you and your company when tackling the digital transition Discover sorting mechanisms. What to do now rather than later?\n",
      "Maskininlärning Samhället och AI Samverkan mellan människor och AI Tillämpningar av AI\n",
      "Digitaliseringen av vissa samhällssektorer och processer går i en rasande fart. Under det senaste året har digitala verktyg möjliggjort virtuella möten och distansundervisning av en volym som få trodde var möjlig. Digitalisering av andra områden tycks vara svårare och kräva djupare vetenskaplig förståelse. När finns det självkörande bilar som klarar att köra i tät stadstrafik? Vilken information behöver dessa bilar utbyta med mänskliga förare? Och hur kommer förarbeteenden att förändras när det är en AI som kör vissa bilar? I den här föreläsningen diskuteras några grundläggande teknikvetenskapliga frågor kring automatisering och det resoneras kring hur vissa beslutsprocesser är enkla att digitalisera medan andra är fundamentalt svåra. Välkommen till öppen föreläsning där Karl Henrik Johansson, professor i samarbetande reglersystem på KTH, berättar om sin forskning. Karl Henrik är även föreståndare för Digital Futures, KTH:s stora satsning på digitala lösningar för samhällsutmaningar tillsammans med Stockholms universitet och RISE. När: 27 april kl.17:00-17:45 Var: Via Zoom-länk\n",
      "Maskininlärning Planering och schemaläggning\n",
      "Answer Set Programming (ASP) är ett logiskt programmeringsparadigm utformat inom området Artificial Intelligence (AI) och används för att lösa komplexa sökproblem. ASP är ett språk för att beskriva och lösa problem av logisk karaktär som är användbart för att hantera verklighetsbaserade scenarier. Fördelarna med logisk programmering tillsammans med ASP: s har nyligen gjort ASP attraktiv både för akademin och industrin. Denna kurs fokuserar på att formalisera och lösa olika sökproblem i planering, schemaläggning och systemkonfiguration i ASP. Kursen riktar sig till dig som är yrkesverksam. Kursen har tre obligatoriska träffar via Zoom. Read more at ÖREBRO UNIVERSITY\n",
      "Datavetenskap Informationsvetenskap Maskininlärning Tillämpningar av AI\n",
      "Kursen ger en introduktion till Machine Learning (ML) och riktar sig till personer med en ingenjörsexamen (eller motsvarande) som inte har haft grundläggande ML och AI i sin grundutbildning. Kursen ingår Högskolan i Halmstads kurspaketet RELIFE (hh.se/relife) där du som deltagare kan läsa hela kurspaketet eller enstaka kurser. Kursen är för yrkesverksamma och ges på distans på engelska. Anmälan är öppen så länge det finns möjlighet att bli antagen.   Mer om kursen (på engelska) The course Supervised Machine Learning (Lärarledd Machine Learning) provides a broad introduction to Machine Learning (ML). Students will learn about standard supervised ML techniques for regression and classification as well as best practices in ML, and gain practice implementing ML algorithms in Python. The course \"Supervised Machine Learning\" is broken down into three modules of 2.5 credits each: 1. Introduction to regression and classification (8 x 45 min) 2. Overfitting and generalization (8 x 45 min) 3. Neural networks (10 x 45 min) Each of the lectures delivered through Zoom is followed by practical lab assignments in Python, provided as a Jupyter notebook, which allows the participants to dig into the concepts presented in the lecture and put them to practice. Exams consist of the practical lab assignments, as well as a test that will be conducted online on Blackboard at the end of each module. The course instructors teach a similar course in our master’s programmes. Read more at Halmstad University\n",
      "Planering och schemaläggning Robotik\n",
      "Det som är lovande med intelligenta robotsystem är att de kan utföra fler uppgifter och mer effektivt än en enda industriell robotlösning. Intelligenta robotar agerar kompetent eftersom de kan planera, ordna och genomföra de åtgärder som är lämpliga i det sammanhang där de befinner sig. För att uppnå denna kapacitet använder intelligenta robotar sökmetoder från artificiell intelligens. Dessa är generella algoritmer för att lösa kombinatoriska problem, dvs de utgör en robots \"förmåga att resonera\". Denna kurs introducerar de viktigaste typerna av AI-sökmetoder, samt deras användningar- inom tre industriellt relevanta tillämpningsområden, nämligen resursplanering, rörelseplanering och samordning av flera robotar. Kursen har tre obligatoriska träffar via Zoom. Kursen riktar sig till dig som är yrkesverksam.   Read more at ÖREBRO UNIVERSITY\n",
      "Robotik Tillämpningar av AI\n",
      "ROS (Robot Operating System) är en uppsättning av mjukvarukomponenter som ofta används i forskning inom autonoma system. Kortfattat så hanterar ROS kommunikation mellan olika plattformar, sensorer och ställdon samt visualisering, simulering och loggning av data. I ROS är det även lätt att dela med sig av egna metoder och algoritmer vilket har gjort att det finns en större mängd forskningsresultat implementerat vilka kan användas direkt. ROS har även börjat användas inom industrin, främst inom R&D. Denna kurs ger dig möjlighet att använda och tillämpa ROS inom ett område du önskar. Kursen har tre obligatoriska träffar via Zoom. Kursen riktar sig till dig som är yrkesverksam. Read more at ÖREBRO UNIVERSITY\n",
      "Maskininlärning\n",
      "För att bra maskininlärningsmodeller ska kunna tas fram med hjälp av de algoritmer som diskuteras på kursen ”Introduktion till maskininlärning – del I” så krävs det att datamaterialet används och förbehandlas på rätt sätt. Förutom detta så kommer du under kursens gång att lära dig mer om de praktiska aspekter kring tillämpning av maskininlärningstekniker på verkliga problem genom att tolka och agera utifrån de resultat som metoden genererat. Dessa inkluderar metoder för attributsextrahering (för att ta reda på vilken del av datamaterialet som är relevant för just ditt problem); avvägning mellan bias och varians (för att ta reda på om du har tillräckligt med data till din modell); flermodellsavvägning (för att kunna kombinera mer än en maskininlärningsmodell); samt andra praktiska rekommendationer. Med hjälp av inhämtad kunskap ska du och övriga kursdeltagare sedan lösa ett klassificeringsproblem och på så vis skaffa er praktisk erfarenhet av att använda maskininlärning. Kursen riktar sig till dig som är yrkesverksam. Kursen har tre obligatoriska halvdagsträffar via Zoom. Read more at ÖREBRO UNIVERSITY\n",
      "Data Science Mathematics and statistics Applications of AI\n",
      "This course introduces students to the fundamentals of experiment design and statistical inference models for data analysis. The courses provides a hands on experience in designing an experiment, collecting data and drawing conclusions. Statistics in a science of decision making and conclusion drawing in the presence of variability. In rare cases, to be able to make a decision about a new product, or draw conclusions from an observed phenomena, we can collect observations from the entire population and as a result make a definitive conclusion about its attributes, but most often that is not the case. As a we often opt to performing an experiment, or making passive observation with only a-sub sample of the population that will help us extrapolate and make decision about the rest of the unobserved samples. The course is given on-line with three mandatory Zoom-meetings. Read more at ÖREBRO UNIVERSITY\n",
      "Maskininlärning\n",
      "Förstärkningslärande (Reinforcement Learning - RL) är en metod för att lösa sekventiella beslutsproblem. Målet med RL är att få en optimal policy genom att lära av försök och misstag. Denna kurs omfattar grundläggande och avancerade RL-algoritmer för både diskreta och kontinuerliga tillstånds- och handlingsrum. Ett exempel är att kombinera inlärning och sökning. Vi förklarar också hur dessa algoritmer implementeras med djupa inlärningstekniker. RL-baserade tillvägagångssätt har visat imponerande prestanda på komplexa uppgifter med stora mängder data. Vi kommer också att prata om hur RL-strategier presterar på verkliga problem och diskutera begränsningar. Read more at ÖREBRO UNIVERSITY\n",
      "Societal Aspects of AI Human-AI Interaction\n",
      "As AI systems become more common and expand their abilities, the decisions they made have a crucial impact on society as a whole. Whether they are designed to recommend content or product online, to assist judges or physicians in their decision-making, or to decide how to distribute mortgages or video surveillance cameras, these systems can have a crucial and lasting impact on all of us. For this reason, it is of paramount importance that those in charge of designing such systems work toward ethical and responsible systems. This course covers the theoretical and practical aspects of normative ethics and how they apply to AI systems, discuss how AI systems can become biased, as well as how to prevent and correct possible bias. Through concrete examples, case studies, and project, this course aims at raising awareness on the problem of ethical AI as well as giving the students practical experience on how to ensure ethical and responsible development of AI systems in their everyday work. This course is given on-line with three mandatory Zoom-meetings.   Read more at ÖREBRO UNIVERSITY\n",
      "Natural Language Processing Knowledge Representation and Reasoning\n",
      "Natural Language Processing (NLP) is a sub-field of Artificial Intelligence (AI) that studies how computers process human language. This course allows learners to obtain knowledge about NLP problems, applications, and recent methods for solving them. The aim of this course is to expertise in applying neural networks and deep learning methods to solve ongoing problems in NLP.   The course is given on-line with three mandatory Zoom-meetings. Read more at ÖREBRO UNIVERSITY\n",
      "Computer Vision Planning and Scheduling Robotics\n",
      "With probabilistic methods, you can make mobile robots navigate safely in shared environments using low-cost onboard sensors without expensive external infrastructure. In this course, we will introduce you to the application of probabilistic methods for automatic map building and accurate self-positioning. This course is given on-line with three mandatory Zoom-meetings. Read more at ÖREBRO UNIVERSITY\n",
      "Samhället och AI Samverkan mellan människor och AI Tillämpningar av AI\n",
      "Kursen ”AI och etik – i teori och praktik” är högaktuell då frågor om AI både väcker stora förhoppningar men också flera etiska spörsmål. I denna kurs fördjupar vi oss teoretiskt i frågor om AI och berör bl a det som har att göra med ansvar och bias. I kursen jobbar vi även praktiskt på så sätt att de dilemman och problem som finns i era verksamheter analyseras och diskuteras med hjälp av teori. Målet med kursen är att ge användbara verktyg för att hantera etiska avvägningar i design och införande av AI.\n",
      "Kursinnehåll Kursen syftar till att ge handgriplig etisk kunskap som kan användas som grund för den samhälleliga debatten om AI i egna utvecklingsprojekt. Samtliga föreläsningar varvas med övningar där deltagarna får vara aktiva och diskutera hur problemområden bör hanteras med fokus på den egna verksamheten. Kursen fokuserar på följande: AI och ansvar Bias Delaktighet Partiskhet Inflytande Kursen examineras genom ett reflektionspaper där kursdeltagaren argumenterar för lösningar på aktuella problemområdena. Kursen ger 3 hp.\n",
      "Målgrupp Kursen riktar sig till personer som arbetar med utveckling eller införande av AI-system. Den är också lämplig för beslutsfattare som har att ta ställning till införande av sådana system. Kursen har begränsat antal platser.\n",
      "Praktisk information Kursen arrangeras under tre heldagar.\n",
      "Nästa kurstillfälle Preliminära kursdatum  29 januari, 12 februari och 5 mars 2021. Preliminär lokal Elite Grand Hotel i Norrköping. Investering 9 800 kronor per deltagare. I kurskostnaden ingår fika och mat.\n",
      "Datavetenskap Maskininlärning Tillämpningar av AI\n",
      "Örebro universitet erbjuder kursen Autonoma robotar och ROS för dig som yrkesverksam. Utbildningarna leds av det internationellt erkända forskningscentret AASS, som ligger i framkant vad gäller forskning inom artificiell intelligens (AI), robotik och autonoma system Denna kurs ger en omfattande introduktion till olika traditionella och innovativa modeller, algoritmer och ansatser inom artificiell intelligens, mer än bara maskininlärning. Grundläggande idéer - till exempel för sökning, optimering, planering, resonemang, osäkerhet och även maskininlärning introduceras i videoföreläsningar tillsammans med konceptuella exempel som ska lösas av studenterna. Detta är en distanskurs med tre obligatoriska fysiska halvdagsträffar på Örebro universitet inklusive två seminarier. Under det första seminariet kommer studenterna att presentera ett arbete / uppsats vald från en aktuell litteraturlista sammanställd av lärarna. Lämpliga AI-lösningar för de problem som intresserar studenterna står i fokus vid det andra seminariet. Read more at ÖREBRO UNIVERSITY\n",
      "Data Science Knowledge Representation and Reasoning Machine Learning Applications of AI\n",
      "The course covers the concepts, models and computation methods for computer programs and systems that can autonomously, learn and generalize new knowledge and feature self-awareness. Read more at Luleå University of Technology\n",
      "Samhället och AI Samverkan mellan människor och AI Tillämpningar av AI\n",
      "This course provides you with an understanding of AI technologies and application opportunities in order to enable you to evaluate the needs and demands of AI processes. It explains how to make a profitable start and understand what you as a business manager need to know in order to guide your organisation into new opportunities based on AI technologies. Both the live online masterclass and the available online courses will demonstrate how AI is changing the business landscape, giving you the tools to ask the right questions and find out how you can use AI to your advantage. Read more at ROYAL INSTITUTE OF TECHNOLOGY (KTH)\n",
      "Computer Science Data Science Machine Learning Applications of AI\n",
      "This course presents a process for developing AI-based systems in a structured way. You will learn about the phases of AI projects that employ machine learning (ML). In particular, we discuss methods and techniques for AI engineering, i.e. implementing continuous training and continuous delivery of ML-based systems. The lectures explain the life cycle of ML systems, including methods for ML model development, testing, deployment and monitoring. In-class computer demonstrations will provide practical examples of ML model training and deployment. The exercises do not require pre-existing knowledge of machine learning but assume that you have a technical and programming background. Read more at CHALMERS UNIVERSITY OF TECHNOLOGY\n",
      "Samhället och AI Samverkan mellan människor och AI Tillämpningar av AI\n",
      "Under denna kurs introduceras och problematiseras centrala företeelser och begrepp som är förbundna med artificiell intelligens (AI). Kursen behandlar en vid syn på AI både som teknik och som en del i utformning av samhälleliga verksamheter och processer. Kursen fokuserar AI i historisk, nutida och framtida kontext. Särskilt fokus läggs vid demokratiska och etiska problem inom AI-området. Read more at UMEÅ UNIVERSITY\n",
      "Computer Vision Robotics Mathematics and statistics Computer Science Data Science Planning and Scheduling Knowledge Representation and Reasoning Intelligent Agents and Multi-agent Systems Natural Language Processing Machine Learning\n",
      "Part 1, theory, 4.5 credits. The course provides theoretical and methodological knowledge and skills in classical AI (artificial intelligence) and robotics. Topics covered: Heuristics for search. Search for games. Applied predicate logic. Classical planning: heuristics. Knowledge representation. Probability theory: axioms, conditional probability, Bayes’ rule. Bayesian networks. Probabilistic reasoning over time, Hidden Markov Models. Decision trees and learning. Robotics: reinforcement learning, learning from demonstration, hybrid architectures, motion planning, odometry, metric and topological route planning, localization and map generation. Part 2, laboratory, 3 credits. In the laboratory part some of the theories and techniques discussed in the theoretical part are put into practice. This part consists of a series of mandatory laboratory assignments, in part carried out with physical robots or advanced simulators. Read more at UMEÅ UNIVERSITY\n",
      "Behandling av naturliga språk Datavetenskap Datorseende Informationsvetenskap Intelligenta agenter och multiagentsystem Kunskapsrepresentation och resonerande Maskininlärning Planering och schemaläggning\n",
      "I denna kurs behandlas designmetoder och olika algoritmer för och varianter av Deep Learning inom klassifiering, prediktion, interaktion och modellering med användning av olika typer av data; ljud, bild, video, text, sekvenser. Begrepp såsom Överträning, Dropouts och Gradienter gås igenom.   Installation och användning av hård- och mjukvaror för experimenterande med topologier såsom CNN, RNN, LSTM, DQN och tillhörande parametrar samt inspektion av resultatet utgör en väsentlig del av kursen. Kursen går även kortfattat igenom Dataanalys, Maskininlärning, Tensorer, Datorseende, Transfer Learning,  Grunder i Robotik, Reinforcement Learning och metoder med kombinationen Deep Reinforcement Learning. Numeriska implementationer studeras översiktligt. Det är en tidsmässig fördel att ha tillgång till lämpliga beräknings(grafik)kort alternativt ha/få tillgång till motsvarande molntjänst. Man kan köra Windows, OS X eller Linux. Read more at UMEÅ UNIVERSITY\n",
      "Computer Vision Intelligent Agents and Multi-agent Systems Robotics\n",
      "After completing the course, the students shall understand, design and program basic functions för touch-free measurement and control of robots. Read more at Luleå University of Technology\n",
      "Applications of AI Computer Science Machine Learning\n",
      "This course introduces Machine Learning (ML) and its applications in manufacturing. We use real-world case studies to explain how ML works and can be applied in different manufacturing-related problems. In addition to the technical focus of applying ML in manufacturing, the course provides knowledge on prerequisites for generating business value from data-driven decision-making. We illustrate the importance of data pre-processing and structured work procedures in ML projects to secure data quality and successful analyses. The crucial link between data-driven decision-making, competence, and internal/external integration will be highlighted. The exercises are prepared in a way to minimize programming and allow you more time to focus on interpreting the quality of the results. Read more at CHALMERS UNIVERSITY OF TECHNOLOGY\n",
      "Machine Learning Applications of AI\n",
      "Machine learning is an important subfield of Artificial Intelligence (AI). This course explains what can be done with machine learning and how to use machine learning in chemical engineering. The course provides insight to how calculations are done behind the scene. You learn the topic through a blend of lectures and worked-through examples from the chemical-engineering domain. Lectures illustrate how machine learning is integrated into R&D and production of materials. At the end of the course, you learn how to create predictive models in cases where physiochemical modelling is not feasible. We have selected computer exercises from systems relevant to Chemical Engineers working within research, development, process engineering and production. The exercises are prepared in a way to minimize programming to allow you more time to focus on interpreting the quality of the results. Read more at CHALMERS UNIVERSITY OF TECHNOLOGY\n",
      "Datavetenskap Maskininlärning Tillämpningar av AI\n",
      "Örebro universitet erbjuder kursen Autonoma robotar och ROS för dig som yrkesverksam. Utbildningarna leds av det internationellt erkända forskningscentret AASS, som ligger i framkant vad gäller forskning inom artificiell intelligens (AI), robotik och autonoma system Denna kurs ger en omfattande introduktion till olika traditionella och innovativa modeller, algoritmer och ansatser inom artificiell intelligens, mer än bara maskininlärning. Grundläggande idéer - till exempel för sökning, optimering, planering, resonemang, osäkerhet och även maskininlärning introduceras i videoföreläsningar tillsammans med konceptuella exempel som ska lösas av studenterna. Detta är en distanskurs med tre obligatoriska fysiska halvdagsträffar på Örebro universitet inklusive två seminarier. Under det första seminariet kommer studenterna att presentera ett arbete / uppsats vald från en aktuell litteraturlista sammanställd av lärarna. Lämpliga AI-lösningar för de problem som intresserar studenterna står i fokus vid det andra seminariet. Read more at ÖREBRO UNIVERSITY\n",
      "Language technology is an interdisciplinary field that encompasses all the development of computer programs to analyze and generate human language, such as Swedish or English, and also modeling and simulation of human linguistic behavior using computers. The course addresses the possibilities and limitations of language technology with a focus on the language technology tools that are close to the research and are relevant for future work with, for example, design and implementation of information systems or requirements in procurement. Read more at Stockholm University\n",
      "Master in Language Technology (One year or Two years) is for students who want to make computers use and understand human language. Read more at UNIVERSITY OF GOTHENBURG\n",
      "Software engineers and software managers will have a key role in all aspects of society, from using AI to provide customer service and even financial advice, to developing self-driving cars, to creating apps in the health and biotech sphere. We are among the largest software engineering groups in the world, and our researchers have worldwide impact. Our programme gives you the opportunity to complete collaborative projects, work with partners in industry, and participate in hackathons, gaining the skills necessary to improve the world with software. Read more at UNIVERSITY OF GOTHENBURG\n",
      "Datavetenskap Matematik och statistik\n",
      "Data Science is concerned with extracting meaning from large volumes of data. It is a field that has grown rapidly in recent years as a result of the increasing availability of large data sets, and the opportunities and challenges that these present. Central topics within Data Science include data mining, machine learning, databases, and the application of data science methods in natural sciences, life sciences, business, humanities and social sciences, as well as in industry and society. Read more at UNIVERSITY OF GOTHENBURG\n",
      "Tillämpningar av AI\n",
      "Kognitionsvetenskap är ett tvärvetenskapligt ämne där du studerar hur människor, andra biologiska varelser och smarta maskiner bearbetar och använder information. På Göteborgs universitet får du en bred utbildning i ämnet grundad i områden som filosofi, psykologi, språk och kommunikation, neurovetenskap samt datavetenskap och informationsteknologi (IT). Du lär dig hur människor bearbetar och kommunicerar information likväl som hur samspelet mellan människor och IT fungerar. Read more at UNIVERSITY OF GOTHENBURG\n",
      "Planning and Scheduling Knowledge Representation and Reasoning\n",
      "The Master’s programme in Artificial Intelligence at Umeå University gives broad knowledge in AI and deepened knowledge in profile areas such as theoretical foundations of artificial intelligence, human-AI interaction, intelligent robotics, machine learning or data science. Read more at UMEÅ UNIVERSITY\n",
      "Computer Vision Robotics Computer Science Data Science Planning and Scheduling Knowledge Representation and Reasoning Intelligent Agents and Multi-agent Systems Natural Language Processing Machine Learning\n",
      "The course provides a basic introduction to classical AI (artificial intelligence) as well as non-classical AI. It addresses fundamental conditions, problems and challenges for AI also from a philosophical perspective. Read more at UMEÅ UNIVERSITY\n",
      "The course aims to provide knowledge and skills in modern techniques and tools for machine learning, to create an understanding of its uses, strengths, and weaknesses. All in order to then apply these skills in a real world application. This course is taught fully on distance and is adapted for working professionals. Read more at Mid Sweden University\n",
      "Computer Science Data Science Machine Learning Applications of AI\n",
      "This course provides a broad introduction to Machine Learning (ML). Students will learn about standard supervised ML techniques for regression and classification as well as best practices in ML, and gain practice implementing ML algorithms in Python. The course \"Supervised Machine Learning\" is broken down into three modules of 2.5 credits each: Introduction to regression and classification (8 x 45 min) Overfitting and generalization (8 x 45 min) Neural networks (10 x 45 min) Each of the lectures delivered through Zoom is followed by practical lab assignments in Python, provided as a Jupyter notebook, which allows the participants to dig into the concepts presented in the lecture and put them to practice. Exams consist of the practical lab assignments, as well as a test that will be conducted online on Blackboard at the end of each module. The course instructors teach a similar course in our master’s programmes. This course is for professionals with an undergraduate degree in engineering (or similar). Prerequisites include knowledge in Python programming, as well as very basic linear algebra. This course is for people who have not had basic Machine Learning and AI in their undergraduate degree. The course is held online in English. Pace: 50%, flexible.\n",
      "Maskininlärning Tillämpningar av AI\n",
      "Join our webinar AI – Under the Hood of Machine Learning designed for you who want to understand the magic. The webinar will give you a basic understanding of AI core concepts and the ability to ask the right questions when involved in AI initiatives within your company. The concepts of Machine Learning and Neural Networks will be in focus, as these concepts deliver great value today. Read more at ROYAL INSTITUTE OF TECHNOLOGY (KTH)\n",
      "Artificial intelligence (AI) can be used to help improve the health of people and ecosystems, reduce urban pollution, spot oil spills at sea and desertification of land, as well as to enhance education systems. However, AI can also increase the gap between the rich and the poor, and the bias of the algorithms may enhance existing injustices towards minorities. Welcome to learn more about Artificial Intelligence and the SDGs at this event organized by KTH Royal Institute of Technology, AI Sustainability Center and MIT. Read more at ROYAL INSTITUTE OF TECHNOLOGY (KTH)\n",
      "Datavetenskap Informationsvetenskap Tillämpningar av AI\n",
      "Cybersäkerhet blir allt viktigare för företag och organisationer, särskilt med tanke på den ökade mängden distansarbete som kan innebära säkerhetsproblem kopplat till integritet, tillgänglighet och sekretess. Kursen ingår i Högskolan i Halmstads kurspaket RELIFE (hh.se/relife) där du som deltagare kan läsa hela kurspaketet eller enstaka kurser. Kursen är för yrkesverksamma och ges på distans på engelska. Anmälan är öppen så länge det finns möjlighet att bli antagen.   Om kursen (in English) About the course Cybersecurity Issues of Distributed Work Places: Cybersecurity is a critical topic, especially since distributed and remote operations have raised many security concerns about maintaining the integrity, availability, and confidentiality of enterprises' assets.? The course \"Cybersecurity Issues of Distributed Work Places\" aims to improve user awareness of security issues pertaining to data, communications, and hardware platforms. In particular to the coronavirus, this course will consider the potential threats that employees can face due to being directed to work from home. The course is broken down into three modules of 2.5 credits each:   Introduction to Cybersecurity (8 x 45 min) Cyber-attacks (8 x 45 min) Introduction to hardware security (10 x 45 minutes)   The course instructors are experts in the fields of hardware and cybersecurity. Guest lecturers will be included from the companies HMS Industrial Networks and AppGate Inc to ensure industry relevance. This course is for professionals and requires a basic computer systems background included in an engineering undergraduate degree. The course is held online in English.\n",
      "Samhället och AI Samverkan mellan människor och AI\n",
      "Autonomous systems can make their own decisions but cannot take responsibility and thus not be accountable for their decisions. It is however also crucial to discuss the question of responsibility in a more forward-looking sense. It is also critical to reflect on how our decisions today lead the development in a certain direction that may be difficult to change later. In this project we study the question of who should do what, and when, to make sure that things do not go wrong (forward-looking responsibility), and consider how different distributions of forward-looking responsibility today between law-makers, industry, developers, consumers etc.) will affect the long-term development of AI concern democracy, AI development, and AI safety. Speakers:  Maria Hedlund, Department of Political Science, Lund University and Erik Persson, Practical Philosophy, Lund University Please register at: http://ai.lu.se/events/registration-2020-06-17 in order to get a link for online participation via the Zoom platform.   The Corona crisis prevents physical meetings and discussions, but in order to continue dialogue and joint learning, we now offer a series of online lunch seminars. Together we watch one or more presentations from previous AI Lund events for 20-30 minutes. Then an expert, often the original speaker, are there to answer questions for 10-15 minutes. You can also ask questions in the text chat during the video. The presentations can be in Swedish or English, and you can ask questions in both language s. On June 17 we watch the presentation that Maria Hedlund and Erik Persson gave at the AI Lund Fika-to-fika-workshop: AI, Social Science and Research in EU 16 october 2020 Read more at LUNDS UNIVERSITY\n",
      "Samhället och AI Samverkan mellan människor och AI\n",
      "The question of how technology changes the labor market has been studied often in the context of manufacturing vs creative job. An interesting thought experiment is to discuss whether more advanced AI algorithms introduced in the public sector can make politicians superfluous. This presentation will discuss the question both theoretically and anchored in empirical examples. Speaker: Anamaria Dutceac Segesten, Senior lecturer at Centre for European Studies, Lund University Please register at: http://ai.lu.se/events/registration-2020-06-10 in order to get a link for online participation via the Zoom platform.   The Corona crisis prevents physical meetings and discussions, but in order to continue dialogue and joint learning, we now offer a series of online lunch seminars. Together we watch one or more presentations from previous AI Lund events for 20-30 minutes. Then an expert, often the original speaker, are there to answer questions for 10-15 minutes. You can also ask questions in the text chat during the video. The presentations can be in Swedish or English, and you can ask questions in both languages. On June 10 we watch the presentation that senior lecturer Anamaria Dutceac Segesten gave during the Lund University annual future week 14 October 2019. Read more at LUNDS UNIVERSITY\n",
      "Samhället och AI Samverkan mellan människor och AI Tillämpningar av AI\n",
      "Jonas Ledendal presenterar resultat från två forsknings- och innovationsprojekt om datahantering och AI. Mer om Sjyst Data! Talare och expert:  Jonas Ledendal Juridisk doktor och rättsvetenskaplig forskare vid Institutionen för handelsrätt vid Lunds universitet Vänligen anmäl dig på: http://ai.lu.se/events/registration-2020-05-27 så får du en anslutningslänk till seminariet i plattformen Zoom   Coronakrisen förhindrar de fysiska mötena och diskussioner som hittills varit en central del av AI Lunds utåtriktade verksamhet. Men för att fortsätta dialogen och det gemensamam lärandet arrangerar vi nu en serie nätbaserade lunchseminarier. Inledningsvis tittar vi på en eller flera presentationer från tidigare AI Lund-evenemang under totalt en halvtimme. Efter det har vi med en expert, ofta presentatören själv, där för att svara på frågor i 10-15 minuter. Du kan också ställa frågor i text-chatten. Presentationerna kan vara på svenska eller engelska, men du kan ställa frågor på båda språken. Den 27 maj titta vi tillsammans på presentationen som Jonas Ledendal höll den 29 januari, 2020 at AI Lund's fika-till-fika workshop om AI i offentlig sektor. Titel: Sjyst data i offentlig sektor - rättsliga nyckelområden       Read more at LUNDS UNIVERSITY\n",
      "Human-AI Interaction Societal Aspects of AI\n",
      "The course consists of a learning build-up session and two half-day workshops with on-line lectures in between. The lecturer will guide you how to develop ethical policies for AI. By the end of the course you will have developed an ethical policy in connection to your work. Read more at CHALMERS UNIVERSITY OF TECHNOLOGY\n",
      "Maskininlärning Matematik och statistik\n",
      "Humans as well as other animals are curious beings that develop cognitive skills on their own without the need for external goals or supervision. Inspired by this, how can we encourage AIs to learn and solve tasks by themselves? This talk presents the fascinating area of intrinsic reward in the context of reinforcement learning by showcasing recent articles and result. Please register at: http://ai.lu.se/events/registration-2020-05-20 in order to get a link for online participation via the Zoom platform. Speaker: Erik Gärtner, WASP PhD student at the Centre for Mathematical Sciences, Lund University   The Corona crisis prevents physical meetings and discussions, but in order to continue dialogue and joint learning, we now try a series of online lunch seminars. Together we watch one or more presentations from previous AI Lund events for 20-30 minutes. Then an expert, often the original speaker, is there to answer questions for 10-15 minutes. You can also ask questions in the text chat during the video. The presentations can be in Swedish or English, and you can ask questions in both languages.On May 20, we watch the presentation that WASP PhD Student Erik Gärtner gave on September 30, 2019 at AI Lund's fika-till-fika workshop on AI & ML Technologies. Title: Intrinsic Motivation - Curiosity and learning for the sake of learning     Read more at LUNDS UNIVERSITY\n",
      "Maskininlärning Tillämpningar av AI\n",
      "Join our Live Online seminar AI – Under the Hood of Machine Learning designed for you who wants to understand the magic. You will be able to understand the core concepts, ask the right questions to be involved in the AI initiatives within your company. The concepts of Machine Learning and Neural Networks will be in focus, as these are core concepts where AI delivers great value today. Read more at ROYAL INSTITUTE OF TECHNOLOGY (KTH)\n",
      "Things such as organizations, persons, or locations are all around us, particularly in the news, forum posts, facebook updates, and tweets. With named things, we can introduce background in news articles, summarize articles, build question-answering systems, and much more.  However, it is challenging to find and link them, as they often may be ambiguous. In this work, we aim to enrich the knowledge graph Wikidata with new relations and things only found in the articles of multilingual Wikipedia. The long term goal is the development of a multilingual system that can answer any natural question and improve how we find new relevant information. Speaker: Dr Marcus Klang, Department of Computer Science, Lund University The Corona crisis prevents physical meetings and discussions, but in order to continue dialogue and joint learning, we now offer a series of online lunch seminars. Together we watch one or more presentations from previous AI Lund events for 20-30 minutes. Then an expert, often the original speaker, are there to answer questions for 10-15 minutes. You can also ask questions in the text chat during the video. The presentations can be in Swedish or English, and you can ask questions in both languages. On May 13 we watch the presentation that Dr Marcus Klang gave at the AI Lund fika-to-fika workshop on AI Technologies 30 August 2019. Title: Finding Things in Strings Read more at LUNDS UNIVERSITY\n",
      "Machine learning (ML) plays an important role in problem solving. ML models are used in many application scenarios such as prediction, image and speech recognition and medical diagnoses. Since every problem is unique, there is not one model/solution that fits all. It is up to the user to balance trade-offs and to identify the models that suit the problem at hand. Read more at CHALMERS UNIVERSITY OF TECHNOLOGY\n",
      "Applications of AI Mathematics and statistics Computer Science Data Science Machine Learning\n",
      "In this course you will discover how computer science, mathematics and statistics can be combined to solve business problems through a hands on introduction to data science and artificial intelligence (AI). The lectures and exercises will guide you through the different areas of such as early AI, basic and advanced computing through to data-driven AI and Machine Learning. After completing the course you will be able to choose and combine these different methods to solve business problems. Read more at CHALMERS UNIVERSITY OF TECHNOLOGY\n",
      "Samhället och AI Samverkan mellan människor och AI\n",
      "Detta är en studiecirkel som riktar sig till dig som vill förstå AI och ny maskininlärningsteknik i ett större samhälleligt perspektiv. Vi höjer blicken och ser på AI i ett större perspektiv, och vi fördjupar oss också i frågor som kan ge större förståelse för teknikens betydelse. Vi kommer vid 5 tillfällen ses för att med utgångspunkt i aktuell kulturvetenskaplig forskning skapa kunskap om de utmaningar, möjligheter och potentiella förändringar som står för dörren. Vi kommer att tillsammans försöka förstå inte bara tekniska utan också sociala och organisatoriska utmaningar. Mer information kommer inom kort. För intresseanmälan vänligen kontakta: susanne.norrman@fsi.lu.se Read more at LUNDS UNIVERSITY\n",
      "Örebro universitet erbjuder kursen Autonoma robotar och ROS för dig som yrkesverksam. Utbildningarna leds av det internationellt erkända forskningscentret AASS, som ligger i framkant vad gäller forskning inom artificiell intelligens (AI), robotik och autonoma system ROS (Robot Operating System) är en uppsättning av mjukvarukomponenter som ofta används i forskning inom autonoma system. Kortfattat så hanterar ROS kommunikation mellan olika plattformar, sensorer och ställdon samt visualisering, simulering och loggning av data. I ROS är det även lätt att dela med sig av egna metoder och algoritmer vilket har gjort att det finns en större mängd forskningsresultat implementerat vilka kan användas direkt. ROS har även börjat användas inom industrin, främst inom R&D. Denna kurs ger dig möjlighet att använda och tillämpa ROS inom ett område du önskar. Kursen har tre obligatoriska halvdagsträffar på Örebro universitet. Read more at ÖREBRO UNIVERSITY\n",
      "Planering och schemaläggning Robotik\n",
      "Det som är lovande med intelligenta robotsystem är att de kan utföra fler uppgifter och mer effektivt än en enda industriell robotlösning. Intelligenta robotar agerar kompetent eftersom de kan planera, ordna och genomföra de åtgärder som är lämpliga i det sammanhang där de befinner sig. För att uppnå denna kapacitet använder intelligenta robotar sökmetoder från artificiell intelligens. Dessa är generella algoritmer för att lösa kombinatoriska problem, dvs de utgör en robots \"förmåga att resonera\". Denna kurs introducerar de viktigaste typerna av AI-sökmetoder, samt deras användningar- inom tre industriellt relevanta tillämpningsområden, nämligen resursplanering, rörelseplanering och samordning av flera robotar. Kursen har tre obligatoriska halvdagsträffar på Örebro universitet. Kursen riktar sig till dig som är yrkesverksam. Read more at ÖREBRO UNIVERSITY\n",
      "Behandling av naturliga språk Datorseende Maskininlärning Samhället och AI Samverkan mellan människor och AI Tillämpningar av AI\n",
      "I kursen ingår moment inom maskininlärning, språkteknologi, datorseende, AI och tjänster samt etiska aspekter på AI - för att deltagarna ska kunna hjälpa sina organisationer att ta nästa steg vad gäller användning av artificiell intelligens. Digitaliseringen håller på att transformera hela vårt samhälle. Artificiell intelligens är nästa steg i denna transformation. Den här kursen ger en översikt över AI och hur AI förhåller sig till digitalisering. Du kommer få en bättre överblick över området, insyn i ett antal viktiga tekniker som maskininlärning, datorseende och tolkning av naturligt språk. Kursen tar även upp frågor kring design och interaktion där AI är en del av en tjänst eller produkt samt några av de etiska aspekter som är centrala för hållbar användning av AI. Kursen är upplagd som tre tvådagarsträffar med föreläsningar och interaktiva seminarier samt en laborationsserie som du gör på egen hand för att fördjupa din förståelse för området. Kursen vänder sig till beslutsfattare och utvecklare utan djupa kunskaper i AI. Kursen förväntas ha deltagare från många olika företag och organisationer vilket är ger dig möjlighet att utvidga ditt nätverk. Read more at LINKÖPING UNIVERSITY\n",
      "Datavetenskap Maskininlärning Matematik och statistik\n",
      "Kursen avser att ge en god förståelse för detta tvärvetenskapliga område, med tillräckligt djup för att använda och utvärdera tillgängliga metoder, och för att kunna följa aktuell vetenskaplig litteratur inom området. Under kursens gång kan vi diskutera eventuella problem med maskininlärningsmetoder, till exempel bias i träningsdata och säkerhet för autonoma agenter. Read more at UNIVERSITY OF GOTHENBURG\n",
      "Applications in artificial intelligence can be found at all levels in today's society. The course gives an introduction which explore the field of artificial intelligence from an applied perspective, where a lot of programming can be expected of the course participants. Examples of keywords from the course are, search strategies, machine learning, intelligent agents, AI game technologies, decision algorithms, and natural language processing   Read more at Stockholm University\n",
      "Welcome to apply for a Master Class Series in Artificial Intelligence (AI) and Design. Apply by sending your CV and a motivation letter by March 2nd. The research centre Business and Design Lab (BDL) organize a master class series investigating the intersection between Artificial Intelligence and Design. It will be organized into four master classes, during evening time the 17, and 24 March and 15 and 28 April at HDK-Valand in Gothenburg. Read more at UNIVERSITY OF GOTHENBURG\n",
      "Intelligenta agenter och multiagentsystem Maskininlärning Planering och schemaläggning Tillämpningar av AI\n",
      "Örebro universitet erbjuder kursen Autonoma robotar och ROS för dig som yrkesverksam. Utbildningarna leds av det internationellt erkända forskningscentret AASS, som ligger i framkant vad gäller forskning inom artificiell intelligens (AI), robotik och autonoma system Denna kurs ger en omfattande introduktion till olika traditionella och innovativa modeller, algoritmer och ansatser inom artificiell intelligens, mer än bara maskininlärning. Grundläggande idéer - till exempel för sökning, optimering, planering, resonemang, osäkerhet och även maskininlärning introduceras i videoföreläsningar tillsammans med konceptuella exempel som ska lösas av studenterna. Detta är en distanskurs med tre obligatoriska fysiska halvdagsträffar på Örebro universitet inklusive två seminarier. Under det första seminariet kommer studenterna att presentera ett arbete / uppsats vald från en aktuell litteraturlista sammanställd av lärarna. Lämpliga AI-lösningar för de problem som intresserar studenterna står i fokus vid det andra seminariet. Read more at ÖREBRO UNIVERSITY\n",
      "Samhället och AI Samverkan mellan människor och AI Tillämpningar av AI\n",
      "Var befinner sig den offentliga sektorn när det gäller artificiell intelligens? Hur kan vi förstå både utmaningar och möjligheter på både kommunal som nationell nivå? Frågorna är många. På dagen ett år efter workshopen om AI i offentlig sektor – från ax till limpa bjuder vi in experterna med några av svaren. Read more at LUNDS UNIVERSITY\n",
      "Artificiell intelligens (AI) studerar hur datorer kan utföra uppgifter som traditionellt har ansetts kräva mänsklig intelligens. Kursen ger en introduktion till ämnet och har två huvudsyften. Det ena syftet är att ge en förståelse för vilka delområden som finns inom AI, deras historiska utveckling, och vilka etiska problemställningar som kan uppkomma inom olika delområden. Detta görs genom att läsa litteratur inom olika AI-områden, att sammanfatta och diskutera litteraturen skriftligt, och att granska uppsatser av andra studenter. Det andra syftet är att lära ut grundläggande begrepp och algoritmer för heuristisk (informerad) sökning, planering och problemlösning, inklusive deras användningsområden, samt hur de kan användas för att lösa intressanta AI-problem.   Read more at UNIVERSITY OF GOTHENBURG\n",
      "Artificiell intelligens (AI) studerar hur datorer kan utföra uppgifter som traditionellt har ansetts kräva mänsklig intelligens. Kursen ger en introduktion till ämnet och har två huvudsyften. Det ena syftet är att ge en förståelse för vilka delområden som finns inom AI, deras historiska utveckling, och vilka etiska problemställningar som kan uppkomma inom olika delområden. Detta görs genom att läsa litteratur inom olika AI-områden, att sammanfatta och diskutera litteraturen skriftligt, och att granska uppsatser av andra studenter. Det andra syftet är att lära ut grundläggande begrepp och algoritmer för heuristisk (informerad) sökning, planering och problemlösning, inklusive deras användningsområden, samt hur de kan användas för att lösa intressanta AI-problem. Read more at UNIVERSITY OF GOTHENBURG\n",
      "Computer Science Machine Learning\n",
      "The course gives an introduction to machine learning techniques and theory, with a focus on its use in practical applications. During the course, a selection of topics will be covered in supervised learning, such as linear models for regression and classification, or nonlinear models such as neural networks, and in unsupervised learning such as clustering. The use cases and limitations of these algorithms will be discussed, and their implementation will be investigated in programming assignments. Methodological questions pertaining to the evaluation of machine learning systems will also be discussed, as well as some of the ethical questions that can arise when applying machine learning technologies. There will be a strong emphasis on the real-world context in which machine learning systems are used. The use of machine learning components in practical applications will be exemplified, and realistic scenarios will be studied in application areas such as ecommerce, business intelligence, natural language processing, image processing, and bioinformatics. The importance of the design and selection of features, and their reliability, will be discussed. Read more at UNIVERSITY OF GOTHENBURG\n",
      "Samhället och AI Samverkan mellan människor och AI Tillämpningar av AI\n",
      "Det här fyra timmar långa seminariet ger en introduktion till artificiell intelligens. Under den första timmen behandlar vi de grundläggande byggstenarna i AI och förklarar hur de senaste algoritmerna fungerar. Den andra timmen ägnar vi åt att beskriva vilka förutsättningarna som krävs för att integrera AI på arbetsplatsen. Vi kommer att gå igenom fördelarna och möjliga fallgropar med att använda AI på olika typer av användningsområden. De två sista timmarna ägnar vi åt dels att förklara relevanta begrepp för beslutsfattare som vill anamma och använda AI, dels att titta på de här aspekterna ur ett globalt perspektiv. Det krävs ingen tidigare erfarenhet eller kunskap om AI för att delta på seminariet. Föreläsningen hålls på engelska men diskussionerna är på svenska. Read more at ÖREBRO UNIVERSITY\n",
      "Datavetenskap Maskininlärning Planering och schemaläggning Tillämpningar av AI\n",
      "I denna kurs erbjuder Linnéuniversitetet i samarbete med Örebro universitet en gedigen introduktion till olika traditionella och innovativa modeller, algoritmer och ansatser inom artificiell intelligens. Grundläggande idéer – till exempel för sökning, optimering, planering, resonemang, osäkerhet och även maskininlärning introduceras i föreläsningar tillsammans med konceptuella exempel som ska lösas av kursdeltagarna. Read more at ÖREBRO UNIVERSITY\n",
      "Samhället och AI Samverkan mellan människor och AI\n",
      "The European Union has taken the stance that AI should be trustworthy and developed in a human-centric way with the goal of improving individual and societal well-being. This talk will present the European approach to trustworthy human-centric AI and some research challenges related to it. To be trustworthy an AI-system should be lawful, ethical and robust, as defined by the European Commission High-Level Expert Group on AI. To operationalize these is a major challenge and will require new research. The second part of the talk gives an overview of the state-of-the-art and potential future solutions to these challenges. Read more at LUNDS UNIVERSITY\n",
      "Behandling av naturliga språk Tillämpningar av AI\n",
      "In medicine and life sciences we are overloaded with different forms of text and speech. Texts in the form of electronic health records, genomic sequences, scientific articles, twitter feeds or patient questionnaires can be analysed to gain new scientific insights. Speech technology can be used in health apps and chatbots, to extract information from doctor-patient-dialogues or to direct medical robots. This workshop will cover the different application areas of language technology in medicine and life sciences as well as the methods used in this exciting and broad field. Read more at LUNDS UNIVERSITY\n",
      "Maskininlärning Samhället och AI Samverkan mellan människor och AI\n",
      "Do you want to decode AI and identify key viable knowledge concerning AI for managers together with Lund University and The ESS? Whether completely, partly or prospectively immersed in this technology, join us for a full day designed for managers who are concerned with the development, implementation and/or other types of uses of AI and Machine Learning from a business standpoint. Read more at LUNDS UNIVERSITY\n",
      "Welcome to this course open both to PhD-students (free) as well as to professionals (fee). The aim of the course is to give necessary knowledge of digital image analysis for further research within the area and to be able to use digital image analysis within other research areas such as computer graphics, image coding, video coding and industrial image processing problems. Read more at LUNDS UNIVERSITY\n",
      "This course is intended for working professionals in Swedish industry who want the learn the basics of machine learning. The course is designed for you who have a previous university education. Read more at LINKÖPING UNIVERSITY\n",
      "Kursen ger en överblick av teorier och praktiskt komputationella implementationer av hur naturligt språk samverkar med världen. I kursen avhandlas områden som semantisk teori för naturligt språk, handling och perception, situerad dialog, situerad språkinlärning, koppling av språk i handling och perception, spatial kognition, bildgenerering och -tolkning, integrerade robotdialogsystem och dyl. Read more at UNIVERSITY OF GOTHENBURG\n",
      "Datavetenskap Datorseende Maskininlärning\n",
      "I denna kurs behandlas designmetoder och olika algoritmer för och varianter av Deep Learning inom klassifiering, prediktion, interaktion och modellering med användning av olika typer av data; ljud, bild, video, text, sekvenser. Begrepp såsom Överträning, Dropouts och Gradienter gås igenom.   Installation och användning av hård- och mjukvaror för experimenterande med topologier såsom CNN, RNN, LSTM, DQN och tillhörande parametrar samt inspektion av resultatet utgör en väsentlig del av kursen. Kursen går även kortfattat igenom Dataanalys, Maskininlärning, Tensorer, Datorseende, Transfer Learning,  Grunder i Robotik, Reinforcement Learning och metoder med kombinationen Deep Reinforcement Learning. Numeriska implementationer studeras översiktligt. Det är en tidsmässig fördel att ha tillgång till lämpliga beräknings(grafik)kort alternativt ha/få tillgång till motsvarande molntjänst. Man kan köra Windows, OS X eller Linux. Read more at UMEÅ UNIVERSITY\n",
      "Samhället och AI Samverkan mellan människor och AI\n",
      "Welcome to a full day of presentations highlighting the link between Artificial Intelligence and Social Science reserach. In the first part of the day we will showcase several research projects driven by Lund University's own scholars in the field of AI and Social Science. In the afternoon, we will focus on the (thinking behind) funding opportunities available for researchers interested in applying for grants at the European level, but also showing evidence of projects that are more local in nature. While the morning session is open to the interested public, the afternoon session is targeted at LU researchers Read more at\n",
      "Machine Learning\n",
      "Over the past few years, neural networks have enjoyed a major resurgence in machine learning, and today yield state-of-the-art results in various fields. This course provides an introduction to deep neural network models, and surveys some the applications of these models in areas where they have been particularly successful. The course covers deep feed-forward networks, recurrent networks, convolutional networks, as well general topics such as input encoding and training techniques. The course also provides practical experience with some of the software libraries available for building and training deep neural networks. Read more at LINKÖPING UNIVERSITY\n",
      "The course offers knowledge on Robot Operating System (ROS), how it works and how it is applied within a variety of artificial intelligence and robotics areas. You will use ROS to design parts of an intelligent system and test these both in simulation as well as on existing robot platforms. Read more at ÖREBRO UNIVERSITY\n",
      "Neurala nätverk är fördelade beräkningsmodeller inspirerade av strukturen i denmänskliga hjärnan som består av många enkla bearbetningselement vilka är anslutna iett nätverk. Neurala nätverk används alltmer inom ingenjörsvetenskap för uppgiftersom mönsterigenkänning, prediktion och kontroll. Teorin om neurala nätverk är ett tvärvetenskaplig fält (neurobiologi, datavetenskap och statistisk fysik). Kursen ger en överblick och en grundläggande förståelse för neurala-nätverksalgoritmer. Read more at UNIVERSITY OF GOTHENBURG\n",
      "Computer Science\n",
      "Artificial intelligence (AI) is a term used for describing technologies that are used for designing and constructing intelligent computer programs. There is a vast amount of AI algorithms and AI software tools to build intelligent computer programs. This course is an introduction to AI for professionals in industry and public organisations who have knowledge in engineering and system development. The course gives a wide perspective on different well-established AI methods and tools in order to show the potential opportunities to develop new AI-based technologies. The main themes of the course are theories and algorithms, which go from classical AI to emerging machine learning algorithms. The course also introduces social implications of the AI-based technologies, such as responsible development of AI. Read more at UMEÅ UNIVERSITY\n",
      "The course covers three specialist fields of knowledge within AI: planning of resources, robot motion planning and multi-robot coordination, as well as introduce the specific principles behind these subjects, namely systematic and sampling-based searching algorithms, constraint-based reasoning and robot kinematics. Within each subject (planning of resources, robot motion planning and multi-robot coordination), a state-of-the-art tool will be presented and used as the basis for a project work. Read more at ÖREBRO UNIVERSITY\n",
      "The course addresses the basic concepts within classical artificial intelligence (other than machine learning). Traditional artificial intelligence is characterized by the so-called declarative approach to problem solving. The course deals with a selection of different intelligent problem-solving methods, both in theory and practice. After completing the course, the student will be able to model and use appropriate generic solution algorithms to solve problems in an intelligent system. Read more at ÖREBRO UNIVERSITY\n",
      "Applications of AI Computer Science Planning and Scheduling Knowledge Representation and Reasoning\n",
      "Answer Set Programming (ASP) is an approach developed within the field of artificial intelligence (AI) for solving difficult search problems. ASP was initially developed for  modeling problems in the branch of AI that is commonly referred to as Knowledge Representation and Reasoning (KRR), and has been identified as a significant contribution in the research field of AI. More recently, ASP has become highly attractive for the representation of and solving search problems both for academia and industry. ASP been successfully applied for solving problems in domains such as bioinformatics, scheduling, timetabling, dynamic reconfiguration, and software engineering. This course introduces the Answer Set Programming paradigm and shows its potential to deal with industrial problems. The course is for professionals in industry and public organisations who have knowledge in engineering and system development, who want to gain deepened knowledge in AI. Read more at UMEÅ UNIVERSITY\n",
      "Knowledge Representation and Reasoning\n",
      "The course addresses the major principles in logic and constraint programming. The main focus of the course is on Stable Model or Answer Set semantics. This course  also focuses on formalizing and solving various problems within a declarative paradigm. After completing the course, the student will be able to apply a suitable symbolic reasoning method based on answer set solvers to solve a problem within an intelligent system. Read more at ÖREBRO UNIVERSITY\n",
      "Samhället och AI Samverkan mellan människor och AI\n",
      "Vilken roll spelar informationsteknologin i morgondagens skola? Hur förändrar AI och digitalisering villkoren för hur vi lär oss och hur vi skapar ny kunskap? Nu kan du som är verksam inom skolan och utbildningsväsendet på ett enkelt och snabbt sätt förstå mer. Vårt kurspaket vänder sig till lärare från grundskola till högre utbildning, som vill fördjupa sig i aktuell forskning om digitalisering och dess betydelse för skolan och för lärande. För att förstå detta komplexa och snabbföränderliga område och få svar på dina frågor har du meriterade forskare och experter inom området till din hjälp. Read more at UNIVERSITY OF GOTHENBURG\n",
      "This course looks into different phenomena of human interaction with mixed reality. We will examine different aspects of human perception of mixed reality, depending on such factors as fidelity, immersion, and presence. In the course we look at the way people act when being immersed into virtual or remote environment. And we look at the methods of building efficient interaction scenarios and measuring interaction quality, using qualitative and quantitative tools. Upon completion of this course, students will be able to apply theoretical knowledge on developing mixed reality experiences to appropriate industrial problems and evaluate the effect of mixed reality in a critical manner. Read more at ÖREBRO UNIVERSITY\n",
      "The course offers knowledge of the basic concepts with machine learning, the selection and application of different machine learning algorithms as well as evaluation of the performance of these learning systems. After completing the course, student should be able to prepare data and apply machine learning techniques to solve a problem in an intelligent system. Read more at ÖREBRO UNIVERSITY\n",
      "Samhället och AI Samverkan mellan människor och AI\n",
      "Vilken roll spelar informationsteknologin i morgondagens skola? Hur förändrar AI och digitalisering villkoren för hur vi lär oss och hur vi skapar ny kunskap? Nu kan du som är verksam inom skolan och utbildningsväsendet på ett enkelt och snabbt sätt förstå mer. Vårt kurspaket vänder sig till lärare från grundskola till högre utbildning, som vill fördjupa sig i aktuell forskning om digitalisering och dess betydelse för skolan och för lärande. För att förstå detta komplexa och snabbföränderliga område och få svar på dina frågor har du meriterade forskare och experter inom området till din hjälp. Read more at UNIVERSITY OF GOTHENBURG\n",
      "Behandling av naturliga språk Datavetenskap Datorseende Maskininlärning\n",
      "This AIML@LU fika-to-fika workshop focuses on the development of the technologies that form the basis of Artificial Intelligence and Machine Learning. Possible topics to discuss are the research front for different types of AI, but also to look at different techniques for machine learning. Read more at LUNDS UNIVERSITY\n",
      "Maskininlärning Samhället och AI Samverkan mellan människor och AI\n",
      "Workshop on opportunities and perils of fake data produced by the AI concept Generative Adversarial Networks (GAN) and other generative machine learning techniques. In the last 5 years the field of so-called creative or generative Artificial Intelligence, notably Generative Adversarial Networks (GANs), has progressed in an enormous pace. What does it mean for our society that AI is gaining an increasing capacity to (re-)produce informational patterns? How does the world look like when it is populated with deep fakes and synthetic realities? Read more at LUNDS UNIVERSITY\n",
      "Maskininlärning Samhället och AI Samverkan mellan människor och AI Tillämpningar av AI\n",
      "Exploring AI – The future benefits and challenges. Beyond the traditional conference we present the unique POWWOW experience. During this POWWOW we gather across sectors and competences around the hot topic Artificial Intelligence. Together we explore the impact it will have on society and our common future, but we also look into specific industries and their applications. Read more at LUNDS UNIVERSITY\n",
      "Informationsvetenskap Matematik och statistik Tillämpningar av AI\n",
      "Learn how to use analytics to improve the performance of your organization. In this 2-day course, you will learn how to analyze data, build models and communicate results to create business value. Read more at CHALMERS UNIVERSITY OF TECHNOLOGY\n",
      "Maskininlärning Samhället och AI Samverkan mellan människor och AI Tillämpningar av AI\n",
      "Artificiell intelligens i t. ex. hälso- och vårdappar, i administrativa system eller för prioritering av vårdköer. Den som ska ta ställning till om man vill, får och bör använda AI-teknologier inom hälsovården har mycket att fundera på redan innan man tänker på hur det kan gå till. Den 9 Maj arrangerar eHealth@LU tillsammans med AIML@LU en fika-till-fika workshop för att belysa möjligheter, utmaningar och forskning inom området. Read more at LUNDS UNIVERSITY\n",
      "Informationsvetenskap Matematik och statistik Tillämpningar av AI\n",
      "Learn how to use analytics to improve the performance of your organization. In this 2-day course, you will learn how to analyze data, build models and communicate results to create business value. Read more at CHALMERS UNIVERSITY OF TECHNOLOGY\n",
      "Behandling av naturliga språk Datorseende Maskininlärning Samhället och AI Samverkan mellan människor och AI\n",
      "Under vårterminen 2019 erbjuder vi en sex dagars översiktskurs som ger 3 högskolepoäng. I kursen ingår bland annat moment inom maskininlärning, språkteknologi, datorseende, AI och tjänster samt etiska aspekter på AI - för att deltagarna ska kunna hjälpa sina organisationer att ta nästa steg vad gäller användning av artificiell intelligens. Read more at LINKÖPING UNIVERSITY\n",
      "Maskininlärning Samhället och AI Samverkan mellan människor och AI\n",
      "Vi ser just nu hur ett nytt handelslandskap växer fram, med nya möjligheter och nya utmaningar – och inte minst ny teknik och nya aktörer. Handeln blir allt mer informationsdriven, och en ny industri kring insamling, hantering och analys av kunddata växer fram. Det handlar dels om att lära sig om och förutse behov, men också om att kunna anpassa budskap och erbjudande. Frågorna kring AI och maskininlärning är många, och under en halvdag kommer vi att få lite olika perspektiv på frågan. Read more at LUNDS UNIVERSITY\n",
      "Matematik och statistik Planering och schemaläggning Samhället och AI Tillämpningar av AI\n",
      "Örebro universitet erbjuder kursen Introduktion till artificiell intelligens som tar upp grundläggande koncept inom klassisk artificiell intelligens (utöver maskininlärning). Traditionell artificiell intelligens karaktäriseras av så kallad deklarativ ansats för problemlösning. Kursen behandlar ett urval av olika intelligenta problemlösningsmetoder, såväl teoretiskt som praktiskt. Read more at ÖREBRO UNIVERSITY\n",
      "Örebro University offers a course in Reinforcement Learning. The course provides a general introduction to reinforcement learning in theory and practice. Read more at ÖREBRO UNIVERSITY\n",
      "Örebro universitet erbjuder en introduktion till ROS (Robot Operating System). Under utbildningen erbjuds kunskap om hur ROS fungerar och hur ROS kan tillämpas inom olika områden inom robotik och artificiell intelligens. Read more at ÖREBRO UNIVERSITY\n",
      "We are arranging a Workshop on Sensing, Imaging, and Machine Learning at Lund University, March 5, 2019. This workshop is organized in cooperation between Lund University and Saab. The aim is to provide the participants with an opportunity to discuss and learn about theory and applications in Sensing, Imaging, and Machine Learning. Read more at LUNDS UNIVERSITY\n",
      "An autonomous robot is a robot that performs behaviors or tasks with a high degree of autonomy. Autonomous robotics is usually considered to be a subfield of artificial intelligence, robotics, and information engineering. Read more at LUNDS UNIVERSITY\n",
      "Maskininlärning Samhället och AI Samverkan mellan människor och AI Tillämpningar av AI\n",
      "Detta blir den femte öppna fika-till-fika-workshopen om och kring forskning om artificiell intelligens vid Lunds universitet. Det övergripande temat är AI i den offentliga sektorn, med underteman som AI i demokratisk processer, AI i beslutsstöd, AI-driven offentlig service... Read more at LUNDS UNIVERSITY\n",
      "Machine learning is a field of artificial intelligence that uses statistical techniques to give computer systems the ability to \"learn\" (e.g., progressively improve performance on a specific task) from data, without being explicitly programmed. Read more at ROYAL INSTITUTE OF TECHNOLOGY (KTH)\n",
      "Informationsvetenskap Matematik och statistik Tillämpningar av AI\n",
      "Learn how to use analytics to improve the performance of your organization. In this 2-day course, you will learn how to analyze data, build models and communicate results to create business value. Read more at CHALMERS UNIVERSITY OF TECHNOLOGY\n",
      "Maskininlärning Samhället och AI Samverkan mellan människor och AI Tillämpningar av AI\n",
      "Artificiell Intelligens har utvecklats kraftigt de senaste åren, speciellt inom delområdet maskininlärning. I den här endags kursen ger vi en översiktlig introduktion till Artificiell Intelligens och Maskininlärning. Du får lära dig mer om vad det är, testa på att göra lite egen maskininlärning och höra mer om dess möjligheter men också rättsliga och etiska utmaningar. Under kursen vill vi även höra mer om era frågeställningar och fundera på vilka kurser som Lunds universitet skulle kunna utveckla för att hjälpa er att möta dessa utmaningar. Read more at LUNDS UNIVERSITY\n",
      "Maskininlärning Samhället och AI Samverkan mellan människor och AI\n",
      "How do you ensure fairness in the application of AI and ML? How transparent and explainable need autonomous and self-learning services be from a legal, social or ethical perspective? How do you balance privacy or other values against a need for access to training-data? Can an algorithm really illegally discriminate? And, to what extent can AI and ML be a method within social sciences or the humanities? Read more at LUNDS UNIVERSITY\n",
      "Att lyfta fram orden unbiased recruitment är det starkaste säljargumentet för AI-rekrytering idag. Liksom att algoritmerna styr. Det handlar om att AI är blind för… Read more at UMEÅ UNIVERSITY\n",
      "From probability and statistics to data analysis and machine learning, master the skills needed to solve complex challenges with data. Read more at ÖREBRO UNIVERSITY\n",
      "Argumentet mot AI är ofta att utvecklingen fortfarande är i ett tidigt skede och att systemen och algoritmerna måste bli mer intelligenta. Men inom HR går det fort och många vill leda utvecklingen Read more at ÖREBRO UNIVERSITY\n",
      "Datorseende Maskininlärning Samverkan mellan människor och AI Tillämpningar av AI\n",
      "The aim of the day is to do an inventory and give information about on ongoing or planned projects in artificial intelligence and machine learning in medicine. It is also a day to make connections and exchange experiences. Examples on common challenges are specific for the medicine are 1) large scale image management, 2) extract images from PACS system, 3) privacy and GDPR, 4) Computational capacity inside hospital firewall. Read more at LUNDS UNIVERSITY\n"
     ]
    }
   ],
   "source": [
    "print(texts[filename][\"dragnet\"][\"doc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "21775e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scroll to find your event or course.\n",
      "\n",
      "Don't you know what you are looking for? Use our competence guide that explains.\n"
     ]
    }
   ],
   "source": [
    "print(texts[filename][\"newspaper3k\"][\"doc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3988f7d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ai-competence.se/en/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "title              AI COMPETENCE FOR SWEDEN\n",
       "country                              Sweden\n",
       "documentUrl    https://ai-competence.se/en/\n",
       "startDate                            2018.0\n",
       "endDate                                 NaN\n",
       "oecdId                                26630\n",
       "Name: 31, dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = oecd.loc[int(filename[:-5])]\n",
    "print(tmp[\"documentUrl\"])\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77f65f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "255d8981",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "543.html\n",
      "word diff 6880\n",
      "newspaper3k: 11589, dragnet 4709\n"
     ]
    }
   ],
   "source": [
    "filename = s[-4][0]\n",
    "print(filename)\n",
    "print(\"word diff %s\" % lens[filename])\n",
    "print(\"newspaper3k: %s, dragnet %s\" %(texts[filename][\"newspaper3k\"][\"len\"], texts[filename][\"dragnet\"][\"len\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3578ad83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on intellectual property rights for the development of artificial intelligence technologies\n",
      "–   having regard to the Treaty on the Functioning of the European Union (TFEU), in particular Articles 4, 16, 26, 114 and 118 thereof,\n",
      "–   having regard to the Berne Convention for the Protection of Literary and Artistic Works,\n",
      "–   having regard to the Interinstitutional Agreement of 13 April 2016 on Better Law-Making [1] and the Commission’s Better Regulations Guidelines ( COM(2015)0215 ),\n",
      "–   having regard to the World Intellectual Property Organisation (WIPO) Copyright Treaty, the WIPO Performances and Phonograms Treaty and the WIPO revised Issues Paper of 29 May 2020 on Intellectual Property Policy and Artificial Intelligence,\n",
      "–   having regard to Directive (EU) 2019/790 of the European Parliament and of the Council of 17 April 2019 on copyright and related rights in the Digital Single Market and amending Directives 96/9/EC and 2001/29/EC [2] ,\n",
      "–   having regard to Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases [3] ,\n",
      "–   having regard to Regulation (EU) 2018/1807 of the European Parliament and of the Council of 14 November 2018 on a framework for the free flow of non-personal data in the European Union [8] ,\n",
      "–   having regard to Regulation (EU) 2019/1150 of the European Parliament and of the Council of 20 June 2019 on promoting fairness and transparency for business users of online intermediation services [9] ,\n",
      "–   having regard to the work of the High-Level Expert Group on Artificial Intelligence set up by the Commission,\n",
      "–   having regard to the Commission communications entitled ‘A European Data Strategy’ ( COM(2020)0066 ) and ‘A New Industrial Strategy for Europe’ ( COM(2020)0102 ),\n",
      "–   having regard to the Guidelines for Examination in the European Patent Office of November 2019,\n",
      "–   having regard to the digital economy working paper 2016/05 of the Commission’s Joint Research Centre and its Institute for Prospective Technological Studies entitled ‘An Economic Policy Perspective on Online Platforms’,\n",
      "–   having regard to the political guidelines for the next European Commission 2019-2024 entitled ‘A Union that strives for more: my agenda for Europe’,\n",
      "–   having regard to its resolution of 16 February 2017 with recommendations to the Commission on Civil Law Rules on Robotics [10] ,\n",
      "–   having regard to Rule 54 of its Rules of Procedure,\n",
      "–   having regard to the opinions of the Committee on the Internal Market and Consumer Protection, the Committee on Transport and Tourism and the Committee on Culture and Education,\n",
      "–   having regard to the report of the Committee on Legal Affairs (A9-0176/2020),\n",
      "A.   whereas the Union’s legal framework for intellectual property aims to promote innovation, creativity and access to knowledge and information;\n",
      "B.   whereas Article 118 of the TFEU stipulates that the Union legislator must establish measures for the creation of European intellectual property rights (IPRs) to provide uniform protection of those rights throughout the Union; whereas the single market is conducive to the stronger economic growth needed to ensure the prosperity of Union citizens;\n",
      "C.   whereas recent developments in artificial intelligence (AI) and similar emerging technologies represent a significant technological advance that is generating opportunities and challenges for Union citizens, businesses, public administrations, creators and the defence sector;\n",
      "D.   whereas AI technologies may render the traceability of IPRs and their application to AI-generated output difficult, thus preventing human creators whose original work is used to power such technologies from being fairly remunerated;\n",
      "E.   whereas the aim of making the Union the world leader in AI technologies must encompass efforts to regain and safeguard the Union’s digital and industrial sovereignty, ensure its competitiveness and promote and protect innovation, and must require a structural reform of the Union’s industrial policy to allow it to be at the forefront of AI technologies while respecting cultural diversity; whereas the Union's global leadership in AI calls for an effective intellectual property system which is fit for the digital age, enabling innovators to bring new products to the market; whereas strong safeguards are crucial to protect the Union’s patent system against abuse, which is detrimental to innovative AI developers; whereas a human-centred approach to AI that is compliant with ethical principles and human rights is needed if the technology is to remain a tool that serves people and the common good;\n",
      "F.   whereas the Union is the appropriate level at which to regulate AI technologies in order to avoid fragmentation of the single market and differing national provisions and guidelines; whereas a fully harmonised Union regulatory framework in the field of AI will have the potential to become a legislative benchmark at international level; whereas new common rules for AI systems should take the form of a regulation in order to establish equal standards across the Union and whereas legislation must be future-proofed to ensure it can keep pace with the fast development of this technology, and must be followed up on through thorough impact assessments; whereas legal certainty fosters technological development, and whereas public confidence in new technologies is essential for the development of this sector, as it strengthens the Union’s competitive advantage; whereas the regulatory framework governing AI should therefore inspire confidence in the safety and reliability of AI and strike a balance between public protection and business incentives for investment in innovation;\n",
      "G.   whereas AI and related technologies are based on computational models and algorithms, which are regarded as mathematical methods within the meaning of the European Patent Convention (EPC) and are therefore not patentable as such; whereas mathematical methods and computer programs may be protected by patents under Article 52(3) of the EPC when they are used as part of an AI system that contributes to producing a further technical effect; whereas the impact of such potential patent protection should be thoroughly assessed;\n",
      "H.   whereas AI and related technologies are based on the creation and execution of computer programs which, as such, are subject to a specific copyright protection regime, whereby only the expression of a computer program may be protected, and not the ideas, methods and principles which underlie any element of it;\n",
      "I.   whereas an increasing number of AI-related patents are being granted;\n",
      "J.   whereas the development of AI and related technologies raises questions about the protection of innovation itself and the application of IPRs to materials, content or data generated by AI and related technologies, which can be of an industrial or artistic nature and which create various commercial opportunities; whereas in this regard it is important to distinguish between AI-assisted human creations and creations autonomously generated by AI;\n",
      "J.   whereas AI and related technologies are heavily dependent on pre-existing content and large volumes of data; whereas increased transparent and open access to certain non-personal data and databases in the Union, especially for SMEs and start-ups, as well as interoperability of data, which limits lock-in effects, will play a crucial role in advancing the development of European AI and supporting the competitiveness of European companies at the global level; whereas the collection of personal data must respect fundamental rights and data protection rules and requires tailored governance, namely in terms of data management and the transparency of data used in developing and deploying AI technologies, and this throughout the entire lifecycle of an AI-enabled system;\n",
      "1.   Takes note of the Commission White Paper on ‘Artificial Intelligence - A European approach to excellence and trust’ and the European Data Strategy; stresses that the approaches outlined therein are likely to contribute to unlocking the potential of human-centred AI in the EU; notes, however, that the issue of the protection of IPRs in the context of the development of AI and related technologies has not been addressed by the Commission, despite the key importance of these rights; highlights the necessity of creating a single European data space and believes that the use thereof will play an important role in innovation and creativity in the Union economy, which should be incentivised; stresses that the Union should play an essential role in laying down basic principles on the development, deployment and use of AI, without hindering its advancement or impeding competition;\n",
      "2.   Highlights the fact that the development of AI and related technologies in the transport and tourism sectors will bring innovation, research, the mobilisation of investment and considerable economic, societal, environmental, public and safety benefits, while making these sectors more attractive to new generations and creating new employment opportunities and more sustainable business models, but stresses that it should not cause harm or damage to people or society;\n",
      "3.   Stresses the importance of creating an operational and fully harmonised regulatory framework in the field of AI technologies; suggests that such a framework should take the form of a regulation rather than a directive in order to avoid fragmentation of the European digital single market and promote innovation;\n",
      "4   Calls on the Commission to take into account the seven key requirements identified in the Guidelines of the High-Level Expert Group, as welcomed by it in its communication of 8 April 2019 [11] , and properly implement them in all legislation dealing with AI;\n",
      "5.   Stresses that the development, deployment and use of AI technologies and the growth of the global data economy make it necessary to address significant technical, social, economic, ethical and legal issues in a variety of policy areas, including IPRs and their impact on these policy areas; highlights that in order to unlock the potential of AI technologies, it is necessary to remove unnecessary legal barriers, so as not to hamper the growth of or innovation in the Union’s developing data economy; calls for an impact assessment to be conducted with regards to the protection of IPRs in the context of the development of AI technologies;\n",
      "6.   Stresses the key importance of balanced IPR protection in relation to AI technologies, and of the multidimensional nature of such protection, and, at the same time, stresses the importance of ensuring a high level of protection of IPRs, of creating legal certainty and of building the trust needed to encourage investment in these technologies and ensure their long-term viability and use by consumers; considers that the Union has the potential to become the frontrunner in the creation of AI technologies by adopting an operational regulatory framework that is regularly assessed in the light of technological developments and by implementing proactive public policies, particularly as regards training programmes and financial support for research and public-private sector cooperation; reiterates the need to ensure sufficient leeway for the development of new technologies, products and services; emphasises that creating an environment conducive to creativity and innovation by encouraging the use of AI technologies by creators must not come at the expense of the interests of human creators, nor the Union’s ethical principles;\n",
      "7.   Considers also that the Union must address the various aspects of AI by means of definitions that are technologically neutral and sufficiently flexible to encompass future technological developments as well as subsequent uses; considers it necessary to continue to reflect on interactions between AI and IPRs, from the perspective of both intellectual property offices and users; believes that the challenge of assessing AI applications creates a need for some transparency requirements and the development of new methods as, for instance, adaptive learning systems may recalibrate following each input, making certain ex ante disclosures ineffective;\n",
      "8.   Stresses the importance of streaming services being transparent and responsible in their use of algorithms, so that access to cultural and creative content in various forms and different languages as well as impartial access to European works can be better guaranteed;\n",
      "9.   Considers the increasing need for AI and related technologies in remote or biometric recognition technologies, such as tracing apps in the transport and tourism sector, as a new way of dealing with COVID-19 and possible future sanitary and public health crises, while keeping sight of the need to protect fundamental rights, privacy and personal data;\n",
      "10.   Recommends that priority be given to assessment by sector and type of IPR implications of AI technologies; considers that such an approach should take into account, for example, the degree of human intervention, the autonomy of AI, the importance of the role and the origin of the data and copyright-protected material used and the possible involvement of other relevant factors; recalls that any approach must strike the right balance between the need to protect investments of both resources and effort and the need to incentivise creation and sharing; takes the view that more thorough research is necessary for the purposes of evaluating human input regarding AI algorithmic data; believes that disruptive technologies such as AI offer both small and large companies the opportunity to develop market-leading products; considers that all companies should benefit from equally efficient and effective IPR protection; therefore calls on the Commission and the Member States to offer support to start-ups and SMEs via the Single Market Programme and Digital Innovation Hubs in protecting their products;\n",
      "11.   Suggests that this assessment focus on the impact and implications of AI and related technologies under the current system of patent law, trademark and design protection, copyright and related rights, including the applicability of the legal protection of databases and computer programs, and the protection of undisclosed know-how and business information (‘trade secrets’) against their unlawful acquisition, use and disclosure; acknowledges the potential of AI technologies to improve the enforcement of IPRs, notwithstanding the need for human verification and review, especially where legal consequences are concerned; emphasises, further, the need to assess whether contract law ought to be updated in order to best protect consumers and whether competition rules need to be adapted in order to address market failures and abuses in the digital economy, the need to create a more comprehensive legal framework for the economic sectors in which AI plays a part, thus enabling European companies and relevant stakeholders to scale up, and the need to create legal certainty; stresses that the protection of intellectual property must always be reconciled with other fundamental rights and freedoms;\n",
      "12.   Points out that mathematical methods as such are excluded from patentability unless they are used for a technical purpose in the context of technical inventions, which are themselves patentable only if the applicable criteria relating to inventions are met; points out, further, that if an invention relates either to a method involving technical means or to a technical device, its purpose, considered as a whole, is in fact technical in nature and is therefore not excluded from patentability; underlines, in this regard, the role of the patent protection framework in incentivising AI inventions and promoting their dissemination, as well as the need to create opportunities for European companies and start-ups to foster the development and uptake of AI in Europe; points out that standard essential patents play a key role in the development and dissemination of new AI and related technologies and in ensuring interoperability; calls on the Commission to support the establishment of industry standards and encourage formal standardisation;\n",
      "13.   Notes that patent protection can be granted provided that the invention is new and not self-evident and involves an inventive step; notes, further, that patent law requires a comprehensive description of the underlying technology, which may pose challenges for certain AI technologies in view of the complexity of the reasoning; stresses also the legal challenges of reverse engineering, which is an exception to the copyright protection of computer programs and the protection of trade secrets, which are in turn of crucial importance for innovation and research and which should be duly taken into account in the context of the development of AI technologies; calls on the Commission to assess possibilities for products to be adequately tested, for example in a modular way, without creating risks for IPR holders or trade secrets due to extensive disclosure of easily replicated products; stresses that AI technologies should be openly available for educational and research purposes, such as more effective learning methods;\n",
      "14.   Notes that the autonomisation of the creative process of generating content of an artistic nature can raise issues relating to the ownership of IPRs covering that content; considers, in this connection, that it would not be appropriate to seek to impart legal personality to AI technologies and points out the negative impact of such a possibility on incentives for human creators;\n",
      "15.   Points out the difference between AI-assisted human creations and AI-generated creations, with the latter creating new regulatory challenges for IPR protection, such as questions of ownership, inventorship and appropriate remuneration, as well as issues related to potential market concentration; further considers that IPRs for the development of AI technologies should be distinguished from IPRs potentially granted for creations generated by AI; stresses that where AI is used only as a tool to assist an author in the process of creation, the current IP framework remains applicable;\n",
      "16.   Takes the view that technical creations generated by AI technology must be protected under the IPR legal framework in order to encourage investment in this form of creation and improve legal certainty for citizens, businesses and, since they are among the main users of AI technologies for the time being, inventors; considers that works autonomously produced by artificial agents and robots might not be eligible for copyright protection, in order to observe the principle of originality, which is linked to a natural person, and since the concept of ‘intellectual creation’ addresses the author’s personality; calls on the Commission to support a horizontal, evidence-based and technologically neutral approach to common, uniform copyright provisions applicable to AI-generated works in the Union, if it is considered that such works could be eligible for copyright protection; recommends that ownership of rights, if any, should only be assigned to natural or legal persons that created the work lawfully and only if authorisation has been granted by the copyright holder if copyright-protected material is being used, unless copyright exceptions or limitations apply; stresses the importance of facilitating access to data and data sharing, open standards and open source technology, while encouraging investment and boosting innovation;\n",
      "17.   Notes that AI makes it possible to process a large quantity of data relating to the state of the art or the existence of IPRs; notes, at the same time, that AI or related technologies used for the registration procedure to grant IPRs and for the determination of liability for infringements of IPRs cannot be a substitute for human review carried out on a case-by-case basis, in order to ensure the quality and fairness of decisions; notes that AI is progressively gaining the ability to perform tasks typically carried out by humans and stresses, therefore, the need to establish adequate safeguards, including design systems with human-in-the-loop control and review processes, transparency, accountability and verification of AI decision-making;\n",
      "18.   Notes, with regard to the use of non-personal data by AI technologies, that the lawful use of copyrighted works and other subject matter and associated data, including pre-existing content, high-quality datasets and metadata, needs to be assessed in the light of the existing rules on limitations and exceptions to copyright protection, such as the text and data mining exception, as provided for by the Directive on copyright and related rights in the Digital Single Market; calls for further clarification as regards the protection of data under copyright law and potential trademark and industrial design protection for works generated autonomously through AI applications; considers that voluntary non-personal data sharing between businesses and sectors should be promoted and based on fair contractual agreements, including licencing agreements; highlights the IPR issues arising from the creation of deep fakes on the basis of misleading, manipulated or simply low-quality data, irrespective of such deep fakes containing data which may be subject to copyright; is worried about the possibility of mass manipulation of citizens being used to destabilise democracies and calls for increased awareness-raising and media literacy as well as for urgently needed AI technologies to be made available to verify facts and information; considers that non-personal auditable records of data used throughout the life cycles of AI-enabled technologies in compliance with data protection rules could facilitate the tracing of the use of copyright-protected works and thereby better protect right-holders and contribute to the protection of privacy, if the requirement to keep auditable records were extended to cover data containing or deriving from images and/or videos containing biometric data; stresses that AI technologies could be useful in the context of IPR enforcement, but would require human review and a guarantee that any AI-driven decision-making systems are fully transparent; stresses that any future AI regime may not circumvent possible requirements for open source technology in public tenders or prevent the interconnectivity of digital services; notes that AI systems are software-based and rely on statistical models, which may include errors; stresses that AI-generated output must not be discriminatory and that one of the most efficient ways of reducing bias in AI systems is to ensure – to the extent possible under Union law – that the maximum amount of non-personal data is available for training purposes and machine learning; calls on the Commission to reflect on the use of public domain data for such purposes;\n",
      "19.   Stresses the importance of full implementation of the Digital Single Market Strategy in order to improve the accessibility and interoperability of non-personal data in the EU; stresses that the European Data Strategy must ensure a balance between promoting the flow of, wider access to and the use and sharing of data on the one hand, and the protection of IPRs and trade secrets on the other, while respecting data protection and privacy rules; highlights the need to assess in that connection whether Union rules on intellectual property are an adequate tool to protect data, including sectoral data needed for the development of AI, recalling that structured data, such as databases, when enjoying IP protection, may not usually be considered to be data; considers that comprehensive information should be provided on the use of data protected by IPRs, in particular in the context of platform-to-business relationships; welcomes the Commission’s intention to create a single European data space;\n",
      "20.   Notes that the Commission is considering the desirability of legislation on issues that have an impact on relationships between economic operators whose purpose is to make use of non-personal data and welcomes a possible revision of the Database Directive and a possible clarification of the application of the directive on the protection of trade secrets as a generic framework; looks forward to the results of the public consultation procedure launched by the Commission on the European Data Strategy;\n",
      "21.   Stresses the need for the Commission to aim to provide balanced and innovation-driven protection of intellectual property, for the benefit of European AI developers, to strengthen the international competitiveness of European companies, including against possible abusive litigation tactics, and to ensure maximum legal certainty for users, notably in international negotiations, in particular as regards the ongoing discussions on AI and data revolution under the auspices of WIPO; welcomes the Commission’s recent submissions of the Union’s views to the WIPO public consultation on the WIPO draft Issues Paper on Intellectual Property Policy and Artificial Intelligence; recalls in this regard the Union’s ethical duty to support development around the world by facilitating cross-border cooperation on AI, including through limitations and exceptions for cross-border research and text and data mining, as provided for by the Directive on copyright and related rights in the Digital Single Market;\n",
      "22.   Is fully aware that progress in AI will have to be paired with public investment in infrastructure, training in digital skills and major improvements in connectivity and interoperability in order to come to full fruition; highlights, therefore, the importance of secure and sustainable 5G networks for the full deployment of AI technologies but, more importantly, of necessary work on the level of infrastructure and security thereof throughout the Union; takes note of the intensive patenting activity taking place in the transport sector when it comes to AI; expresses its concern that this may result in massive litigation that will be detrimental to the industry as a whole and may also affect traffic safety if we do not legislate on the development of AI-related technologies at Union level without further delay;\n",
      "23.   Endorses the Commission’s willingness to invite the key players from the manufacturing sector – transport manufacturers, AI and connectivity innovators, service providers from the tourism sector and other players in the automotive value chain – to agree on the conditions under which they would be ready to share their data;\n",
      "24.   Instructs its President to forward this resolution to the Council and the Commission as well as to the parliaments and the governments of the Member States.\n"
     ]
    }
   ],
   "source": [
    "print(texts[filename][\"dragnet\"][\"doc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d4557dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procedure : 2020/2015(INI) Document stages in plenary Document selected : A9-0176/2020 Texts tabled : A9-0176/2020\n",
      "\n",
      "Debates : PV 19/10/2020 - 18\n",
      "\n",
      "CRE 19/10/2020 - 15\n",
      "\n",
      "CRE 19/10/2020 - 18\n",
      "\n",
      "Votes : PV 20/10/2020 - 17\n",
      "\n",
      "PV 20/10/2020 - 21\n",
      "\n",
      "Texts adopted : P9_TA(2020)0277\n",
      "\n",
      "\n",
      "\n",
      "<Date> {02/10/2020} 2.10.2020 </Date> <NoDocSe> A9-0176/2020 </NoDocSe> 232k 98k <TitreType>REPORT</TitreType> <Titre>on intellectual property rights for the development of artificial intelligence technologies</Titre> <DocRef>(2020/2015(INI))</DocRef>\n",
      "\n",
      "<Commission>{JURI}Committee on Legal Affairs</Commission> Rapporteur: <Depute>Stéphane Séjourné</Depute> MOTION FOR A EUROPEAN PARLIAMENT RESOLUTION EXPLANATORY STATEMENT OPINION OF THE COMMITTEE ON THE INTERNAL MARKET AND CONSUMER PROTECTION OPINION OF THE COMMITTEE ON TRANSPORT AND TOURISM OPINION OF THE COMMITTEE ON CULTURE AND EDUCATION INFORMATION ON ADOPTION IN COMMITTEE RESPONSIBLE FINAL VOTE BY ROLL CALL IN COMMITTEE RESPONSIBLE\n",
      "\n",
      "MOTION FOR A EUROPEAN PARLIAMENT RESOLUTION on intellectual property rights for the development of artificial intelligence technologies (2020/2015(INI)) The European Parliament, – having regard to the Treaty on the Functioning of the European Union (TFEU), in particular Articles 4, 16, 26, 114 and 118 thereof, – having regard to the Berne Convention for the Protection of Literary and Artistic Works, – having regard to the Interinstitutional Agreement of 13 April 2016 on Better Law-Making[1] and the Commission’s Better Regulations Guidelines (COM(2015)0215), – having regard to the World Intellectual Property Organisation (WIPO) Copyright Treaty, the WIPO Performances and Phonograms Treaty and the WIPO revised Issues Paper of 29 May 2020 on Intellectual Property Policy and Artificial Intelligence, – having regard to Directive (EU) 2019/790 of the European Parliament and of the Council of 17 April 2019 on copyright and related rights in the Digital Single Market and amending Directives 96/9/EC and 2001/29/EC[2], – having regard to Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases[3], – having regard to Directive 2009/24/EC of the European Parliament and of the Council of 23 April 2009 on the legal protection of computer programs[4], – having regard to Directive (EU) 2016/943 of the European Parliament and of the Council of 8 June 2016 on the protection of undisclosed know-how and business information (trade secrets) against their unlawful acquisition, use and disclosure[5], – having regard to Directive (EU) 2019/1024 of the European Parliament and of the Council of 20 June 2019 on open data and the re-use of public sector information[6], – having regard to Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC[7], – having regard to Regulation (EU) 2018/1807 of the European Parliament and of the Council of 14 November 2018 on a framework for the free flow of non-personal data in the European Union[8], – having regard to Regulation (EU) 2019/1150 of the European Parliament and of the Council of 20 June 2019 on promoting fairness and transparency for business users of online intermediation services[9], – having regard to the Commission White Paper of 19 February 2020 entitled ‘Artificial Intelligence - A European approach to excellence and trust’ (COM(2020)0065), – having regard to the work of the High-Level Expert Group on Artificial Intelligence set up by the Commission, – having regard to the Commission communications entitled ‘A European Data Strategy’ (COM(2020)0066) and ‘A New Industrial Strategy for Europe’ (COM(2020)0102), – having regard to the Guidelines for Examination in the European Patent Office of November 2019, – having regard to the digital economy working paper 2016/05 of the Commission’s Joint Research Centre and its Institute for Prospective Technological Studies entitled ‘An Economic Policy Perspective on Online Platforms’, – having regard to the political guidelines for the next European Commission 2019-2024 entitled ‘A Union that strives for more: my agenda for Europe’, – having regard to its resolution of 16 February 2017 with recommendations to the Commission on Civil Law Rules on Robotics[10], – having regard to Rule 54 of its Rules of Procedure, – having regard to the opinions of the Committee on the Internal Market and Consumer Protection, the Committee on Transport and Tourism and the Committee on Culture and Education, – having regard to the report of the Committee on Legal Affairs (A9-0176/2020), A. whereas the Union’s legal framework for intellectual property aims to promote innovation, creativity and access to knowledge and information; B. whereas Article 118 of the TFEU stipulates that the Union legislator must establish measures for the creation of European intellectual property rights (IPRs) to provide uniform protection of those rights throughout the Union; whereas the single market is conducive to the stronger economic growth needed to ensure the prosperity of Union citizens; C. whereas recent developments in artificial intelligence (AI) and similar emerging technologies represent a significant technological advance that is generating opportunities and challenges for Union citizens, businesses, public administrations, creators and the defence sector; D. whereas AI technologies may render the traceability of IPRs and their application to AI-generated output difficult, thus preventing human creators whose original work is used to power such technologies from being fairly remunerated; E. whereas the aim of making the Union the world leader in AI technologies must encompass efforts to regain and safeguard the Union’s digital and industrial sovereignty, ensure its competitiveness and promote and protect innovation, and must require a structural reform of the Union’s industrial policy to allow it to be at the forefront of AI technologies while respecting cultural diversity; whereas the Union's global leadership in AI calls for an effective intellectual property system which is fit for the digital age, enabling innovators to bring new products to the market; whereas strong safeguards are crucial to protect the Union’s patent system against abuse, which is detrimental to innovative AI developers; whereas a human-centred approach to AI that is compliant with ethical principles and human rights is needed if the technology is to remain a tool that serves people and the common good; F. whereas the Union is the appropriate level at which to regulate AI technologies in order to avoid fragmentation of the single market and differing national provisions and guidelines; whereas a fully harmonised Union regulatory framework in the field of AI will have the potential to become a legislative benchmark at international level; whereas new common rules for AI systems should take the form of a regulation in order to establish equal standards across the Union and whereas legislation must be future-proofed to ensure it can keep pace with the fast development of this technology, and must be followed up on through thorough impact assessments; whereas legal certainty fosters technological development, and whereas public confidence in new technologies is essential for the development of this sector, as it strengthens the Union’s competitive advantage; whereas the regulatory framework governing AI should therefore inspire confidence in the safety and reliability of AI and strike a balance between public protection and business incentives for investment in innovation; G. whereas AI and related technologies are based on computational models and algorithms, which are regarded as mathematical methods within the meaning of the European Patent Convention (EPC) and are therefore not patentable as such; whereas mathematical methods and computer programs may be protected by patents under Article 52(3) of the EPC when they are used as part of an AI system that contributes to producing a further technical effect; whereas the impact of such potential patent protection should be thoroughly assessed; H. whereas AI and related technologies are based on the creation and execution of computer programs which, as such, are subject to a specific copyright protection regime, whereby only the expression of a computer program may be protected, and not the ideas, methods and principles which underlie any element of it; I. whereas an increasing number of AI-related patents are being granted; J. whereas the development of AI and related technologies raises questions about the protection of innovation itself and the application of IPRs to materials, content or data generated by AI and related technologies, which can be of an industrial or artistic nature and which create various commercial opportunities; whereas in this regard it is important to distinguish between AI-assisted human creations and creations autonomously generated by AI; J. whereas AI and related technologies are heavily dependent on pre-existing content and large volumes of data; whereas increased transparent and open access to certain non-personal data and databases in the Union, especially for SMEs and start-ups, as well as interoperability of data, which limits lock-in effects, will play a crucial role in advancing the development of European AI and supporting the competitiveness of European companies at the global level; whereas the collection of personal data must respect fundamental rights and data protection rules and requires tailored governance, namely in terms of data management and the transparency of data used in developing and deploying AI technologies, and this throughout the entire lifecycle of an AI-enabled system; 1. Takes note of the Commission White Paper on ‘Artificial Intelligence - A European approach to excellence and trust’ and the European Data Strategy; stresses that the approaches outlined therein are likely to contribute to unlocking the potential of human-centred AI in the EU; notes, however, that the issue of the protection of IPRs in the context of the development of AI and related technologies has not been addressed by the Commission, despite the key importance of these rights; highlights the necessity of creating a single European data space and believes that the use thereof will play an important role in innovation and creativity in the Union economy, which should be incentivised; stresses that the Union should play an essential role in laying down basic principles on the development, deployment and use of AI, without hindering its advancement or impeding competition; 2. Highlights the fact that the development of AI and related technologies in the transport and tourism sectors will bring innovation, research, the mobilisation of investment and considerable economic, societal, environmental, public and safety benefits, while making these sectors more attractive to new generations and creating new employment opportunities and more sustainable business models, but stresses that it should not cause harm or damage to people or society; 3. Stresses the importance of creating an operational and fully harmonised regulatory framework in the field of AI technologies; suggests that such a framework should take the form of a regulation rather than a directive in order to avoid fragmentation of the European digital single market and promote innovation; 4 Calls on the Commission to take into account the seven key requirements identified in the Guidelines of the High-Level Expert Group, as welcomed by it in its communication of 8 April 2019[11], and properly implement them in all legislation dealing with AI; 5. Stresses that the development, deployment and use of AI technologies and the growth of the global data economy make it necessary to address significant technical, social, economic, ethical and legal issues in a variety of policy areas, including IPRs and their impact on these policy areas; highlights that in order to unlock the potential of AI technologies, it is necessary to remove unnecessary legal barriers, so as not to hamper the growth of or innovation in the Union’s developing data economy; calls for an impact assessment to be conducted with regards to the protection of IPRs in the context of the development of AI technologies; 6. Stresses the key importance of balanced IPR protection in relation to AI technologies, and of the multidimensional nature of such protection, and, at the same time, stresses the importance of ensuring a high level of protection of IPRs, of creating legal certainty and of building the trust needed to encourage investment in these technologies and ensure their long-term viability and use by consumers; considers that the Union has the potential to become the frontrunner in the creation of AI technologies by adopting an operational regulatory framework that is regularly assessed in the light of technological developments and by implementing proactive public policies, particularly as regards training programmes and financial support for research and public-private sector cooperation; reiterates the need to ensure sufficient leeway for the development of new technologies, products and services; emphasises that creating an environment conducive to creativity and innovation by encouraging the use of AI technologies by creators must not come at the expense of the interests of human creators, nor the Union’s ethical principles; 7. Considers also that the Union must address the various aspects of AI by means of definitions that are technologically neutral and sufficiently flexible to encompass future technological developments as well as subsequent uses; considers it necessary to continue to reflect on interactions between AI and IPRs, from the perspective of both intellectual property offices and users; believes that the challenge of assessing AI applications creates a need for some transparency requirements and the development of new methods as, for instance, adaptive learning systems may recalibrate following each input, making certain ex ante disclosures ineffective; 8. Stresses the importance of streaming services being transparent and responsible in their use of algorithms, so that access to cultural and creative content in various forms and different languages as well as impartial access to European works can be better guaranteed; 9. Considers the increasing need for AI and related technologies in remote or biometric recognition technologies, such as tracing apps in the transport and tourism sector, as a new way of dealing with COVID-19 and possible future sanitary and public health crises, while keeping sight of the need to protect fundamental rights, privacy and personal data; 10. Recommends that priority be given to assessment by sector and type of IPR implications of AI technologies; considers that such an approach should take into account, for example, the degree of human intervention, the autonomy of AI, the importance of the role and the origin of the data and copyright-protected material used and the possible involvement of other relevant factors; recalls that any approach must strike the right balance between the need to protect investments of both resources and effort and the need to incentivise creation and sharing; takes the view that more thorough research is necessary for the purposes of evaluating human input regarding AI algorithmic data; believes that disruptive technologies such as AI offer both small and large companies the opportunity to develop market-leading products; considers that all companies should benefit from equally efficient and effective IPR protection; therefore calls on the Commission and the Member States to offer support to start-ups and SMEs via the Single Market Programme and Digital Innovation Hubs in protecting their products; 11. Suggests that this assessment focus on the impact and implications of AI and related technologies under the current system of patent law, trademark and design protection, copyright and related rights, including the applicability of the legal protection of databases and computer programs, and the protection of undisclosed know-how and business information (‘trade secrets’) against their unlawful acquisition, use and disclosure; acknowledges the potential of AI technologies to improve the enforcement of IPRs, notwithstanding the need for human verification and review, especially where legal consequences are concerned; emphasises, further, the need to assess whether contract law ought to be updated in order to best protect consumers and whether competition rules need to be adapted in order to address market failures and abuses in the digital economy, the need to create a more comprehensive legal framework for the economic sectors in which AI plays a part, thus enabling European companies and relevant stakeholders to scale up, and the need to create legal certainty; stresses that the protection of intellectual property must always be reconciled with other fundamental rights and freedoms; 12. Points out that mathematical methods as such are excluded from patentability unless they are used for a technical purpose in the context of technical inventions, which are themselves patentable only if the applicable criteria relating to inventions are met; points out, further, that if an invention relates either to a method involving technical means or to a technical device, its purpose, considered as a whole, is in fact technical in nature and is therefore not excluded from patentability; underlines, in this regard, the role of the patent protection framework in incentivising AI inventions and promoting their dissemination, as well as the need to create opportunities for European companies and start-ups to foster the development and uptake of AI in Europe; points out that standard essential patents play a key role in the development and dissemination of new AI and related technologies and in ensuring interoperability; calls on the Commission to support the establishment of industry standards and encourage formal standardisation; 13. Notes that patent protection can be granted provided that the invention is new and not self-evident and involves an inventive step; notes, further, that patent law requires a comprehensive description of the underlying technology, which may pose challenges for certain AI technologies in view of the complexity of the reasoning; stresses also the legal challenges of reverse engineering, which is an exception to the copyright protection of computer programs and the protection of trade secrets, which are in turn of crucial importance for innovation and research and which should be duly taken into account in the context of the development of AI technologies; calls on the Commission to assess possibilities for products to be adequately tested, for example in a modular way, without creating risks for IPR holders or trade secrets due to extensive disclosure of easily replicated products; stresses that AI technologies should be openly available for educational and research purposes, such as more effective learning methods; 14. Notes that the autonomisation of the creative process of generating content of an artistic nature can raise issues relating to the ownership of IPRs covering that content; considers, in this connection, that it would not be appropriate to seek to impart legal personality to AI technologies and points out the negative impact of such a possibility on incentives for human creators; 15. Points out the difference between AI-assisted human creations and AI-generated creations, with the latter creating new regulatory challenges for IPR protection, such as questions of ownership, inventorship and appropriate remuneration, as well as issues related to potential market concentration; further considers that IPRs for the development of AI technologies should be distinguished from IPRs potentially granted for creations generated by AI; stresses that where AI is used only as a tool to assist an author in the process of creation, the current IP framework remains applicable; 16. Takes the view that technical creations generated by AI technology must be protected under the IPR legal framework in order to encourage investment in this form of creation and improve legal certainty for citizens, businesses and, since they are among the main users of AI technologies for the time being, inventors; considers that works autonomously produced by artificial agents and robots might not be eligible for copyright protection, in order to observe the principle of originality, which is linked to a natural person, and since the concept of ‘intellectual creation’ addresses the author’s personality; calls on the Commission to support a horizontal, evidence-based and technologically neutral approach to common, uniform copyright provisions applicable to AI-generated works in the Union, if it is considered that such works could be eligible for copyright protection; recommends that ownership of rights, if any, should only be assigned to natural or legal persons that created the work lawfully and only if authorisation has been granted by the copyright holder if copyright-protected material is being used, unless copyright exceptions or limitations apply; stresses the importance of facilitating access to data and data sharing, open standards and open source technology, while encouraging investment and boosting innovation; 17. Notes that AI makes it possible to process a large quantity of data relating to the state of the art or the existence of IPRs; notes, at the same time, that AI or related technologies used for the registration procedure to grant IPRs and for the determination of liability for infringements of IPRs cannot be a substitute for human review carried out on a case-by-case basis, in order to ensure the quality and fairness of decisions; notes that AI is progressively gaining the ability to perform tasks typically carried out by humans and stresses, therefore, the need to establish adequate safeguards, including design systems with human-in-the-loop control and review processes, transparency, accountability and verification of AI decision-making; 18. Notes, with regard to the use of non-personal data by AI technologies, that the lawful use of copyrighted works and other subject matter and associated data, including pre-existing content, high-quality datasets and metadata, needs to be assessed in the light of the existing rules on limitations and exceptions to copyright protection, such as the text and data mining exception, as provided for by the Directive on copyright and related rights in the Digital Single Market; calls for further clarification as regards the protection of data under copyright law and potential trademark and industrial design protection for works generated autonomously through AI applications; considers that voluntary non-personal data sharing between businesses and sectors should be promoted and based on fair contractual agreements, including licencing agreements; highlights the IPR issues arising from the creation of deep fakes on the basis of misleading, manipulated or simply low-quality data, irrespective of such deep fakes containing data which may be subject to copyright; is worried about the possibility of mass manipulation of citizens being used to destabilise democracies and calls for increased awareness-raising and media literacy as well as for urgently needed AI technologies to be made available to verify facts and information; considers that non-personal auditable records of data used throughout the life cycles of AI-enabled technologies in compliance with data protection rules could facilitate the tracing of the use of copyright-protected works and thereby better protect right-holders and contribute to the protection of privacy, if the requirement to keep auditable records were extended to cover data containing or deriving from images and/or videos containing biometric data; stresses that AI technologies could be useful in the context of IPR enforcement, but would require human review and a guarantee that any AI-driven decision-making systems are fully transparent; stresses that any future AI regime may not circumvent possible requirements for open source technology in public tenders or prevent the interconnectivity of digital services; notes that AI systems are software-based and rely on statistical models, which may include errors; stresses that AI-generated output must not be discriminatory and that one of the most efficient ways of reducing bias in AI systems is to ensure – to the extent possible under Union law – that the maximum amount of non-personal data is available for training purposes and machine learning; calls on the Commission to reflect on the use of public domain data for such purposes; 19. Stresses the importance of full implementation of the Digital Single Market Strategy in order to improve the accessibility and interoperability of non-personal data in the EU; stresses that the European Data Strategy must ensure a balance between promoting the flow of, wider access to and the use and sharing of data on the one hand, and the protection of IPRs and trade secrets on the other, while respecting data protection and privacy rules; highlights the need to assess in that connection whether Union rules on intellectual property are an adequate tool to protect data, including sectoral data needed for the development of AI, recalling that structured data, such as databases, when enjoying IP protection, may not usually be considered to be data; considers that comprehensive information should be provided on the use of data protected by IPRs, in particular in the context of platform-to-business relationships; welcomes the Commission’s intention to create a single European data space; 20. Notes that the Commission is considering the desirability of legislation on issues that have an impact on relationships between economic operators whose purpose is to make use of non-personal data and welcomes a possible revision of the Database Directive and a possible clarification of the application of the directive on the protection of trade secrets as a generic framework; looks forward to the results of the public consultation procedure launched by the Commission on the European Data Strategy; 21. Stresses the need for the Commission to aim to provide balanced and innovation-driven protection of intellectual property, for the benefit of European AI developers, to strengthen the international competitiveness of European companies, including against possible abusive litigation tactics, and to ensure maximum legal certainty for users, notably in international negotiations, in particular as regards the ongoing discussions on AI and data revolution under the auspices of WIPO; welcomes the Commission’s recent submissions of the Union’s views to the WIPO public consultation on the WIPO draft Issues Paper on Intellectual Property Policy and Artificial Intelligence; recalls in this regard the Union’s ethical duty to support development around the world by facilitating cross-border cooperation on AI, including through limitations and exceptions for cross-border research and text and data mining, as provided for by the Directive on copyright and related rights in the Digital Single Market; 22. Is fully aware that progress in AI will have to be paired with public investment in infrastructure, training in digital skills and major improvements in connectivity and interoperability in order to come to full fruition; highlights, therefore, the importance of secure and sustainable 5G networks for the full deployment of AI technologies but, more importantly, of necessary work on the level of infrastructure and security thereof throughout the Union; takes note of the intensive patenting activity taking place in the transport sector when it comes to AI; expresses its concern that this may result in massive litigation that will be detrimental to the industry as a whole and may also affect traffic safety if we do not legislate on the development of AI-related technologies at Union level without further delay; 23. Endorses the Commission’s willingness to invite the key players from the manufacturing sector – transport manufacturers, AI and connectivity innovators, service providers from the tourism sector and other players in the automotive value chain – to agree on the conditions under which they would be ready to share their data; 24. Instructs its President to forward this resolution to the Council and the Commission as well as to the parliaments and the governments of the Member States.\n",
      "\n",
      "EXPLANATORY STATEMENT Artificial intelligence (AI) is a field of scientific research whose origins date back to the mid-20th century. The objective is an ambitious one: to understand how the human cognitive system works in order to reproduce it and so create comparable decision-making processes. Some years ago, a new era began in AI, thanks to a combination of vast computing power, much larger numbers of data sets and powerful algorithms. The resulting new impetus is fuelling the development and deployment of AI in many sectors. It is making it possible, for example, to automate the analysis of clinical samples, or to adjust traffic lights in response to road traffic flows without human intervention. The potential of this technology, in terms of innovation, is therefore enormous, and it is important that the European Union adopt an operational legal framework for the development of European AI and public policies that are commensurate with the issues at stake, particularly with reference to the training of people in Europe and financial support for applied and fundamental research. This framework must necessarily include thinking about intellectual property rights (IPRs) in order to encourage and protect innovation and creativity in this area. The definition of AI is still a matter for debate, but legal certainty is likely to stimulate the necessary investment in this area in the EU. A form of legislative flexibility should therefore be promoted in order to take account of the multifaceted reality of AI and create a framework that is future-proof (catering for further technological progress). Upstream, consideration must first be given to assessing patent law in the light of the development of AI. Patents protect technical inventions, i.e. products that provide a new technical solution to a given technical problem. Thus, although algorithms, mathematical methods and computer programs are not patentable as such, they may form part of a technical invention that can be patented. It is crucial for the deployment of European AI that economic operators, in particular European start-ups, are aware of this opportunity. Patent applications registered by the European Patent Office for inventions directly related to the operation of AI (core AI technologies) have more than tripled in a decade: from 396 in 2010 to 1 264 in 2017. However, it should be noted that more applications are being submitted in some third countries and that international competition in this strategic area is strong. AI is also used by patent offices to facilitate research into the state of the art. In that connection, it seems important to point out that the technology provides useful assistance, but should not replace analysis by a human examiner as a basis for granting rights. In the field of patents, it must also be pointed out that the complexity of the reasoning used by certain AI technologies may increase the difficulty of checking that these inventions comply with existing rules. Downstream, the growing autonomisation of certain decision-making processes can give rise to technical or artistic creations. Assessing all IPRs in the light of these developments must be a priority for this area of EU law, in order to foster an environment conducive to creativity and innovation by rewarding creators. The role of human intervention remains fundamental to the programming of AI devices, the selection of input data and the application of the results obtained. The prospect of a ‘strong’ AI, that is to say one that is conscious of itself, seems after all still to be very futuristic. As regards copyright, the condition of originality, which imprints on the work the personality of its author, could constitute an obstacle to the protection of AI-generated creations. However, the general trend with regard to that condition is towards an objective concept of relative novelty, making it possible to distinguish a protected work from works already created. AI-generated creation and ‘traditional’ creation still have in common the aim of expanding cultural heritage, even if the creation takes place by means of a different act. At a time when artistic creation by AI is becoming more common (one example being the ‘Next Rembrandt’ painting[12] generated after 346 works by the painter were digitised so that they could be processed using AI), we seem to be moving towards an acknowledgement that an AI-generated creation could be deemed to constitute a work of art on the basis of the creative result rather than the creative process. It should also be noted that a failure to protect AI-generated creations could leave the interpreters of such creations without rights, as the protection afforded by the system of related rights implies the existence of copyright on the work being interpreted. Therefore, it is proposed that an assessment should be undertaken of the advisability of granting copyright to such a ‘creative work’ to the natural person who prepares and publishes it lawfully, provided that the designer(s) of the underlying technology has/have not opposed such use. This reasoning would be in line with the European system of protection of ‘works data’; such data may be exploited as part of the data used to train AI technologies which can then generate secondary creations, including for commercial purposes, provided that the right to such use has not been expressly reserved by their rightholders. Lastly, given the essential role of data and its selection in the development of AI technologies, a number of questions arise concerning the accessibility of such data, in particular dependence on data, lock-in effects, the dominant position of certain undertakings and, in general, insufficient data flow. It will therefore be important to encourage the sharing of data generated in the European Union in order to stimulate innovations in artificial intelligence. In the short term, this may in particular be based on the transposition of the Open Data Directive and promotion of the conclusion of licensing agreements to encourage the sharing of industrial data. In the medium term, the Commission’s forthcoming proposal on the generic legislative framework for the governance of common European data areas will be decisive, in particular for access to sensitive databases such as those in the field of health.\n",
      "\n",
      "OPINION OF THE COMMITTEE ON THE INTERNAL MARKET AND CONSUMER PROTECTION ( 9.7.2020 )\n",
      "\n",
      "<CommissionInt>for the Committee on Legal Affairs</CommissionInt>\n",
      "\n",
      "<Titre>on Intellectual property rights for the development of artificial intelligence technologies</Titre> <DocRef>(2020/2015(INI))</DocRef> Rapporteur for opinion: <Depute>Adam Bielan</Depute> SUGGESTIONS The Committee on the Internal Market and Consumer Protection calls on the Committee on Legal Affairs, as the committee responsible, to incorporate the following suggestions into its motion for a resolution: 1. Recalls the potential that artificial intelligence (AI) has when it comes to delivering innovative services to businesses, consumers and the public sector; stresses the key role that AI technologies can play in the digitisation of the economy in many sectors, such as industry, healthcare, construction and transport, which can lead to the establishment of new business models; highlights that the Union must actively embrace developments in this area to advance the digital single market; underlines that the development and use of AI in the internal market will benefit from a reliable, balanced and effective system of intellectual property rights (IPRs); notes the importance of differentiating between AI applications or algorithms, AI-generated technology and products, data bases and individual data, which require different forms of rights; 2. Believes that disruptive technologies such as AI offer both small and large companies the opportunity to develop market-leading products; considers that all companies or other owners of such products should benefit from equally efficient and effective IPR protection; considers that this may foster the emergence of European small and medium-sized enterprises (SMEs) and result in a significant competitive advantage in the Union; calls for an analysis of the impact of abusive practices by ‘patent trolls’ and strategic IPR litigation, which can act as an artificial barrier to entry and protect market incumbents; underlines the importance of AI technologies when it comes to enabling a more transparent, efficient and reliable management of IP-related aspects of transactions; 3. Stresses the importance of measures and information channels that help SMEs and start-ups to effectively use IPR protection in AI technologies; calls on the Commission and the Member States to offer support to start-ups and SMEs via the Single Market Programme and Digital Innovation Hubs to develop and protect their products and thus enable them to fully develop their potential for growth and jobs in Europe; stresses the importance of the Commission and the Member States seeking coordination with other important global players in IPR for the development of AI, so as to create a globally compatible approach that would be beneficial for both SMEs and start-ups; 4. Stresses the importance of protecting IPRs, including trade secrets, in any regulatory framework for AI, in particular as regards any detailed requirements for the narrow set of applications deemed ‘high-risk’, while recognising the need to reconcile these with the application of other public policy objectives, including respect for fundamental rights or freedoms; believes that in order to ensure the development of human-centric, trusted AI, effective implementation of the legislation concerning whistle-blowers is needed; 5. Stresses that in addition to protecting IPRs, it is in the interest of consumers to have legal certainty about allowed uses of protected works, especially when it comes to complicated algorithmic products; calls for the Commission to propose measures for data traceability, while taking into account both the legality of data acquisition and the protection of consumer and fundamental rights; 6. Believes that the challenge of assessing AI applications requires the development of new methods and proper administrative capacity for the market surveillance authorities; notes that adaptive learning systems may recalibrate following each input, making certain ex ante disclosures alone ineffective; 7. Considers that where AI applications are certified, they should demonstrate transparency, explainability – as much as feasibly possible – and compliance with ethical standards; notes that this goal cannot be solely achieved through the simple disclosure of the algorithm or code, if at all; recalls that data sets are also important in this process; 8. Calls on the Commission to consider how to assess ways that allow for products to be tested, for instance in a modular way or with the use of verification tools that would allow products to be adequately tested while observing confidentiality in order to protect the commercial secrets held by IPR holders.\n",
      "\n",
      "INFORMATION ON ADOPTION IN COMMITTEE ASKED FOR OPINION Date adopted 7.7.2020 Result of final vote +: –: 0: 43 0 1 Members present for the final vote Alex Agius Saliba, Andrus Ansip, Brando Benifei, Adam Bielan, Hynek Blaško, Biljana Borzan, Vlad-Marius Botoş, Markus Buchheit, Dita Charanzová, Deirdre Clune, David Cormand, Petra De Sutter, Carlo Fidanza, Evelyne Gebhardt, Alexandra Geese, Sandro Gozi, Maria Grapini, Svenja Hahn, Virginie Joron, Eugen Jurzyca, Arba Kokalari, Marcel Kolaja, Kateřina Konečná, Andrey Kovatchev, Jean-Lin Lacapelle, Maria-Manuel Leitão-Marques, Adriana Maldonado López, Antonius Manders, Beata Mazurek, Leszek Miller, Kris Peeters, Anne-Sophie Pelletier, Christel Schaldemose, Andreas Schwab, Tomislav Sokol, Ivan Štefanec, Kim Van Sparrentak, Marion Walsmann, Marco Zullo Substitutes present for the final vote Pascal Arimont, Marco Campomenosi, Maria da Graça Carvalho, Edina Tóth, Stéphanie Yon-Courtin\n",
      "\n",
      "FINAL VOTE BY ROLL CALL IN COMMITTEE ASKED FOR OPINION 43 + ECR Adam Bielan, Carlo Fidanza, Eugen Jurzyca, Beata Mazurek EPP Pascal Arimont, Maria da Graça Carvalho, Deirdre Clune, Arba Kokalari, Andrey Kovatchev, Antonius Manders, Kris Peeters, Andreas Schwab, Tomislav Sokol, Ivan Štefanec, Edina Tóth, Marion Walsmann EUL/NGL Kateřina Konečná, Anne‑Sophie Pelletier GREENS/EFA David Cormand, Petra De Sutter, Alexandra Geese, Marcel Kolaja, Kim Van Sparrentak, ID Markus Buchheit, Marco Campomenosi, Virginie Joron, Jean‑Lin Lacapelle NI Marco Zullo RENEW Andrus Ansip, Vlad‑Marius Botoş, Dita Charanzová, Sandro Gozi, Svenja Hahn, Stéphanie Yon‑Courtin S&D Alex Agius Saliba, Brando Benifei, Biljana Borzan, Evelyne Gebhardt, Maria Grapini, Maria‑Manuel Leitão‑Marques, Adriana Maldonado López, Leszek Miller, Christel Schaldemose 0 - 1 0 ID Hynek Blaško Key to symbols: + : in favour - : against 0 : abstention\n",
      "\n",
      "OPINION OF THE COMMITTEE ON TRANSPORT AND TOURISM ( 14.7.2020 )\n",
      "\n",
      "<CommissionInt>for the Committee on Legal Affairs</CommissionInt>\n",
      "\n",
      "<Titre>on intellectual property rights for the development of artificial intelligence technologies</Titre> <DocRef>(2020/2015(INI))</DocRef> Rapporteur for opinion: <Depute>Andor Deli</Depute> SUGGESTIONS The Committee on Transport and Tourism calls on the Committee on Legal Affairs, as the committee responsible, to incorporate the following suggestions into its motion for a resolution: Introduction 1. Welcomes the ambitions affirmed by the Commission in its communications of 19 February 2020, as well as in its White Paper on ‘Artificial Intelligence – A European approach to excellence and trust’ and in the European Data Strategy, in the area of artificial intelligence (AI) and data; notes, however, that the issue of the protection of intellectual property rights (IPRs) in the context of the development of AI and related technologies has to be taken more seriously; 2. Stresses that the development and deployment of AI and related technologies make it necessary to address technical, social, economic, ethical and legal issues and cross-sectoral implications in a variety of policy areas, including IPRs, and to provide answers and formulate policies at the European level; 3. Highlights the fact that the development of AI and related technologies in the transport and tourism sectors will bring innovation, research, the mobilisation of investment and considerable economic, societal, environmental, public and safety benefits, while making the sector more attractive to new generations and creating new employment opportunities and more sustainable business models, but should not cause harm or damage to people or society; 4. Takes note of the global competition between companies and economic regions in the development of AI solutions for the transport sector; highlights the need to strengthen the international competitiveness of European companies operating in the transport sector by establishing the EU as an environment favourable for the development and application of AI solutions; underlines furthermore that AI should also be deployed in all modes of transport, in both urban and rural areas, and that a holistic, technologically neutral and flexible approach is therefore needed to tackle adequately all the challenges in the transport and mobility sector; 5. Affirms that defining the appropriate legal framework at EU level for IPRs for AI and connectivity innovations, as well as for access to and security of data will be key in the development and smooth, safe and wide dissemination of AI and related technologies in transport and tourism ecosystems; 6. Considers that intellectual property (IP) protection strategies will constantly evolve over time as AI evolves, and that it will be necessary to take account of issues such as adapting to this changing environment with flexible copyright, patent, trademark and design protection or even trade secret rules, and to consider what route will provide innovators with the broadest and most robust means of IP protection that combine legal certainty and encourage new investment in private enterprises, universities, SMEs and clusters using public-private collaboration to support research and development; 7. Calls on the Commission to take into account the seven key requirements identified in the Guidelines of the High-Level Expert Group, as welcomed by it in its communication of 8 April 2019[13], and properly implement them in all legislation dealing with AI; 8. Considers the increasing need for AI and related technologies in remote or biometric recognition technologies, such as tracing apps in the transport and tourism sector, as a new way of dealing with COVID-19 and possible future sanitary and public health crises, while keeping sight of the need to protect fundamental rights, privacy and personal data; IP rights and AI innovations 9. Notes that the current fragmented legal framework of IP rights and legal uncertainty affect the development of AI and related technologies in transport; calls on the Commission, therefore, to evaluate the fitness of its intellectual property regime for the development of AI technologies and, after a thorough analysis and review of the current legislation, to put forward the legislative proposals it finds necessary in order to ensure confidence, legal certainty and transparency and avoid further fragmentation, thus encouraging investment in these technologies; 10. Notes that although AI makes it possible to process a large quantity of data relating to IPRs, it cannot be a substitute for human verification in relation to the granting of IPRs and the determination of liability for infringements of IPRs; 11. Notes, with regard to the use of data by AI, that the use of copyrighted data needs to be assessed in the light of the text and data mining exceptions provided for by the Directive on copyright and related rights in the Digital Single Market, and in the light of all uses covered by limitations and exceptions to IPR protection; 12. Calls on the Commission to evaluate the possibility and relevance for companies, including SMEs, of obtaining patents for software or algorithms with a view to ensuring both the protection of innovation and the need for transparency required for trustworthy AI, as well as the availability of algorithms used for public purposes; stresses the need to maintain a level playing field between these companies, as well as the importance of remaining consistent with competition law; 13. Is fully aware that progress in AI will have to be paired with public investment in infrastructure, training in digital skills and major improvements in connectivity and interoperability in order to come to full fruition; highlights, therefore, the importance of secure and sustainable 5G networks for the full deployment of AI technologies but, more importantly, necessary work on the level of infrastructure and security thereof throughout the Union; takes note of the intensive patenting activity taking place in the transport sector when it comes to AI; expresses its concern that this may result in massive litigation that will be detrimental to the industry as a whole and may also affect traffic safety if we do not legislate the development of AI-related technologies at European level without further delay; 14. Points out that standard essential patents (SEPs) play a key role in the development and dissemination of new AI and related technologies and ensuring interoperability; calls on the Commission to encourage the emergence of cross-industry standards and formal standardisation; recalls in this regard the Commission’s communication of 29 November 2017 on SEP licensing and the key principles it set out for transparency in SEPs, namely fair, reasonable and non-discriminatory (FRAND) licensing and enforcement; draws particular attention to SEPs that can improve accessibility, road safety and security for transport users; Intellectual property rights and data 15. Welcomes the Commission’s willingness to ensure that data will be collected and used in full compliance with the EU’s General Data Protection Regulation and other strict data protection rules; stresses the need to continue to safeguard the data of European citizens, but considers a right balance between data protection and IP rules is needed in order to grant necessary flexibility to AI innovators; 16. Welcomes the Commission’s aim of creating a single European data space with investment in standards, tools and infrastructure; supports in particular the establishment of a common European mobility data space, taking into consideration the existing European legislative framework on data protection; 17. Calls on the Commission to address adequately and urgently the question of, and legislative proposals relating to, data and intellectual property protection with fair and appropriate flexibility and in compliance with the principle of technological neutrality, also by developing initiatives for the exchange of best practices and investing in research in this field; 18. Welcomes the future establishment of an enabling and flexible legislative framework for the governance of common European data spaces, as well as the Commission’s willingness to foster business-to-government and business-to-business data sharing and to limit mandatory access to data under FRAND conditions to the cases where specific circumstances so dictate; highlights the importance of access to vehicle generated data for all mobility stakeholders in order to promote the development of innovative data-driven services; 19. Calls on the Commission to pay special attention to access for SMEs and clusters to data that could boost their activity, as well as to technology centres and universities to promote their research programmes; 20. Endorses the Commission’s willingness to invite the key players from the manufacturing sector - transport manufacturers, AI and connectivity innovators, service providers from the tourism sector and other players in the automotive value chain - to agree on the conditions under which they would be ready to share their data.\n",
      "\n",
      "INFORMATION ON ADOPTION IN COMMITTEE ASKED FOR OPINION Date adopted 14.7.2020 Result of final vote +: –: 0: 41 2 6 Members present for the final vote Magdalena Adamowicz, Andris Ameriks, José Ramón Bauzá Díaz, Izaskun Bilbao Barandica, Marco Campomenosi, Ciarán Cuffe, Jakop G. Dalunde, Johan Danielsson, Andor Deli, Karima Delli, Anna Deparnay-Grunenberg, Ismail Ertug, Gheorghe Falcă, Giuseppe Ferrandino, Mario Furore, Søren Gade, Isabel García Muñoz, Jens Gieseke, Elsi Katainen, Kateřina Konečná, Elena Kountoura, Julie Lechanteux, Bogusław Liberadzki, Benoît Lutgen, Elżbieta Katarzyna Łukacijewska, Marian-Jean Marinescu, Tilly Metz, Giuseppe Milazzo, Cláudia Monteiro de Aguiar, Caroline Nagtegaal, Jan-Christoph Oetjen, Philippe Olivier, Rovana Plumb, Dominique Riquet, Dorien Rookmaker, Massimiliano Salini, Barbara Thaler, István Ujhelyi, Elissavet Vozemberg-Vrionidi, Lucia Vuolo, Roberts Zīle, Kosma Złotowski Substitutes present for the final vote Leila Chaibi, Angel Dzhambazki, Markus Ferber, Carlo Fidanza, Maria Grapini, Roman Haider, Alessandra Moretti\n",
      "\n",
      "FINAL VOTE BY ROLL CALL IN COMMITTEE ASKED FOR OPINION 41 + ECR Angel Dzhambazki, Carlo Fidanza, Roberts Zīle, Kosma Złotowski NI Mario Furore, Dorien Rookmaker PPE Magdalena Adamowicz, Andor Deli, Gheorghe Falcă, Markus Ferber, Jens Gieseke, Elżbieta Katarzyna Łukacijewska, Benoît Lutgen, Marian‑Jean Marinescu, Giuseppe Milazzo, Cláudia Monteiro de Aguiar, Massimiliano Salini, Barbara Thaler, Elissavet Vozemberg‑Vrionidi RENEW José Ramón Bauzá Díaz, Izaskun Bilbao Barandica, Søren Gade, Elsi Katainen, Caroline Nagtegaal, Jan‑Christoph Oetjen, Dominique Riquet S&D Andris Ameriks, Johan Danielsson, Ismail Ertug, Giuseppe Ferrandino, Isabel García Muñoz, Maria Grapini, Bogusław Liberadzki, Alessandra Moretti, Rovana Plumb, István Ujhelyi VERTS/ALE Ciarán Cuffe, Jakop G. Dalunde, Karima Delli, Anna Deparnay‑Grunenberg, Tilly Metz 2 - GUE/NGL Leila Chaibi, Kateřina Konečná 6 0 GUE/NGL Elena Kountoura ID Marco Campomenosi, Roman Haider, Julie Lechanteux, Philippe Olivier, Lucia Vuolo Key to symbols: + : in favour - : against 0 : abstention\n",
      "\n",
      "OPINION OF THE COMMITTEE ON CULTURE AND EDUCATION ( 3.9.2020 )\n",
      "\n",
      "<CommissionInt>for the Committee on Legal Affairs</CommissionInt>\n",
      "\n",
      "<Titre>on intellectual property rights for the development of artificial intelligence technologies</Titre> <DocRef>(2020/2015(INI))</DocRef> Rapporteur: <Depute>Sabine Verheyen</Depute>\n",
      "\n",
      "SUGGESTIONS The Committee on Culture and Education calls on the Committee on Legal Affairs, as the committee responsible, to incorporate the following suggestions into its motion for a resolution: 1. Recalls that artificial intelligence (AI) and related technologies, more broadly, should be at the service of humanity and that their benefits should be widely shared, without any discrimination; stresses that, as AI is an ever-changing collection of technologies that are being developed at great speed, and as it is progressively gaining the ability to perform more tasks typically carried out by humans, it may even surpass human intellectual capacity in some areas in the long term; stresses the need, therefore, to establish adequate safeguards including, when reasonable, design systems with human-in-the-loop control and review processes, transparency and verification of AI decision-making; recognises that in the cultural and creative sectors, creators already make extensive use of new AI technologies to produce their artistic works; 2. Stresses that the Union should play an essential role in laying down basic principles on the development, deployment, programming and use of AI, without hindering its advancement or impeding competition, notably in Union regulations and codes of conduct; recalls that Directive (EU) 2019/790 provides a legal framework for the use of copyright protected works in text and data mining (TDM) processes, which are key in any AI-related process; emphasises, therefore, the requirement that any work used must be accessed lawfully, as well as the guaranteed right of rights holders to pre-emptively opt out their works from being used in an AI-related process without their authorisation; also stresses the need for an ethical framework and strategy for digital data, accompanied, if necessary, by legislation in which fundamental rights and Union values are enshrined; 3. Underlines the importance of using AI in schools and universities, enabling them to adopt new and more efficient learning methods that will increase pupils’ and students’ success rates; stresses the importance of promoting AI curricula designed to help pupils and students to acquire the know-how needed for future jobs; stresses that AI technologies should be openly available for educational and research purposes; 4. Stresses that open and equal access to AI across the Union and within the Member States is of upmost importance; stresses that Union support for AI innovation and research should be widely available across the Union; highlights that special support should be given to AI developers and beneficiaries from disadvantaged groups and those with disabilities; 5. Considers that guidance and counselling for AI developers and users on protecting IPR should be widely available; 6. Recalls that AI cannot only perform activities which used to be exclusively human, but can also acquire and develop autonomous and cognitive features through experience learning or reinforcement learning; stresses the notion of responsibility with regard to AI systems capable of learning through reinforcement; stresses that trained AI systems can quasi-autonomously create and generate cultural and creative works, with only minimum human input; notes, moreover, that AI systems can evolve in an unpredictable way, by creating original works unknown even to their initial programmers, a fact that should also be taken into account when establishing a framework for the protection of the exploitation rights derived from such works; reiterates, nevertheless, that AI should assist and not replace the creative human mind; 7. Takes note that AI systems are software-based and display intelligent behaviour based on an analysis of their environment; highlights that this analysis is based on statistical models of which errors form an inevitable part, sometimes with feedback loops that replicate, reinforce and prolong pre-existing biases, errors and assumptions; notes the need to ensure that systems and methods are in place to allow algorithms to be verified and explained and any problems to be resolved; 8. Considers that IPR for the development of AI technologies should be distinguished from IPR for content generated by AI; stresses the need to remove unnecessary legal barriers to AI development in order to unlock the potential of such technologies in culture and education; 9. Emphasises the need to address copyright issues relating to AI-generated cultural and creative works; underlines that creation by human beings as authors and producers of works must form the basis of the IPR system; notes, furthermore, that the question of the extent to which a work created by AI can be traced back to a human creator is of key importance; draws attention to the need to assess whether there is such a thing as an ‘original creation’ that does not require any human intervention; considers that thorough research is needed to understand whether automatically assigning the copyright of AI-generated works to the copyright holder of the AI software, algorithm or programme is the best way forward, as there is a need for a human to be credited as the author of a new creative work; welcomes the Commission’s call for a study on copyright and new technologies; 10. Expresses concern about the potential vacuum between IPR and the development of AI, which could make the cultural and creative sectors and education vulnerable to AI-generated copyright-protected works; is concerned about possible infringements of intellectual property and stresses the need to monitor any market failures or damage that occur; calls on the Commission to support a horizontal, evidence-based and technologically neutral approach to common, uniform copyright provisions applicable to AI-generated works in the Union, which would increase their growth and also attract private sector investment in the technological and economic development of the AI and robotics sector; 11. Notes the development of AI capacities in the dissemination of misinformation and the creation of disinformation; is concerned that this could lead to many breaches of intellectual property legislation, and is, furthermore, extremely worried about the possibility of mass manipulation of citizens being used to destabilise democracies; calls in this regard for action to increase information and media literacy, taking account of the fact that digital transformation is an indispensable aspect thereof; calls for the development of software to verify facts and information to be made a priority; 12. Recalls that data is the central element of the development and training of any AI system; stresses that this includes structured data, such as databases, copyright-protected works and other creations enjoying IP protection which may not usually be considered to be data; stresses therefore that it is also important to address the notion of IP-relevant uses relating to the functioning of AI technologies; 13. Points out that the most efficient way of reducing bias in AI systems is to ensure that the maximum amount of data is available to train them, for which it is necessary to limit any unnecessary barriers to TDM and to facilitate cross-border uses; 14. Stresses that where AI is used only as a tool to assist an author in the process of creation, the current copyright framework remains applicable to the work created and the intervention of AI is not taken into consideration; 15. Recommends that special security features and rules be introduced in order to protect privacy rights related to AI technologies; stresses that privacy auditing of AI technologies should be compulsory; 16. Further recalls that the Union copyright reform introduced a TDM exception according to which scientific research may benefit from free data uses, and that TDM carried out for other purposes will also be allowed under the new exception if further requirements are met; 17. Emphasises that AI can also be an effective tool for detecting and reporting the presence of copyright-protected content online; also emphasises the need to address the issue of liability for copyright and other intellectual property infringements by AI systems, as well as the issue of data ownership; stresses, however, that a clear distinction has to be made between autonomous infringements and the copying of third party works that were facilitated or not prevented by the operator of the AI software; states that traceability should be an indispensable condition in allocating responsibility, as it acts both as a basis for legal action and enables the diagnosis and correction of malfunctions; 18. Stresses the importance of streaming services being transparent and responsible in their use of algorithms, so that access to cultural and creative content in various forms and different languages as well as impartial access to European works can be better guaranteed; 19. Recalls the Union’s ethical duty to support development around the world by facilitating cross-border cooperation on AI, including through limitations and exceptions for cross-border research and TDM, and therefore urges the speeding up of international action at the World Intellectual Property Organization to achieve this; 20. Recognises that due to the technological advancement of certain states, the Union has a fundamental obligation to promote the sharing of the benefits of AI, utilising a number of tools, including investment in research in all Member States.\n",
      "\n",
      "INFORMATION ON ADOPTION IN COMMITTEE ASKED FOR OPINION Date adopted 1.9.2020 Result of final vote +: –: 0: 28 1 1 Members present for the final vote Isabella Adinolfi, Christine Anderson, Ilana Cicurel, Gilbert Collard, Gianantonio Da Re, Laurence Farreng, Tomasz Frankowski, Romeo Franz, Hannes Heide, Irena Joveva, Petra Kammerevert, Niyazi Kizilyürek, Predrag Fred Matić, Dace Melbārde, Victor Negrescu, Peter Pollák, Marcos Ros Sempere, Andrey Slabakov, Massimiliano Smeriglio, Michaela Šojdrová, Sabine Verheyen, Salima Yenbou, Milan Zver Substitutes present for the final vote Isabel Benjumea Benjumea, Christian Ehler, Ibán García Del Blanco, Bernard Guetta, Marcel Kolaja, Elżbieta Kruk, Martina Michels\n",
      "\n",
      "FINAL VOTE BY ROLL CALL IN COMMITTEE ASKED FOR OPINION 28 + PPE Isabel Benjumea Benjumea, Christian Ehler, Tomasz Frankowski, Peter Pollák, Michaela Šojdrová, Sabine Verheyen, Milan Zver S&D Ibán García del Blanco, Hannes Heide, Petra Kammerevert, Predrag Fred Matić, Victor Negrescu, Marcos Ros Sempere, Massimiliano Smeriglio RENEW Ilana Cicurel, Laurence Farreng, Bernard Guetta, Irena Joveva ID Gilbert Collard VERTS/ALE Romeo Franz, Marcel Kolaja, Salima Yenbou ECR Elżbieta Kruk, Dace Melbārde, Andrey Slabakov GUE/NGL Niyazi Kizilyürek, Martina Michels NI Isabella Adinolfi 1 - ID Christine Anderson 1 0 ID Gianantonio Da Re Key to symbols: + : in favour - : against 0 : abstention\n",
      "\n",
      "INFORMATION ON ADOPTION IN COMMITTEE RESPONSIBLE Date adopted 1.10.2020 Result of final vote +: –: 0: 19 3 2 Members present for the final vote Manon Aubry, Gunnar Beck, Geoffroy Didier, Angel Dzhambazki, Ibán García Del Blanco, Jean-Paul Garraud, Esteban González Pons, Mislav Kolakušić, Gilles Lebreton, Karen Melchior, Jiří Pospíšil, Franco Roberti, Marcos Ros Sempere, Liesje Schreinemacher, Stéphane Séjourné, Raffaele Stancanelli, József Szájer, Marie Toussaint, Adrián Vázquez Lázara, Axel Voss, Tiemo Wölken, Javier Zarzalejos Substitutes present for the final vote Patrick Breyer, Evelyne Gebhardt\n",
      "\n",
      "FINAL VOTE BY ROLL CALL IN COMMITTEE RESPONSIBLE 19 + EPP Geoffroy Didier, Esteban González Pons, Jiří Pospíšil, József Szájer, Axel Voss, Javier Zarzalejos S&D Ibán García Del Blanco, Evelyne Gebhardt, Franco Roberti, Marcos Ros Sempere, Tiemo Wölken RENEW Liesje Schreinemacher, Stéphane Séjourné, Adrián Vázquez Lázara ID Jean‑Paul Garraud, Gilles Lebreton ECR Angel Dzhambazki, Raffaele Stancanelli NI Mislav Kolakušić 3 - VERTS/ALE Patrick Breyer, Marie Toussaint GUE/NGL Manon Aubry 2 0 RENEW Karen Melchior ID Gunnar Beck Key to symbols: + : in favour - : against 0 : abstention [1] OJ L 123, 12.5.2016, p. 1. [2] OJ L 130, 17.5.2019, p. 92. [3] OJ L 77, 27.3.1996, p. 20. [4] OJ L 111, 5.5.2009, p. 16. [5] OJ L 157, 15.6.2016, p. 1. [6] OJ L 172, 26.6.2019, p. 56. [7] OJ L 119, 4.5.2016, p. 1. [8] OJ L 303, 28.11.2018, p. 59. [9] OJ L 186, 11.7.2019, p. 57. [10] OJ C 252, 18.7.2018, p. 239. [11] ‘Building trust in human-centric artificial intelligence’ (COM(2019)0168). [12] https://www.nextrembrandt.com/ [13] ‘Building trust in human-centric artificial intelligence’ (COM(2019)0168).\n"
     ]
    }
   ],
   "source": [
    "print(texts[filename][\"newspaper3k\"][\"doc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "054a69a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.europarl.europa.eu/doceo/document/A-9-2020-0176_EN.html\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "title          RESOLUTION ON INTELLECTUAL PROPERTY RIGHTS FOR...\n",
       "country                                           European Union\n",
       "documentUrl    https://www.europarl.europa.eu/doceo/document/...\n",
       "startDate                                                 2020.0\n",
       "endDate                                                      NaN\n",
       "oecdId                                                     27011\n",
       "Name: 543, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = oecd.loc[int(filename[:-5])]\n",
    "print(tmp[\"documentUrl\"])\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c873b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3a900dd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.html\n",
      "word diff 5743\n",
      "newspaper3k: 17227, dragnet 22970\n"
     ]
    }
   ],
   "source": [
    "filename = s[-5][0]\n",
    "print(filename)\n",
    "print(\"word diff %s\" % lens[filename])\n",
    "print(\"newspaper3k: %s, dragnet %s\" %(texts[filename][\"newspaper3k\"][\"len\"], texts[filename][\"dragnet\"][\"len\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d494d8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date introduced:  31 July 2019 House:  House of Representatives Portfolio:  Home Affairs and Foreign Affairs and Trade Commencement:  Both Bills commence the day after Royal Assent.\n",
      "Links: The links to the Bills, their Explanatory Memorandum and second reading speech can be found on the Bill’s home page for the Identity-matching Services Bill 2019 and Australian Passports Amendment (Identity-matching Services) Bill 2019 , or through the Australian Parliament website .\n",
      "When Bills have been passed and have received Royal Assent, they become Acts, which can be found at the Federal Register of Legislation website .\n",
      "All hyperlinks in this Bills Digest are correct as at August 2019.\n",
      "The Bills Digest at a glance\n",
      "Purpose of the Bills The Identity-matching Services Bill 2019 (IMS Bill) authorises the Department of Home Affairs (DOHA) to create and maintain facilities for the sharing of facial images and other identity information between government agencies, and in some cases, private organisations. It provides a legislative basis for certain measures contained in the Intergovernmental Agreement on Identity Matching Services (IGA), agreed to by COAG leaders on 5 October 2017. This agreement aims to facilitate the ‘secure, automated and accountable’ exchange of identity information to help prevent identity crime and promote a range of law enforcement, community safety and service delivery objectives. The Australian Passports Amendment (Identity-matching Services) Bill 2019 (Passports Bill) authorises the Department of Foreign Affairs and Trade to disclose information in order to participate in identity-matching services and provides for computerised decision-making. Both Bills were introduced in the same form during the 45th Parliament, but were not debated before the dissolution of the House of Representatives in April 2019.\n",
      "How the IMS Bill works The IMS Bill authorises DOHA to develop, operate and maintain two centralised facilities for the provision of identity-matching services: an ‘interoperability hub’, intended to operate as a router through which participating agencies and organisations can request and transmit information and the National Driver Licence Facial Recognition Service (NDLRFS), a federated database of information contained in government identity documents such as driver licences. The Bill specifies identity-matching services which will operate through the hub. This includes the Face Verification Service (FVS), which allows users to verify a specific person’s identity, and the Face Identification Service (FIS), which involves the electronic matching of a facial image with the images of one or more people, in order to identify a person. Private sector entities and local government authorities may have access to the FVS. The Bill does not authorise certain agencies to use identity-matching services—entities seeking access will need a legal basis for collecting and disclosing personal information, and must meet access requirements set out in the IGA. The Bill creates an offence for entrusted persons to record or disclose protected information in connection with these services, and sets out circumstances where disclosure will be authorised. The Minister for Home Affairs will be required to report annually to Parliament about the use of the services. A statutory review is to be started within five years of the Act’s commencement.\n",
      "Key issues The Bills are currently being reviewed by the Parliamentary Joint Committee on Intelligence and Security (PJCIS). The Committee previously commenced an inquiry into the 2018 versions of the Bills, but the inquiry lapsed at the dissolution of the House of Representatives in April 2019. In relation to the 2018 Bills, the Parliamentary Joint Committee for Human Rights, Senate Standing Committee for the Scrutiny of Bills and submissions to the PJCIS inquiry raised concerns that the broad scope of the IMS Bill may enable substantial infringements on privacy rights, allowing disclosure of personal information for an extremely wide range of purposes. Stakeholders suggested the IMS Bill provides inadequate protection against misuse of this information, and queried why it does not include key safeguards contained in the IGA, such as access criteria and limitations on the amount of information released by the identity-matching systems. Another area of concern is private sector access, with submissions questioning whether this is appropriate, and arguing that there are insufficient safeguards in the Bill at present. Stakeholders also raised concerns with the computerised decision-making provision in the Passports Bill, suggesting that it is too broad and should be refined.\n",
      "History of the Bill\n",
      "The Identity-matching Services Bill 2018 (IMS Bill 2018) and Australian Passports Amendment (Identity-matching Services) Bill 2018 (Passports Bill 2018) were introduced into the House of Representatives on 7 February 2018. They were not debated, and lapsed at the dissolution of the 45th Parliament on 11 April 2019. [1]\n",
      "The present Bills were introduced into the House of Representatives on 31 July 2019, and are in the same terms as the 2018 Bills.\n",
      "A Bills digest was prepared in respect of the 2018 Bills. [2] Much of the material in the present Digest has been sourced from that earlier one.\n",
      "Purpose of the Bill\n",
      "The purpose of the Identity-matching Services Bill 2019 (IMS Bill) is to authorise the Commonwealth to facilitate the sharing of identification information, including facial images, between the Commonwealth, states and territories for the purposes of identity-matching. The Bill provides a legal basis for certain aspects of the Intergovernmental Agreement on Identity Matching Services , signed by Council of Australian Governments (COAG) leaders on 5 October 2017. The Agreement provides for sharing and matching of identity information to ‘prevent identity crime, support law enforcement, uphold national security, promote road safety, enhance community safety and improve service delivery’. [3]\n",
      "The purpose of the Australian Passports Amendment (Identity-matching Services) Bill 2019 (Passports Bill) is to amend the Australian Passports Act 2005 ( Passports Act ) to enable the Department of Foreign Affairs and Trade (DFAT) to disclose information for the purpose of participating in identity-matching services, and to authorise the use of computer programs to make decisions.\n",
      "Structure of the Bill\n",
      "The IMS Bill has five Parts: Part 1 contains a simplified outline of the Act and sets out definitions Part 2 authorises the development and operation of identity-matching facilities Part 3 authorises the collection, use and disclosure of information by the Department of Home Affairs (DOHA) Part 4 contains a disclosure offence and sets out exceptions to this Part 5 contains miscellaneous provisions relating to delegation, reporting, review of the operation of the Act and the Minister’s rule-making powers.\n",
      "The Passports Bill has one Schedule, which expands the circumstances in which the Minister for Foreign Affairs and Trade may disclose information and allows the Minister to arrange for the use of computer programs to make decisions.\n",
      "Biometrics and identity-matching\n",
      "The collection and use of biometric information is becoming increasingly prevalent in government agencies and the private sector. Biometric information can be understood as information about unique biological or behavioural characteristics which can be used to identify an individual. [4] Biometric identifiers can include ‘physiological’ identifiers such as fingerprints and palm prints, iris/retinal scans and facial images, as well as ‘behavioural’ identifiers such as gait and voice. [5]\n",
      "Although biometric technologies have long existed, the use of biometrics is increasing as advances in technology allow a person’s biometric data to be easily collected and matched against existing data-sets, to establish or verify their identity and allow law enforcement authorities to identify individuals of concern. [6]\n",
      "Facial recognition technologies\n",
      "The IMS Bill helps to establish a framework for the automated sharing of biometric data—particularly facial images—between federal, state and territory government agencies (and in some cases, local government and private sector organisations). While this sharing is already occurring to some extent, the Explanatory Memorandum provides:\n",
      "Current image-based methods of identifying an unknown person can also be slow, difficult to audit, and often involve manual tasking between requesting agencies and data holding agencies, sometimes taking several days or longer to process. [7]\n",
      "In contrast, the identity-matching services provided for in the Bill enable the rapid, automated sharing and matching of images held in existing government databases, including driver licence, passport and visa photographs. Law academics Monique Mann and Marcus Smith provide the following explanation of how automated facial recognition technology (AFRT) works:\n",
      "Traditional forensic facial mapping involves comparing measurements between facial features [...] or the similarities and differences in facial features [...]. In comparison with these techniques, AFRT involves the automated extraction, digitisation and comparison of the spatial and geometric distribution of facial features. Using an algorithm similar to the ones used in fingerprint recognition, AFRT compares an image of a face with one stored in a database. At the enrolment stage, a digital photograph of a subject's face is taken and a contour map of the position of facial features is converted into a digital template using an algorithm. AFRT systems digitise, store and compare facial templates that measure the relative position of facial features. [8] (References omitted)\n",
      "AFRT can be used to conduct ‘one-to-one’ matching (to verify an individual’s identity) or ‘one-to-many’ searching (in which an image of a person can be compared with all images in a database in order to ascertain their identity). [9]\n",
      "In other countries including the UK, US and Russia, AFRT has been integrated with CCTV systems to enable police to identify persons suspected of committing an offence or subject to an arrest warrant. [10] Similar technology has been trialled in some Australian jurisdictions, including the Northern Territory and Queensland. [11] For example, in 2015 the Northern Territory Government described its use of facial recognition technology as follows:\n",
      "Footage or images captured on CCTV footage can be submitted to NT Police’s facial recognition team who can load it into the facial recognition system for analysis and comparison with existing images in the database.\n",
      "About 100,000 images have been copied into the system database from existing Police information holdings, with the first part of the trial in early 2015 successfully identifying around 300 individuals from photos and CCTV footage. [12]\n",
      "Perth City Council is currently undertaking a twelve-month trial using facial recognition technology in cameras installed across East Perth. It has been reported:\n",
      "... success will be measured by how many times a lawful authority requested the use of the facial recognition capability and how many times a person of interest (which may include missing persons or lost children, as well as criminal suspects) is located. If successful, the council may consider expanding it. [13]\n",
      "Biometric collection and face recognition is already used extensively in connection with immigration control and the issuing of visas. The Migration Act 1958 authorises immigration officials to collect biometric data (referred to as ‘personal identifiers’) from citizens and non-citizens entering or leaving Australia. [14] This can include fingerprints and handprints, height and weight measurements, face images, audio or video recordings, an iris scan or signature. [15] Visa applicants located in certain countries are required to provide biometric information (usually their facial image and fingerprints) at the time they lodge their application. [16]\n",
      "Facial recognition technology and biometric templates are currently used by airport smartgates to verify a traveller’s identity by comparing their ePassport photo with a live image captured at the smartgate. [17] This is being further developed to allow for contactless processing, in which the face matching can take place without a person needing to produce their passport. [18] A trial of such technology at Canberra Airport was paused in July 2019. [19] In March 2018, DOHA announced a $44.2 million contract with Unisys Australia for the provision of a new Enterprise Biometric Identification Services (EBIS) system. It is reported that the new system will match face images and fingerprints of people wishing to travel to Australia against biometric watch lists, in order to identify people of concern. [20]\n",
      "The Australian Criminal Intelligence Commission (ACIC) also provides a number of biometric matching services to federal, state and territory police, including through the National Criminal Investigation DNA Database and National Automated Fingerprint Identification System (NAFIS). [21] However, its planned Biometric Identification Services Project (‘BIS project’), which was intended to replace the NAFIS and develop a facial recognition capability for law enforcement agencies, was terminated in June 2018 following delays and a blowout in the projected costs. [22] In January 2019, the Auditor-General released a performance audit report on the ACIC’s administration of the BIS project, which NEC Australia had been contracted to carry out. It found the ACIC had not effectively managed the project, and that none of the project’s milestones or deliverables had been met despite a total expenditure of $34 million. [23]\n",
      "In April 2019, the Parliamentary Joint Committee on Law Enforcement tabled the report on its inquiry into the impact of new and emerging information and communications technology. [24] It noted the termination of the BIS project, and endorsed a recommendation of the Law Council of Australia that the Australian Government take the following considerations into account when developing future strategies for biometric data and facial recognition systems: the development of an appropriate regime for detecting, auditing, reporting on, responding to and guarding against events that may breach biometric data security the use of methods for assessing the implications of any security breach and communicating the breach to both the general public and the technical, privacy and security communities and publicly releasing additional technical information about the nature of the facial matching scheme, and the process for ensuring that there are not false matches, in order to inform the public about its operation and to allow informed debate about its use and future database links. [25]\n",
      "Identity crime in Australia\n",
      "In his second reading speech for the IMS Bill, the Minister for Immigration, Citizenship, Migrant Services and Multicultural Affairs, David Coleman, stated that the identity-matching services provided for in the Bill will:\n",
      "... help to protect Australians from identity crime, which continues to be one of the most common crimes in Australia. One in four Australians will be a victim of identity crime at some point in their lives, with an estimated annual direct cost of more than $2 billion to the economy. The face verification service will also help people to reclaim their lost or stolen identification documents faster, without the need re-establish their identity. [26]\n",
      "As part of the Australian Government’s National Identity Security Strategy (NISS), the Australian Institute of Criminology (AIC) and the Australian Bureau of Statistics (ABS) have produced a series of reports on identity crime in Australia, drawing on data from federal, state and territory agencies and surveys. The most recent reports estimate the cost of identity crime in Australia in 2015–16 to be $2.65 billion. [27] This figure includes direct and indirect losses incurred by government agencies and individuals, and the cost of identity crimes recorded by police. They estimated the costs of preventing and responding to identity crime during this period for Commonwealth, state and territory agencies (excluding state and territory police) to be $271 million, and $175.7 million for state and territory police. [28]\n",
      "Surveys conducted by the AIC have found that over 20 per cent of respondents each year report having experienced misuse of personal information at some time in the past. [29] The AIC’s 2017 survey found a significant increase in respondents experiencing misuse of their personal information in the previous 12 months (13.1 per cent, compared with 8.5 per cent in 2016) and in the proportion of respondents incurring out-of-pocket losses as a result of this misuse (9.6 per cent, up from 4.9 per cent in 2016). [30] Personal information and identity credentials are obtained from a variety of sources, including physical theft, accidental loss, automated telemarketing calls, and online phishing and malware attacks. [31]\n",
      "Identity crime and national security\n",
      "The Government has also drawn attention to the national security implications of identity crime. In his second reading speech, Minister Coleman highlighted the connections between identity crime and organised crime, stating:\n",
      "Identity crime is a key enabler of serious and organised crime, including terrorism.\n",
      "Australians previously convicted of terrorism related offences are known to have used fake identities to purchase items such as ammunition, chemicals that can be used to manufacture explosives, and mobile phones to communicate anonymously to evade detection.\n",
      "Identity crime is aided by the growing sophistication of criminal syndicates and the technology now able to support them in manufacturing fake identity documents. [32]\n",
      "National security concerns were also emphasised by COAG at the time of the signing of the Intergovernmental Agreement on Identity Matching Services , with a Communiqué stating that the agreement:\n",
      "... will help to protect Australians by making it easier for security and law enforcement agencies to identify people who are suspects or victims of terrorist or other criminal activity, and prevent the use of fake or stolen identities — which is a key enabler of terrorism and other serious crime. [33]\n",
      "There appears to be little publicly available data regarding the connections between identity crime and organised crime. The ACIC, and previously the Australian Crime Commission (ACC), have identified identity crime as a key enabler of organised crime for some time, with the ACC’s first Organised Crime in Australia report in 2007 reporting identity crime to be increasing and ‘fundamental to many organised crime activities’. [34] Internationally, the European Union’s law enforcement agency Europol has similarly reported document fraud to be a key facilitator for organised crime, with the production and use of fraudulent documents being linked to a range of crime areas including drug and people trafficking, migrant smuggling, money laundering and terrorism. [35]\n",
      "The ACIC has identified identity crime as one of the key enablers of serious financial crime, and reports that personal identifying information is traded and sold by criminals to serious and organised crime groups. [36] At the same time, the ACIC suggests that identity crime is likely to become more prevalent with the increased online use and storage of personal information:\n",
      "As more financial services are provided online, there is a requirement for more personal identifiers, such as personal identification numbers, passwords, access codes and security questions, to be created and stored. These personal identifiers are of value to criminal entities and will continue to be harvested, sold and used in fraud and to access systems for other criminal purposes.\n",
      "Identity takeover is likely to emerge as the primary identity crime methodology used to facilitate financial crime, rather than identity creation. As government agencies and private institutions increase services offered online, it is likely that new identity crime enabled financial crime methodologies will be observed. [37]\n",
      "This highlights the difficulties faced by governments in responding to the fraudulent use of identity information, as an increased reliance on personal identifiers to verify a person’s identity also leads to large amounts of personal identification data being collected, shared and stored.\n",
      "National Identity Security Strategy\n",
      "In 2007, heads of COAG signed an Intergovernmental Agreement on a National Identity Security Strategy (NISS), aimed at combatting identity theft and the fraudulent use of stolen and assumed identities. [38] The parties agreed to strengthen government processes and standards for identifying (and verifying the identity of) persons, including through enhancing the interoperability of biometric security measures. [39]\n",
      "The NISS was revised in 2012. [40] The revised strategy highlights the importance of a shared approach to the protection of identity information, noting:\n",
      "Identity crime and misuse is a cross-border activity. It operates on a national and international scale – and will exploit weaknesses in one jurisdiction to obtain benefits in another. This is particularly relevant in Australia, where individuals build their identity with a combination of credentials. These credentials can be issued by multiple jurisdictions, and are often mutually recognised.\n",
      "Jurisdictions have a mutual reliance on the integrity of each other’s identity security frameworks. If one jurisdiction has a less rigorous framework for allocating an identity credential, then it can be exploited. [41]\n",
      "Reflecting this, one goal of the revised NISS was the development of a National Biometric Interoperability Framework, setting out guiding principles for ensuring a consistent approach to the collection, use, disclosure and management of biometrics. The Framework is intended to work within existing legislation, and improve the interoperability of biometric systems across jurisdictions. [42]\n",
      "Document Verification Service\n",
      "Another initiative arising out of the NISS was the Document Verification Service (DVS), which has been operational in the public sector since 2009. [43] The DVS enables the comparison of details on an identity document with records held by the issuing authority, to verify that the details are still valid and the document has not expired or been cancelled. [44] In a similar way to the identity-matching services provided for in the IMS Bill, data is not stored on the DVS itself; instead, requests to verify a person’s identifying information are encrypted and sent through a secure ‘DVS hub’ to the issuing authority. [45] The person must provide express consent for their personal information to be used in this way. [46]\n",
      "The private sector has had access to the DVS since May 2014. [47] Additionally, in November 2015 Australia reached an agreement with New Zealand to allow government agencies and businesses to verify identity documents issued by either country. [48] Businesses seeking to use the DVS must meet criteria set out in the access policy—this includes being subject to Australia’s privacy laws (or the New Zealand equivalent), having a physical presence in Australia or New Zealand, and the use or disclosure of the information being either required by an Australian law or reasonably necessary for the organisation’s activities or functions. [49]\n",
      "There has been a rise in both private and public sector usage of the DVS since 2014. The 2017 AIC report on Identity Crime and Misuse in Australia found that 513 private-sector organisations and 79 government entities used the service at 30 June 2017, compared with 350 private-sector organisations and 45 government agencies the previous year. [50] The DVS can be used to verify information relating to most government-issued identity credentials, including four documents identified by the report as being at particular risk of misuse: Medicare cards, driver licences, birth certificates and passports. [51]\n",
      "The Explanatory Memorandum to the IMS Bill identifies shortcomings in the capacity of the DVS to detect all forms of identity crime:\n",
      "[the DVS] helps to prevent the use of fake identities (false names, dates of birth etc) by detecting when a document does not match a record held by the issuing authority. However, this has incentivised criminals to steal genuine identities and use them for criminal purposes, rather than create entirely false identities. Organised crime groups in particular are developing increasingly sophisticated methods for replicating genuine identification documents with fake photographs, using the same technologies used by the document-issuing agency. These documents are not detected by the DVS because the biographical details are genuine. [52]\n",
      "National Facial Biometric Matching Capability\n",
      "The development of systems to support the sharing and matching of facial images across jurisdictions has been in progress for some years. In October 2014, a meeting of COAG’s then Law, Crime and Community Safety Council (LCCSC) [53] noted the Commonwealth’s plans to establish a National Facial Biometric Matching Capability (Capability), which would provide a mechanism for the cross-jurisdictional sharing of existing information collected by agencies. [54] In subsequent meetings the LCCSC affirmed its support for the Capability and took steps towards the development of an intergovernmental agreement on state and territory participation. [55]\n",
      "In September 2015, the Minister for Justice, Michael Keenan announced that the Commonwealth was spending $18.5 million to develop the Capability, as part of a broader series of measures to combat terrorism and identity crime. [56] The announcement—which corresponded with the release of the Identity Crime and Misuse in Australia 2013–14 report—noted that the Capability would initially involve ‘one-to-one’ image-based verification between Commonwealth agencies, with more agencies to join over time. It would then be further developed to allow ‘one-to-many’ identification matching, enabling law enforcement and security agencies to match the photograph of an unknown person against the photos in government records, to establish the person’s identity. [57] Minister Keenan stated:\n",
      "The report by the Attorney-General’s Department and the AIC estimates that identity crime costs Australia around $2 billion per year, and supports findings from the Australian Crime Commission that identity crime is one of the key enablers of terrorism and organised crime.\n",
      "... the new capability will allow agencies to match a person’s photograph against an image on one of their government records. This will help prevent more insidious forms of identity fraud –where criminals create fake documents using their own photos, with personal information stolen from innocent victims. It will also assist victims more easily restore their compromised identities. [58]\n",
      "The Face Verification Service (FVS) commenced operation in November 2016, enabling the Department of Foreign Affairs and Trade (DFAT) and the Australian Federal Police (AFP) to access citizenship images held by the Immigration Department. At the time of the launch it was announced that other types of images such as visa, passport and driver licence photos would be added over time, and that access would subsequently be expanded to other government agencies. [59]\n",
      "Intergovernmental agreement\n",
      "On 5 October 2017, at a special meeting of COAG on counter-terrorism, all state and territory leaders signed the Intergovernmental Agreement on Identity Matching Services (IGA), providing for the sharing and matching of identity information across jurisdictions. [60] The objective of the IGA is to:\n",
      "... facilitate the secure, automated and accountable exchange of identity information, with robust privacy safeguards, in order to prevent identity crime and promote law enforcement, national security, road safety, community safety and service delivery outcomes. [61]\n",
      "The IGA provides for the exchange of identity information through six specified Identity Matching Services, and other services subsequently developed under the auspices of the Agreement. Of the six named services, at least two—the DVS and FVS—are already in operation. The National Identity Security Coordination Group (Coordination Group) is responsible for developing and maintaining the policies and procedures governing access to each of the services. Participating agencies will also enter into a common Participation Agreement which provides the framework within which the agencies negotiate the details of data sharing arrangements. [62]\n",
      "Schedules to the IGA set out the financial contributions from each state and territory as well as the particular agencies that will have access. The ACT’s participation is subject to limitations: as well as providing that its participation must be consistent with the Human Rights Act 2004 (ACT), Schedule G of the IGA states that the Territory will only allow access to its data for certain purposes, and will not participate in the ‘One Person One Licence System’. [63]\n",
      "Information about how the identity-matching scheme will operate is set out in the Key Issues and Provisions section below.\n",
      "State and territory legislation\n",
      "The IGA does not provide agencies with the legal authority to share information through these services—it is intended that this authorisation is to come from the laws of each state and territory. Part 8 of the IGA provides that each jurisdiction will preserve or introduce legislation as necessary, to support the collection, use and disclosure of facial images and related identity information between the parties.\n",
      "Queensland was the first jurisdiction to pass new legislation on this front, with the Police and Other Legislation (Identity and Biometric Capability) Amendment Act 2018 (Qld) enacted in March 2018. [64] This amended a range of transport and policing laws to authorise Queensland’s participation in the identity matching scheme. Following the passage of the Bill, the Queensland Minister for Police and Corrective Services, Mark Ryan stated that the Bill:\n",
      "... will be of real benefit to those tasked with the security of the Commonwealth Games, which represents a once-in-a-lifetime event that will demonstrate to the world the great things Queensland has to offer.\n",
      "We are expecting both international and interstate guests to attend so I encourage the Federal Government and all states and territories to ensure this legislation is passed in time for the Commonwealth Games. [65]\n",
      "However, an evaluation conducted by the Queensland Police Service after the 2018 Gold Coast Commonwealth Games reportedly found problems with the rollout of the system, including the following:\n",
      "Difficulties were experienced in data ingestion into one of the systems with the testing and availability not available until the week Operation Sentinel [the Games security operation] commenced...\n",
      "The inability of not having the legislation passed, both Commonwealth and state, in time for the Commonwealth Games reduced the database from an anticipated 46 million images to approximately eight million. [66]\n",
      "The ABC reported that while police records had been included in the system, images from Queensland’s Department of Transport and other sources had not been used. It also reported that none of the 16 ‘high-priority targets’ requested as part of the operation could be identified, and that halfway through the Games, the system was opened up to ‘basic policing’. [67]\n",
      "In November 2018, NSW Parliament passed the Road Transport Amendment (National Facial Biometric Matching Capability) Act 2018 , which amended the Road Transport Act 2013 (NSW) to authorise certain government agencies to share information through the identity-matching scheme. [68] A Parliamentary inquiry into the Bill before it was passed noted that the NSW Government had indicated:\n",
      "... at the present stage Roads and Maritime Services has no plans to access or use the Capability, only to provide information to the hub. However, the witnesses noted that in the future the agency may consider signing up to the One Person One Licence Service...another identity-matching service envisaged under the Intergovernmental Agreement which will be available to assist States in upholding the integrity of driver licence and other identification systems. [69]\n",
      "While no other jurisdiction to date has passed legislation in relation to the scheme, the Minister’s second reading speech notes that five states now have the legislative frameworks in place to implement the IGA. [70] Tasmania has amended its driver licensing Regulations to authorise the disclosure of protected information for the purposes of identity-matching services. [71] Existing laws in South Australia [72] and Victoria [73] are also considered to facilitate implementation of the IGA. [74]\n",
      "Privacy and data security\n",
      "Biometric data and privacy concerns\n",
      "The increasing use of biometric systems and templates has amplified concerns regarding the privacy and data security implications of this technology. In a speech to the Biometrics Institute in 2010, the then Deputy Privacy Commissioner, Timothy Pilgrim stated that the collection and handling of biometric information attracts strong public concern because:\n",
      "... biometric information is about a person's physical characteristics. When we collect biometric information from a person, we are not just collecting information about that person, but information of that person.\n",
      "Biometric information cuts across both information privacy and physical privacy. It can reveal sensitive information about us, including information about our health, genetic background and age, and most importantly, it is intrinsic to each of us. [75]\n",
      "In 2008, the ALRC identified a number of general privacy concerns arising from the use of biometric technologies, including: widespread use of biometric systems enables extensive monitoring of the activities of individuals, particularly where the same form of biometric information is used to identify individuals in a number of different contexts biometric technologies, such as facial recognition technologies, may be used to identify individuals without their knowledge or consent biometric information could be used to reveal sensitive personal information, such as information about a person’s health or religious beliefs the security of biometric systems could be compromised and the accuracy and reliability of many biometric systems remains unknown, creating the potential for serious consequences for an individual who is falsely accepted or rejected by such a system. [76]\n",
      "As noted by the ALRC, particular concerns arise with the collection of facial data, as unlike the collection of fingerprints or DNA, facial images can be captured from a distance and without the knowledge or consent of the individual. [77] Furthermore, faces are difficult to hide or alter, and therefore the misuse of this information can be more prolonged than credit card or tax file number data, which can be replaced. [78]\n",
      "Public discussion and reporting on the Capability has situated it within the broader context of governmental data collection, data-matching and data security. Questions have been raised about the security of data stored and shared as part of the Capability, particularly in light of incidents which have drawn attention to potential vulnerabilities in government and non-government systems. [79] This includes reports in 2017 that the Medicare details of any Australian were being sold to order through a darknet auction site, and a mass data breach at US credit agency Equifax which exposed the personal data of 143 million US customers. [80]\n",
      "Bruce Arnold, a law academic and director of the Australian Privacy Foundation, has argued that Australia’s privacy laws are insufficient to protect against misuse or inadvertent disclosure of biometric information:\n",
      "The sharing occurs in a nation where Commonwealth, state and territory privacy law is inconsistent. That law is weakly enforced, in part because watchdogs such as the Office of the Australian Information Commissioner (OAIC) are under-resourced, threatened with closure or have clashed with senior politicians.\n",
      "Australia does not have a coherent enforceable right to privacy. Instead we have a threadbare patchwork of law (including an absence of a discrete privacy statute in several jurisdictions). [81]\n",
      "Privacy Act and biometric data\n",
      "The proposed identity-matching services will be subject to existing privacy laws. The Privacy Act 1988 (Cth), and the Australian Privacy Principles (APPs) made under this Act regulate the handling of personal information by Commonwealth government agencies as well as private sector organisations with an annual turnover of more than $3 million, all private health service providers and some other small businesses. [82] Most states and territories also have privacy laws regulating their respective public sector agencies. [83]\n",
      "Under the Privacy Act , biometric information used for the purpose of automated biometric verification or identification, as well as biometric templates, is classified as ‘sensitive information’. [84] Sensitive information is generally afforded a higher level of protection than other personal information, in recognition of the adverse consequences which may flow from the inappropriate handling of such information. [85] Limitations include that sensitive information can only be collected with consent (unless a specified exception applies) and can only be used or disclosed for a secondary purpose to which it was collected if this is directly related to the primary purpose of collection. [86] However, it is an exception to these restrictions if the collection, use or disclosure is required or authorised by an Australian law.\n",
      "Notifiable data breaches scheme\n",
      "The Notifiable Data Breaches scheme came into effect on 22 February 2018, and applies to agencies and organisations with obligations under the APPs. It requires entities to notify the Australian Information Commissioner and affected individuals about data breaches which are likely to cause serious harm. The notification must include recommendations about the steps individuals should take in response to the breach. [87]\n",
      "Privacy impact assessments\n",
      "In August 2015, a privacy impact assessment (PIA) was carried out in relation to the design and initial operation of the interoperability hub system, through which agencies can request and share facial image data, during its early stages of development. [88] The PIA, conducted by Information Integrity Solutions Pty Ltd (IIS), found that the hub design process and proposed governance arrangements were generally consistent with the requirements of the APPs. At the same time, it highlighted the broad scope of the Capability and the privacy risks associated with the proposed system as a whole:\n",
      "... it is important to recognise that the Hub will have an impact on the circumstances in which facial biometric information is shared, by whom and the volume of images shared, and these risks will have to be actively managed. There is also the risk, which IIS considers is low, that the Hub and the metadata generated by transactions performed through it could potentially allow for some tracking or surveillance of individuals’ everyday activities. However, it is the view of IIS that the privacy impacts of the whole system could well be greater than the risks at individual agency or Hub level. As such, IIS considers that strong, widely respected governance of the system as a whole as, particularly as it evolves over time, is equally and potentially more important than governance of the individual participating agencies and the Hub. [89]\n",
      "In recognition of these risks, the PIA made a series of recommendations to strengthen privacy practices in the design and operation of the hub. This included limiting the metadata generated by the hub, strictly controlling access to one-to-many matching and clarifying the limits on the initial scope of the Capability, as well as including an independent representative on relevant governance bodies to provide the ‘people’s voice’. [90] The AGD accepted or partially accepted all recommendations, though did not support the suggestion of a people’s representative, stating that the public interest would be represented through the OAIC’s involvement in the Coordination Group, and consultation with state and territory privacy commissioners and/or ombudsmen. [91]\n",
      "In 2016, AGD commissioned an independent PIA on the initial use of the Face Verification Service by federal government departments to access citizenship and visa data held by the (then) Department of Immigration and Border Protection. It reported that the PIA found the exchange of data via the FVS to be ‘privacy positive’, due to the service controlling the disclosure of data and maintaining clear audit trails. The PIA made five recommendations to address privacy risks and concerns that may be heightened with increasing use of the FVS. [92] A copy of the PIA has not been publicly released.\n",
      "A Memorandum of Understanding is currently in place between the OAIC and the Attorney-General’s Department for the OAIC to conduct privacy assessments of: the AGD’s management of the interoperability hub and the governance, operation and information security of the National Driver Licence Facial Recognition Solution, provided for in the IMS Bill. [93]\n",
      "The first report was due to be completed by 1 October 2018, but does not appear to have been publicly released. The second is due by 1 October 2019. [94]\n",
      "Committee consideration\n",
      "Parliamentary Joint Committee on Intelligence and Security\n",
      "A review by the Parliamentary Joint Committee on Intelligence and Security (PJCIS) into the 2018 Bills lapsed at the dissolution of the House of Representatives on 11 April 2019. [95] The inquiry had received 20 submissions and had held two public hearings at the time it lapsed.\n",
      "The PJCIS is currently undertaking a review of the reintroduced Bills, and has accepted as evidence all submissions and transcripts from the previous review. [96] Further details can be found at the inquiry homepage .\n",
      "Senate Standing Committee for the Scrutiny of Bills\n",
      "The Senate Standing Committee for the Scrutiny of Bills has not yet reported on the current Bills, but issued a report on the 2018 Bills on 14 February 2018. [97] A key area of concern identified by the Committee was the privacy implications of the IMS Bill, and the fact that a number of safeguards identified in the explanatory materials (and in the IGA) are not included in the Bill itself. [98] The Committee noted that the IMS Bill’s provisions would:\n",
      "... give a broad power for the Home Affairs department to collect, use and disclose personal information for a wide range of purposes to a wide range of government agencies (and some local government authorities and private entities) ... The Bill has clear implications for the privacy of the millions of individuals whose facial images and other biographical information will be available for collection, use and disclosure. [99]\n",
      "Although acknowledging that the explanatory materials provided a detailed analysis of the Bill’s privacy implications, and set out a number of safeguards to help protect privacy, the Committee raised concerns that the Bill may ‘unduly trespass on personal rights and liberties’ due to the breadth of the authorised disclosures. It noted that potential safeguards such as access criteria, requirements for privacy impact assessments and limitations on the amount of information released by the systems, are contained in the IGA but not in the Bill. The Committee sought the Minister’s advice as to whether the intended policy and administrative safeguards could be included as legal requirements in the Bill, or alternatively whether the Bill could include a requirement that such safeguards be implemented by agencies seeking access to identity-matching services. [100]\n",
      "The Minister for Home Affairs responded to the Committee’s comments on 4 April 2018, and the Committee considered this response in its report on 9 May 2018. [101] On the issue of privacy safeguards, the Minister stated that the protections contained in the Bill, and obligations imposed by the IGA, already provide a ‘strong degree of protection for the information transmitted through the identity-matching services’. [102] He further noted that the identity-matching services will be ‘supported by a broad system of controls and arrangements that govern the provision and use of the services’, with the IMS Bill being just one aspect of this. [103] In response, the Committee reiterated its concerns about the adequacy of safeguards in the IMS Bill. [104]\n",
      "Concerns raised by the Committee in relation to specific provisions are discussed in the Key Issues and Provisions section below.\n",
      "Policy position of non-government parties/independents     \n",
      "The Australian Labor Party does not appear to have commented on the Bills directly. The IGA was agreed to by all state and territory leaders, including Labor leaders in Queensland, Victoria, Northern Territory, ACT, Western Australia and South Australia. However, the ACT and Victorian Governments have both stated that the IMS Bill goes beyond the scope of the IGA. [105]\n",
      "At the time the IGA was reached, then Opposition Leader Bill Shorten offered cautious support for the identity-matching system, stating:\n",
      "We think that biometric technology can be a real addition in terms of keeping Australians safe. But of course, when it comes to the final detail, we'll wait to see what the final detail from the Government is. But I just want to reassure Australians that Labor takes a bipartisan approach to good ideas about keeping Australians safe. [106]\n",
      "Shadow Attorney-General, Mark Dreyfus has also stated:\n",
      "... on the face of it, these measures appear sensible; but we will wait to see the detail of what is being proposed ... It is important that the balance between security and privacy is maintained in the face of new threats and there are appropriate protections in place. [107]\n",
      "The Australian Greens have expressed opposition to the measures, with justice spokesperson Senator Nick McKim stating: ‘creating a massive database of people’s photographs is a privacy invasion that creates a honeypot for hackers’. [108]\n",
      "Other minor parties and independents have not commented on the measures to date.\n",
      "Position of major interest groups\n",
      "Civil liberties and privacy organisations have expressed strong concern about the privacy implications of the identity-matching scheme in general. In October 2017, immediately following the signing of the IGA, organisations including the Australian Privacy Foundation, Digital Rights Watch and state and territory civil liberties groups issued a joint statement condemning the creation of a national facial database. The statement described the database as ‘an unnecessary and disproportionate invasion of the privacy rights of all Australians’ and ‘fundamentally incompatible with a free and open society’. [109]\n",
      "These concerns were reiterated in submissions to the PJCIS inquiry in 2018. A number of submissions argued that the IMS Bill is not a proportionate response to the harms it is purporting to address, and may enable substantial infringements on the privacy rights of individuals. [110] A joint submission by Future Wise and the Australian Privacy Foundation contended that the broad purposes of the Bill—which include removing duplicate records and targeting avoidance of traffic fines as well as detecting terrorism—undermine a case for the proportionality of the Bill’s measures:\n",
      "There appears to be no need, for example, to expose all Australian citizens to biometric data matching to remove duplicate records. It is incumbent on government to design other methods of record management that do not involve significant privacy incursions.\n",
      "... The extent of the law enforcement activities contemplated by the Bill should therefore be re-examined, to be limited to those absolutely necessary for public safety—rather than those that are simply convenient or ‘efficient’. [111]\n",
      "Interest groups have expressed doubts about the adequacy of the governance frameworks for the identity-matching services, and the safeguards contained in the IMS Bill. [112] One particular concern has been that many of the rules for access to the services will be contained in access policies and participation agreements made under the intergovernmental agreement. These are not referenced in the Bill. The Office of the Victorian Information Commissioner expressed concern that managing compliance through such instruments ‘may not be sufficiently robust’, noting that they may not be enforceable and could allow ‘fundamental controls to be amended without parliamentary oversight’. [113] This point was similarly made by the Queensland Office of the Information Commissioner, which submitted that the IMS Bill ‘does not adequately embed into law the core intents of the regime to which the Governments have agreed’. [114]\n",
      "In addition to questions about the adequacy of safeguards built into the scheme, some stakeholders also suggested that Australia’s privacy laws do not provide sufficient protection against possible misuse of information under the scheme. [115] A number of submissions raised the possibility of establishing an independent authority responsible for oversight of the retention, collection and use of biometric information, citing the UK’s creation of a Commissioner for the Retention and Use of Biometric Material. [116]\n",
      "It was also suggested that further information about the identity-matching scheme may be required to enable proper consideration of the IMS Bill. For example, the Law Council of Australia argued that insufficient information is available regarding the technical aspects of scheme:\n",
      "It is difficult ... to comment further on the nature and operation of the Interoperability Hub or various identity matching services as there has been very little information released by the Government on their technical development.\n",
      "...The Law Council is of the view that additional technical information about the nature of the identity matching services and the process for ensuring that there are not false matches should be released publicly to allow informed debate about the proposed legislation. [117]\n",
      "Other organisations, including Civil Liberties Australia and the Queensland Office of the Information Commissioner, raised concerns that Privacy Impact Assessments have not yet been completed and published in relation to all services referred to in the Bill and the various uses to be made of them. [118]\n",
      "Support for the measures has been largely based on a security rationale. Anthony Bergin, a senior analyst at the Australian Strategic Policy Institute (ASPI), expressed support for the scheme as provided for in the IGA, arguing that ‘most Australians would be surprised to learn that police don’t have this capability and would be disturbed by the heightened risks faced by our law enforcement officers’. [119]\n",
      "Stakeholder comments in relation to specific provisions of the two Bills are discussed under the Key issues and Provisions section below.\n",
      "Financial implications\n",
      "The Explanatory Memorandum to the IMS Bill states that it does not propose any new expenditure and the overall financial impact is low. [120]\n",
      "As indicated in the background, the Capability received funding of $18.5 million over four years in the 2014–15 Mid-Year Economic and Fiscal Outlook. Further funding of $2.5 million was provided in the 2017–18 Budget to complete the Capability’s build. [121]\n",
      "The IGA specifies that the Commonwealth is responsible for the establishment costs for this system and for 50 per cent of annual operating and maintenance costs. It will also be responsible for the ongoing costs of maintaining and operating the DVS hub and interoperability hub. [122] Each state and territory has committed to a specific financial contribution towards the ongoing operating and maintenance costs of the National Driver Licence Facial Recognition Solution. [123]\n",
      "Statement of Compatibility with Human Rights\n",
      "As required under Part 3 of the Human Rights (Parliamentary Scrutiny) Act 2011 (Cth), the Government has assessed the Bills’ compatibility with the human rights and freedoms recognised or declared in the international instruments listed in section 3 of that Act. The Government considers that the Bills are compatible. [124]\n",
      "Parliamentary Joint Committee on Human Rights\n",
      "The Parliamentary Joint Committee on Human Rights has not yet reported on the Bills, but reported on the 2018 Bills on 27 March 2018. [125] The Committee queried whether the measures are a proportionate limitation on the right to privacy, and sought advice from the Minister for Home Affairs (in relation to the IMS Bill) and Minister for Foreign Affairs (in relation to the Passports Bill) on this point.\n",
      "The Committee raised particular concerns about the scope of the IMS Bill and queried whether the provisions governing access to facial images and other biometric data are sufficiently circumscribed for each of the identity matching services. [126] It noted:\n",
      "As the Hub will permit access to driver licences, the personal information of a significant proportion of the adult Australian population will be retained. A centralised facility for searching such large repositories of facial images and biometric data is a very extensive limitation on the right to privacy... There is a serious question as to whether having databases of, and facilitating access to, facial images of a very significant portion of the population in case they are needed is the least rights restrictive approach to achieving the stated objectives of the measure. [127]\n",
      "The Committee also raised questions about the types of information which may be used—such as social media photographs and historical facial images—and the extent to which the hub will effectively protect against misuse of such information, particularly in relation to vulnerable groups. [128] It noted that international human rights case law has raised concerns about the compatibility of biometric data retention programs with the right to privacy, where the programs involve an indiscriminate or open-ended retention of data. [129] It further queried whether the Privacy Act provides an adequate safeguard for the purposes of international human rights law. [130]\n",
      "Key issues and provisions\n",
      "The IMS Bill is intentionally limited in scope—it is not designed to give effect to the spectrum of information-sharing arrangements and procedures envisioned under the IGA. Instead, it should be seen as one piece of a patchwork of laws and policies which will regulate the use of identity-matching services.\n",
      "The Bill establishes an express legal basis for the Department of Home Affairs (DOHA) to provide identity-matching services and places restrictions on the circumstances in which the services may be used and types of information involved. It does not authorise particular agencies to use the services. Organisations seeking access must be authorised to collect, use and disclose identification information by some other federal, state or territory law. They will also need to meet criteria as specified in the IMS Bill, IGA and in various access policies and agreements made under the IGA.\n",
      "How does the system work?\n",
      "Identity-matching facilities\n",
      "The IMS Bill expressly authorises DOHA to develop, operate and maintain two facilities through which identity-matching services are provided. The system is intended to operate based on a ‘hub and spoke’ model, in which the Commonwealth operates the centralised facilities through which state and territory agencies (and other participating entities) communicate with each other to request or provide information. [131] Details about how these facilities will operate is largely contained in the IGA, rather than in the provisions of the Bill.\n",
      "Clause 14 of the Bill provides that DOHA may develop, operate and maintain the interoperability hub , through which agencies and organisations may electronically relay requests for the provision of identity-matching services, and transmit information in response to such requests. [132] Agencies will access the hub (at least initially) via a web-based user interface into which they log in to manually enter search requests. The IGA provides that over time, the hub will also be able to receive requests via ‘system-to system connections with Agencies’ existing systems’. [133] Identification information of an individual is not stored in the hub itself—in his second reading speech for the 2018 IMS Bill, Minister for Home Affairs, Peter Dutton explained:\n",
      "The hub is not a database and does not conduct any facial biometric matching. Rather it acts like a router, transmitting matching requests received from user agencies to facial image databases. These databases conduct the matching using facial recognition software and return a response back via the hub. [134]\n",
      "The second facility provided for in the Bill is the National Driver Licence Facial Recognition Solution (NDLFRS). [135] This is a federated database of the identity information contained in government identification documents, such as (but not necessarily limited to) driver licences. [136] Each state and territory road agency will have its own partitioned data store, with individual agency-based access controls. Unlike the interoperability hub, the NDLFRS will store identification information contributed by state and territory agencies. It will be connected to the interoperability hub to facilitate data sharing with other agencies. [137]\n",
      "The IGA provides that the Commonwealth, though it hosts and operates the database, will not have the ability to view or modify the information within each partitioned data store. [138] However, the Bill itself does not place any express restrictions on DOHA’s ability to access, collect or disclose information held in the system. [139] Furthermore, the NDLFRS will also include common facial biometric matching software and ‘a central store of biometric templates, derived from facial images replicated by the states and territories using the facial biometric matching software’. Both the software and templates will be managed by the Commonwealth Data Hosting Agency (CDHA). [140]\n",
      "Identity-matching services\n",
      "The Bill provides that the interoperability hub is to be used for the purposes of requesting and providing ‘identity-matching services’. [141] Subclause 7(1) states that an identity-matching service is any of the following: a face identification service ( FIS ), defined under subclause 8(1) as a service which involves electronically comparing the facial image of a person with the identification information of one or more persons contained in government identification documents (often referred to as ‘one to many’ matching) [142] a face verification service ( FVS ), defined under subclause 10(1) as a service comparing the identification information about a person with information contained in a particular government identification document, where a facial image of the person is included in the request and/or in a response to the request (also known as ‘one to one’ matching). [143] Unlike FIS , the service is aimed at verifying—rather than ascertaining—a person’s identity a facial recognition analysis utility service ( FRAUS ), defined under clause 9 as the electronic comparison of a person’s facial image with identification information about the person supplied by the same state or territory authority, which is included in a database in the NDLFRS. The comparison must be for the purpose of assessing the accuracy or quality of information held by the relevant authority [144] the One Person One Licence service ( OPOLS ), in which a person’s facial image and other identification information is compared with information included in a NDLFRS database, for the purpose of determining whether the person holds multiple government identification documents [145] and an identity data sharing service ( IDSS ), defined under clause 11 as a service, other than the four services listed above, which involves a disclosure of a person’s identification information through the interoperability hub. The disclosure must be between Commonwealth, state or territory authorities and for the purpose of an identity or community protection activity (explained below). [146]\n",
      "Minister’s power to prescribe additional services\n",
      "Additionally, paragraph 7(1)(f) gives the Minister the power to make rules prescribing other services as identity-matching services , where they: involve the collection, use and disclosure of identification information and involve the interoperability hub or NDLFRS. [147]\n",
      "Any such rules are in the form of a disallowable legislative instrument. [148] The Minister may prescribe services which permit access by local government authorities or non-government entities if the purpose of the service is for identity verification and certain other conditions are met (these are discussed under ‘private sector access’). [149] The Bill requires the Minister to consult with the Human Rights Commissioner and Information Commissioner about the proposed rules, though does not provide further guidance as to the nature of any consultation. [150]\n",
      "The Queensland Office of the Information Commissioner has raised concerns that the breadth of the rule-making power under paragraph 7(1)(f) may allow the Minister to prescribe ‘many-to-many’ matching services or blanket surveillance. It has recommended that the provision expressly exclude such services. [151]\n",
      "What information may be shared?\n",
      "Identification information\n",
      "The IMS Bill provides for the collection, use and disclosure of identification information . The scope of this term is set out under clause 5 , which provides that it may be information about a living, dead, real or fictitious person and encompasses: current and former names and addresses, place and date of birth, and age (including an age range) the current or former sex, gender identity or intersex status of the person information about whether the person is alive or dead any information contained in or associated with a person’s driver licence, or other licence or identity document issued by a state or territory authority the person’s current or former citizenship, any information about a visa the person holds or has held, and any information contained in or associated with an Australian or foreign travel document and a facial image of the person, biometric template derived from the image or the result of a biometric comparison involving such an image. [152]\n",
      "The Minister may also make rules (in the form of a disallowable legislative instrument) prescribing other types of information to be identification information. [153] Before doing so, the Minister must be satisfied that the information that can be used to identify an individual (whether alone or in conjunction with other information), is reasonably necessary for the provision of an identity-matching service and assists one or more identity or community protection activities. The Minister must also consult with the Human Rights Commissioner and Information Commissioner. [154]\n",
      "Additionally, the IMS Bill specifies information which is not identification information and which therefore cannot be collected, used or disclosed under the Bill. This includes information or an opinion about a person’s: racial or ethnic origin political opinions, philosophical beliefs or religious beliefs or affiliations membership of a political association, professional or trade association or trade union sexual orientation or practices criminal record or health or genetics. [155]\n",
      "However, where information is not primarily one of the above kinds, but nonetheless allows such information about a person to be reasonably inferred (for example, where a person’s racial or ethnic origin may be inferred through their name or place of birth), this may still be identification information and subject to disclosure. [156]\n",
      "What are the limitations on access?\n",
      "As indicated in Minister Coleman’s second reading speech, the IMS Bill does not in itself authorise government agencies or other entities to use identity-matching services, though it provides a broad framework under which the services can operate. [157] An agency or organisation must have a separate legal basis on which it is authorised to disclose information for the purpose of participating in identity-matching services.\n",
      "As indicated above, in addition to legislative authorisation to disclose information, an agency’s ability to access these services will be based on a combination of requirements set out in either or both the Bill and IGA. In particular, the IGA (but not the Bill) provides that participating bodies must meet the criteria set out in the relevant Access Policy, developed by the Coordination Group.\n",
      "Face Verification Service—access policy\n",
      "The Access Policy for the Face Verification Service was issued in June 2017, and provides an example of the criteria an agency must meet in order to participate in identity-matching services. In order to gain access to the FVS, an agency must: provide a statement referencing legislation that provides the legal basis for using and/or disclosing identity information via the FVS undertake or contribute to a privacy impact assessment (PIA) to account for every information flow which occurs through the FVS, to which the agency is a party (unless the agency’s use of the FVS is exempt from the relevant Commonwealth, state or territory privacy laws) enter into an Interagency Data Sharing Arrangement (IDSA) with each agency with which it intends to share information via the FVS. The Access Policy states that where possible, classes of agencies with like functions should enter into common, multilateral agreements maintain a register of Nominated Users who are authorised to submit queries via the FVS, ensure the users undertake training in security awareness and privacy obligations, and ensure that any IT systems connected with the hub receive and maintain appropriate security accreditation have an independent audit conducted of all its data sharing via the FVS at least once every financial year and enter into a memorandum of understanding with DOHA in relation to the services through the interoperability hub. [158]\n",
      "The content of the IDSA must include details of the IDSA’s agreed duration, arrangements for dispute settlement, non-compliance and termination, as well as arrangements for assigning costs associated with the FVS (where relevant). The IDSA must identify the scope of the data-sharing arrangements (such as the accreditation requirements and access permissions for users, maximum number of Nominated Users and method of access, and agreed maximum number of transactions) and the arrangements for protecting personal information shared via the FVS. [159]\n",
      "DOHA is responsible for reviewing IDSAs to ensure consistency with the Access Policy, and for reviewing audit and compliance reports.\n",
      "Although the IMS Bill does not authorise particular agencies to participate in the identity-matching services, Part 3 of the Bill does provide authorisation for DOHA to collect, use and disclose identification information in connection with these services and articulates the scope of the Department’s powers in this area.\n",
      "Clause 17 authorises DOHA to collect identification information where the collection is via an electronic communication to the interoperability hub or the NDLFRS, and for one of the purposes set out in subclause 17(2) . The purposes for which collection is authorised include: providing or developing an identity-matching service for the purpose of an identity or community protection activity (explained below) developing, operating or maintaining the NDLFRS or protecting a person who has acquired an assumed identity under the Crimes Act 1914 (Cth) or is involved in a Commonwealth, state or territory witness protection program. [160]\n",
      "Clause 18 enables DOHA to use or disclose identification information collected through an electronic communication to the interoperability hub or NDLFRS, or held in or generated using the NDLFRS. Again, the use or disclosure must be for one of the purposes set out in subclause 17(2) .\n",
      "Clause 19 specifies that where a state or territory law limits the disclosure of identification information by a state or territory authority (or by a body or person acting on behalf of the authority), but provides an exemption for disclosures authorised by a Commonwealth law, then such an authority, body or person will be permitted to disclose identification information to DOHA for inclusion in the NDLFRS. The Explanatory Memorandum states this is intended to facilitate the disclosure of driver licence data by states and territories, where the existing legislation allows disclosures authorised by Commonwealth law:\n",
      "This is to reduce the number of states and territories that would need to amend their own legislation before Home Affairs could develop the database. [161]\n",
      "Identity or community protection activity\n",
      "As explained above, DOHA will be authorised to collect, use and disclose identification information in developing or providing an identity-matching service for the purpose of an identity or community protection activity . Additionally, certain identity-matching services provided for in the Bill—in particular the FIS and IDSS —can only be accessed in the course of such an activity.\n",
      "Clause 6 provides a definition of identity or community protection activity , as an activity covered by one of the following categories: preventing and detecting identity-related fraud, including the use of stolen or fraudulently obtained government identification documents (or identification information from such documents) [162] law enforcement—that is, the preventing, detecting, investigating or prosecuting an offence against a Commonwealth, state or territory law or in relation to proceedings (or potential proceedings) under the Proceeds of Crime Act 2002 [163] national security—conducting an investigation or gathering intelligence relevant to Australia’s national security [164] protective security—promoting the security of an asset, facility or person associated with government, including by checking the background of a person with access to such an asset/facility or by protecting a person under witness protection/with a legally assumed identity [165] community safety—promoting community safety, including by identifying an individual who has suffered or is reasonably believed to be at risk of suffering physical harm or an individual who is reasonably believed to be involved with a significant risk to public health or safety [166] road safety activities, including promoting the integrity of driver licensing systems [167] and verifying the identity of an individual. [168]\n",
      "The Scrutiny of Bills Committee noted the breadth of some of these purposes, arguing that the sharing of information in relation to any federal, state or territory offence, for road safety or for identity information more broadly:\n",
      "... could allow state and territory agencies to share and seek to match facial images and other biographical information for persons suspected of involvement in very minor offences, such as jaywalking, or for verifying the identity of an individual for any purpose. [169]\n",
      "Submissions to the PJCIS inquiry also raised concerns about the breadth of these categories. The joint submission by Future Wise and the Australian Privacy Foundation suggested that terms such as community safety or road safety:\n",
      "... are defined so widely as to potentially draw almost all activities within the Bill’s ambit. The effect is that biometric matching might be deployed for almost any purpose without limit. [170]\n",
      "Australian Lawyers for Human Rights noted that many of the purposes under clause 6 ‘relate not to uncovering of wrongdoing that has already occurred, but ‘prevention’ and ‘promotion’ activities’, and objected to the use of identity-matching services where there is no clear connection to a likely offence. [171]\n",
      "Face identification service (FIS)\n",
      "The FIS , in providing for one-to-many matches, is one of the more controversial measures in the IGA, as it can involve the use and disclosure of images (and other personal information) of multiple persons who may have no connection to the person in the original image. Reflecting this, the IMS Bill and IGA place greater restrictions on use of this service than on the other services which form part of the scheme.\n",
      "One restriction, noted above, is that the FIS can only be used for the purpose of identifying the individual in the original image, or determining whether they have multiple identities, in the course of an identity or community protection activity covered by any of subclauses 6(2) to 6(6) . [172] This will capture most categories of the definition of identity and community protection activity set out above, but will not allow access for the purposes of road safety activities or identity verification.\n",
      "This largely reflects the IGA’s list of permitted purposes for which agencies may use the FIS. [173] One notable difference is in relation to the ‘law enforcement activities’ category—the IGA states that where the sharing is between agencies in different jurisdictions, the service may only be used for activities relating to an offence which carries a maximum penalty of at least three years imprisonment. [174] This limitation is not replicated in the Bill. The Explanatory Memorandum notes this but does not explain the reason for the omission, stating:\n",
      "The Bill will not specifically restrict this activity to offences that carry a maximum penalty of not less than three years imprisonment ... but it is intended that this restriction will apply on a policy basis. Any amendment to the provisions of the IGA ... will be by agreement between the Commonwealth and the states and territories. As with all of the identity or community protection activities, state or territory agreement will be required before a jurisdiction’s data can be used in relation to additional offences. [175]\n",
      "The absence of any lower limit in the Bill in regards to offences appears to envision future changes to the IGA that expand the offences for which the FIS may be used. Possibly in connection with this, the IGA provides that twelve months after the FIS commences operation, the Coordination Group will review the definition and operation of the general law enforcement purpose, and ‘should consider whether the definition maximises the utility of the FIS for law enforcement agencies, while maintaining appropriate privacy safeguards’. [176] Without amendments to the IGA, it is unlikely—but theoretically possible—that agencies could use the FIS to ascertain the identity of a person suspected of committing a minor infringement.\n",
      "A second restriction is in relation to who may access the FIS . Subclause 8(2) provides a list of authorised agencies—this includes the Australian Border Force; [177] Australian Crime Commission; Australian Federal Police; ASIO; a federal Department administered by a Minister administering citizenship, migration or passports legislation; and state and territory police forces and anti-corruption agencies. The Minister may prescribe further authorities in the rules, but only where satisfied that the authority has a function previously performed by one of the specified state or territory agencies. [178]\n",
      "Private sector access\n",
      "Another concern that has been raised in relation to the IGA and IMS Bill is the extent to which they allow the private sector to access personal information contained in government databases. The use of identity-matching services by private sector entities and local government authorities will be regulated by a combination of provisions under the IMS Bill, the IGA and access policies developed under the agreement.\n",
      "Restrictions under the Bill\n",
      "The IMS Bill provides that, of the five services expressly provided for under the IGA, non-government entities and local government authorities can potentially access the face verification service ( FVS ) only. Such organisations will be able to request information about an individual through the FVS if: verifying the individual’s identity is reasonably necessary for one or more of the organisation’s functions or activities the individual has consented to the organisation using and disclosing their identification information for the purpose of verifying their identity the organisation carries on activities in Australia from premises located in Australia, or resides in Australia and either the Privacy Act applies to the organisation, or in the case of a local government authority, it is bound by a state or territory law or has entered into a written agreement with DOHA which provides for the protection of personal information (and means of recourse for affected individuals) comparable to that provided by the Australian Privacy Principles. [179]\n",
      "Restrictions under the IGA\n",
      "Additionally, the IGA states that private sector access to the FVS to match information held by the states and territories is subject to: the express approval of the relevant minister in each state or territory to use their jurisdiction’s information for this purpose the outcomes of a privacy impact assessment covering the types of organisations to be given access compliance with a ‘FVS Commercial Service Access Policy’ developed by the Coordination Group (including a fee for service arrangement) and an FVS Commercial Service audit and compliance program, overseen by the Coordination Group. [180]\n",
      "The Law Council of Australia has argued that these restrictions provided for in the IGA are ‘important safeguards that should be incorporated into the Bill’. [181] Furthermore, it notes that the Bill does not provide for penalties for private organisations where they make an unauthorised use of the hub or identification information, and suggests the existing controls are insufficient. [182]\n",
      "On the issue of consent, the Law Council has suggested that further information is needed as to how informed consent will be recorded and verified to a standard that enables access to the FVS. [183] Other interest groups have questioned the adequacy of this consent requirement. The joint submission to the PJCIS inquiry by the Australian councils for civil liberties, which opposed private sector access to the identity-matching services, argued:\n",
      "In all cases, consent should be valid, free and voluntary. This is quite often not the case when no real choice or alternative is offered and there is little or no opportunity to opt out. [184]\n",
      "The Office of the Victorian Information Commissioner has also raised concerns about private sector and local government access to the scheme, stating:\n",
      "The variation in the quality of governance and security that can be expected, particularly from local government, raises issues in relation to the adequacy of information management practices and personal information protection. The potential for scope creep—in that personal information may be used for additional purposes other than those for which it was initially collected—is also a significant concern. [185]\n",
      "What protections are in place?\n",
      "Disclosure offence\n",
      "The IMS Bill creates an offence of recording or disclosing protected information when the person making the record or disclosure has obtained the information in their capacity as an entrusted person . [186] The maximum sentence for the offence is imprisonment for two years. It is an exception to the offence where the conduct is either authorised by, or in compliance with, a Commonwealth, state or territory law. [187]\n",
      "An entrusted person is defined broadly as: the Secretary or an APS employee in DOHA an officer or employee of a Commonwealth agency or authority, state, territory or foreign government or authority, or public international organisation, whose services are made available to DOHA or a contractor engaged to provide services to DOHA in connection with the interoperability hub or NDLFRS (or officer or employee of such a contractor). [188]\n",
      "Protected information is: identification information obtained from the NDLFRS or from an electronic communication to or from the NDLFRS or interoperability hub information about the making, content or addressing of such an electronic communication, or about identification information held in the NDLFRS or information that enables access to the hub or NDLFRS. [189]\n",
      "The Scrutiny of Bills Committee raised concerns with the provision, in which authorised disclosure of information is an exception to the offence, rather than the offence being drafted to apply only to ‘unauthorised’ disclosures. The Committee has pointed out that the Criminal Code Act 1995 provides that a defendant who wishes to rely on an exception bears an evidential burden. [190] This means that a defendant who believes the disclosure or recording was authorised must raise evidence on this point (though does not need to positively prove the matter). The Committee has noted that the explanatory materials do not address the issue and asked the Minister to advise why an ‘offence-specific defence’ is being used in this instance. It has suggested:\n",
      "... it may be appropriate if proposed subclause 21(1) was amended to provide that a person commits the offence if the conduct is not authorised by, or in compliance with a requirement under, a law of the Commonwealth or of a State or Territory. [191]\n",
      "In response, the Minister stated that if this defence was included as an element of the offence itself, ‘it would be extremely difficult for the prosecution to establish that the conduct was not authorised under any law’, whereas an entrusted person should be aware of the legislative basis on which they are relying when disclosing information. [192] The Minister suggested the Bill ensures that in handling protected information, the onus is on an entrusted person to show a level of care commensurate with the sensitivity of the information. [193] The Committee requested that this information be included in the Explanatory Memorandum, and reiterated its concerns about the appropriateness of reversing the evidential burden of proof in this case. [194] The Explanatory Memorandum for the 2019 Bill does not provide further information on this point.\n",
      "When will disclosure be authorised?\n",
      "Clauses 22 to 25 set out circumstances in which the recording and disclosure of protected information will be authorised, and therefore act as exceptions to the disclosure offence under clause 21 . An entrusted person may disclose or record protected information: for the purposes of the Identity-matching Services Act 2018 or in the course of exercising powers or performing functions or duties in relation to the interoperability hub or NDLFRS [195] if the person reasonably believes the disclosure is necessary to lessen or prevent a serious and imminent threat to the life or health of an individual, and makes the disclosure for this purpose [196] where the disclosure is to the Integrity Commissioner in relation to a corruption issue (within the meaning of the Law Enforcement Integrity Commissioner Act 2006 ) [197] or where the information relates to the affairs of a person and the person has consented to the recording or disclosure (and the recording or disclosure is in accordance with that consent). [198]\n",
      "Minister’s rule-making power and the obligation to consult\n",
      "Clause 30 provides that the Minister may, by legislative instrument, make rules prescribing matters: required or permitted by the Act to be prescribed by the rules or necessary and convenient to carry out or give effect to the Act.\n",
      "There are some specified limitations on the rules—they cannot create an offence or civil penalty; provide powers of arrest or detention, entry, search or seizure; impose a tax or create an appropriation; or directly amend the text of the Act. [199] The rules are subject to disallowance as well as sunsetting. [200]\n",
      "As explained above, in exercising his power to make rules prescribing additional types of identification information or additional identity-matching services, the Minister will be required to consult the Information Commissioner and Human Rights Commissioner. [201]\n",
      "The Scrutiny of Bills Committee welcomed the Bill’s inclusion of this requirement to consult. However, the Committee suggested that the requirement be strengthened by making such consultation a condition of the validity of the legislative instrument. [202] The Committee also queried the inclusion of significant matters such as this in a rule rather than in Regulations, noting that Regulations are subject to a higher level of executive scrutiny as they must be drafted by the Office of Parliamentary Counsel and approved by the Federal Executive Council. [203]\n",
      "The Law Council raised similar concerns, suggesting that there are risks that through these provisions, the scope of the identity-matching scheme could be determined by delegated rather than primary legislation. It has also queried whether either the Australian Human Rights Commission or Office of the Australian Information Commissioner are sufficiently resourced to take on this additional consultation role. [204] The Law Council recommended that the consultation requirement be amended to include a requirement for the Minister to report to the public on the results of these consultations, and any reasons for departing from advice provided by the commissioners, before making a relevant rule. [205]\n",
      "In response to the concerns raised by the Scrutiny of Bills Committee, the Minister accepted the Committee’s recommendation that the Minister be required to have regard to any submissions made by the commissioners prior to making the rules, and if the rules depart from the commissioners’ advice, provide reasons for this. He indicated he would propose Government amendments to this effect. [206] However, no changes have been made to the 2019 IMS Bill to incorporate such a requirement. On the question of the appropriateness of rules rather than Regulations, the Minister pointed to the Office of Parliamentary Counsel’s Drafting Direction No. 3.8 – Subordinate Legislation , which provides that its starting point is that subordinate instruments should be made in the form of legislative instruments (as distinct from Regulations), and noted that the Bill expressly prohibits certain matters from being prescribed in rules. [207] The Committee stated it would make no further comment on the matter. [208]\n",
      "Annual reporting requirement\n",
      "Clause 28 requires the Secretary of DOHA to give a report to the Minister at the end of each financial year, for tabling in each House of Parliament, with statistics relating to all requests from Commonwealth, state and territory authorities (except ASIO) for an FIS, FVS or OPOLS. The statistics are to be broken down by requesting authority, service requested, number of requests in which information (or confirmation of identity) was provided and those in which no information or confirmation was provided, and in the case of the FIS, the kind of identity or community protection activity for which the service was requested. [209]\n",
      "The Secretary must similarly report statistics on requests made by non-government entities for an FVS. However, this data is not required to identify the particular organisations, but rather the total number of requests and total number of entities (as well as the number in which information was or was not provided). [210]\n",
      "Additionally, for each government authority (other than ASIO) which used an IDSS to disclose or collect identification information, the Secretary must provide the name of the authority, a brief description of the nature of the information and an indication whether the authority collected or disclosed that information. [211] The report must also include any other information required by the Minister in relation to an identity-matching service or administration of the Act. [212]\n",
      "Subclause 28(2) provides that the report must not ‘unreasonably’ disclose personal information about an individual. The Explanatory Memorandum notes that this is aimed at ensuring the report does not disclose personal information ‘that is not reasonably required for accountability purposes’. [213] It states that this is not intended to prevent the inclusion of publicly available information about an individual. [214]\n",
      "A number of stakeholders and interest groups have suggested that this reporting requirement be further strengthened. The Office of the Victorian Information Commissioner has noted that clause 28 does not expressly require reporting on data breaches or misuse of the services:\n",
      "... it tells the public about the quantum of requests but little about the security of the data or the compliance of participants in the IMS ecosystem. [215]\n",
      "Noting that the new Notifiable Data Breaches scheme will not capture all agencies and bodies accessing the identity matching services (such as state and territory government organisations), the Office suggested that another mechanism be inserted into the Bill to include specific reporting relating to instances of unauthorised or inappropriate access and the remedial action taken in response. [216] It suggests that the complex nature of the identity-matching scheme makes this particularly important:\n",
      "...The inter-related nature of the Bill, the IGA and the other agreements also makes assurance of compliance activities more complex, and is another reason for more transparent reporting. [217]\n",
      "The Law Council has criticised the fact that the reporting requirements do not capture non-government entities or ASIO. Although noting that the Explanatory Memorandum states this is due to considerations of commercial confidentiality, it has argued that ‘the public have a right to know which non-government entities have access to the Face Verification Service’. [218] It has further suggested that restrictions on the reporting of ASIO-related data ‘should be determined on a case by case basis and not included ... as a blanket exception’. [219] The Queensland Office of the Information Commissioner has similarly recommended that the reporting requirement be expanded to capture data breaches and incidents as well as non-government access to the FVS. [220]\n",
      "The Scrutiny of Bills Committee queried whether the reporting requirement should be extended to capture instances where information is disclosed pursuant to clause 23 (disclosures to lessen or prevent a threat to life or health) or clause 24 (disclosures relating to a corruption issue). [221] In response, the Minister accepted the suggestion in relation to clause 23 , and indicated that he would propose an amendment to the Bill to accommodate this. [222] However, no such change has been included in the 2019 IMS Bill. In relation to reporting on information disclosed pursuant to clause 24 , the Minister noted that such a requirement could jeopardise the confidentiality of disclosures, which may occur without the Secretary’s knowledge, and that the Integrity Commissioner already has reporting requirements in relation to these types of disclosures under the Law Enforcement Integrity Commissioner Act 2006 . [223] The Committee requested this information be included in the Explanatory Memorandum, and stated it would not comment further on the matter. [224] The Explanatory Memorandum for the 2019 IMS Bill does not include further information on this point.\n",
      "Statutory review\n",
      "The IMS Bill requires the Minister to cause a review of the operation of the Act and the provision of identity-matching services to be started within five years of the Act’s commencement. [225] The report is to be tabled in each House of Parliament within 15 sitting days after it is received by the Minister.\n",
      "This is a longer timeframe than specified in the IGA, which provides that a general review into the operation of the identity-matching services will be conducted three years from the commencement of the agreement. The IGA states that the review is to assess matters including the effectiveness of the services in progressing the objectives of the agreement, the effectiveness of governance arrangements, the privacy impacts and effectiveness of privacy safeguards in protecting personal information. [226] The terms of reference are to be set by the Coordination Group and the review is to be published online by the Commonwealth.\n",
      "It is unclear whether the review provided for in the Bill is intended to be separate to that in the IGA, and the explanatory materials do not directly discuss this point. The Explanatory Memorandum states that a five year timeframe is necessary as:\n",
      "... it may take some time for all of the states and territories to commence participation in the identity-matching services, and sufficient operating time is needed to ensure that the functioning of the services in relation to all jurisdictions can be assessed adequately. [227]\n",
      "The Queensland Office of the Information Commissioner has stated it would be preferable for the review to commence two years after commencement of the legislation, noting that this was recommended by the Queensland Parliamentary Legal Affairs and Community Safety Committee following its consideration of the Queensland Bill. [228] It has also suggested that it may be appropriate for the IMS Bill to specify ‘critical components’ of the review, such as ‘expansion of services within the IMS regime, abuse of the system, mistakes arising from false positives ,[and] unintended outcomes of the IMS’. [229]\n",
      "The Passports Bill amends the Passports Act to allow for the disclosure of personal information in relation to identity-matching services. Currently, section 46 of that Act provides that the Minister for Foreign Affairs may disclose personal information for a number of specified purposes—this includes law enforcement, confirming or verifying information about a passport applicant or facilitating a person’s international travel. [230] Disclosure is limited to the types of information and persons specified by the Minister under the Australian Passports Determination 2015 , and this is dependent on the particular purpose of disclosure. [231] There are currently three classes of information which may be disclosed (though not in all circumstances): data page information , which means information contained on the data page of an Australian travel document, such as the document number, expiry date, and the name, data of birth, photograph and signature of the document holder status information , which means information about whether the document is currently valid, including whether it has been lost or stolen or has restrictions on its use and authenticity information , which is information necessary to establish the authenticity of a person applying for or holding an Australian travel document. [232]\n",
      "Item 1 of the Passports Bill inserts proposed paragraph 46(da) into the Passports Act to provide that the Minister may disclose personal information for the purposes of participating in a service to share or match information relating to a person’s identity. The service must be specified or of a kind specified in the Minister’s determination.\n",
      "The amendment does not appear to significantly expand the Minister’s power to disclose personal information—section 46 already permits the disclosure of photographs to a wide range of federal, state and territory government agencies as well as Interpol and foreign border authorities. Proposed paragraph 46(da) , in providing a broad authority for disclosures expressly in relation to identity-matching services, will cover any existing gaps which might limit DFAT’s capacity to participate in identity-matching services.\n",
      "Computerised decision-making\n",
      "Item 3 of the Passports Bill inserts proposed section 56A into the Passports Act to provide for computerised decision-making. This empowers the Minister to arrange for the use of computer programs to make decisions or exercise other powers of the Minister under the Act (or associated legislative instruments). The Minister is taken to have made the decision or exercised the relevant power that was made or exercised by the computer program. [233] Proposed subsection 56A(3) enables the Minister to substitute a decision for a decision made by a computer program, where satisfied that the decision made by the computer program is incorrect.\n",
      "The Explanatory Memorandum provides that it is intended that automation will be used for ‘low-risk decisions that a computer can make within objective parameters’. [234] In particular, it indicates that the provision will allow the Minister to arrange automated disclosures of personal information for the purposes of the identity-matching services, as provided for under proposed paragraph 46(da) , stating ‘this is necessary to facilitate DFAT’s full participation in the services, given that they will operate on an automated basis’. [235]\n",
      "Proposed section 56A is in similar terms to computerised decision-making provisions in a broad range of other Acts. [236] The use of computer programs to automate government decision-making has been occurring in various forms for some time, with benefits including the ability for such programs to instantaneously apply complex rules and policies and reduce inaccuracy, inconsistency and bias in decision-making. However, there are also risks associated with automated decision-making, with the potential for seemingly minor programming errors to lead to large numbers of incorrect decisions. [237]\n",
      "Submissions to the PJCIS inquiry raised concerns with this provision. Australian Lawyers for Human Rights argued that proposed section 56A is overly broad and does not distinguish between programs being used to assist in decision-making and to actually make the decision. [238] The Australian councils for civil liberties suggested that if the provision is to be enacted, the decisions which are made by computers and the data used to generate the decisions are made publicly available, and that ‘strong procedural fairness criteria’ be included. [239]\n",
      "[1] .      Parliament of Australia, ‘ Identity-matching Services Bill 2018 homepage ’, Australian Parliament website; Parliament of Australia, ‘ Australian Passports Amendment (Identity-matching Services) Bill 2018 homepage ’, Australian Parliament website.\n",
      "[2] .      C Petrie, Identity-matching Services Bill 2018 and Australian Passports Amendment (Identity-matching Services) Bill 2018 , Bills digest, 110, 2017–18, Parliamentary Library, Canberra, 22 May 2018.\n",
      "[3] .      Council of Australian Governments (COAG), Intergovernmental Agreement on Identity Matching Services , COAG meeting, Canberra, 5 October 2017.\n",
      "[4] .      Attorney-General’s Department (AGD), National identity proofing guidelines , AGD, Canberra, 2016, Appendix A, p. 24; H Clark and C Morris, ‘ Managing biometric information: the future is in the palm of your hands (and in your fingerprints, your iris and your facial features) ’, Privacy Law Bulletin , 14(6), August 2017, p. 94.\n",
      "[6] .      Australian Law Reform Commission (ALRC), For your information: Australian privacy law and practice , report, 108, ALRC, Canberra, 2008, p. 407.\n",
      "[7] .      Explanatory Memorandum , Identity-matching Services Bill 2019, p. 3.\n",
      "[8] .      M Mann and M Smith, ‘ Automated facial recognition technology: recent developments and approaches to oversight ’, University of New South Wales Law Journal , 40(1), 2017, p. 122.\n",
      "[9] .      Ibid., p. 123.\n",
      "[10] .    Ibid., pp. 123–4; S Levin, ‘ Half of US adults are recorded in police facial recognition databases, study says ’, The Guardian , 19 October 2016; V Dodd, ‘ Met police to use facial recognition software at Notting Hill carnival ’, The Guardian , 5 August 2017; C McGoogan, ‘ Facial recognition fitted to 5,000 CCTV cameras in Moscow ’, The Telegraph (UK) , 29 September 2017.\n",
      "[11] .    A Guest, ‘ Facial recognition software trials in Queensland alarm privacy advocates ’, ABC News , 10 March 2017; A Giles (Chief Minister of the Northern Territory) and P Chandler (Minister for Police, Fire and Emergency Services NT), Facial recognition technology for police to help keep Territorians safe , media release, 27 August 2015.\n",
      "[12] .    Giles and Chandler, Facial recognition technology for police to help keep Territorians safe , op. cit.\n",
      "[13] .    E Thomas, ‘ Perth council facial recognition trial greeted with concern and scepticism ’, The Guardian (Australia) , 12 June 2019; M Cormann (Minister for Finance and the Public Service) and A Tudge (Minister for Cities, Urban Infrastructure, and Population), Perth Smart City project to improve citizen safety , media release, 13 May 2019.\n",
      "[14] .    Migration Act 1958 (Cth), section 257A. For further background about the development of the migration law with regards to biometrics, see: MA Neilsen, Migration Amendment (Strengthening Biometrics Integrity) Bill 2015 , Bills digest, 111, 2014–15, Parliamentary Library, Canberra, 2015.\n",
      "[15] .    Migration Act , section 5A.\n",
      "[18] .    C Petrie, Migration Amendment (Visa Revalidation and Other Measures) Bill 2016 , Bills digest, 51, 2016–17, Parliamentary Library, Canberra, 2016, pp. 5–6, 13–14; S Trask, ‘ Airport trial of SmartGate technology ’, The Canberra Times , 30 November 2017, p. 12; M O’Sullivan, ‘ Your face will be your passport ’, The Sydney Morning Herald , 22 February 2018, p. 1.\n",
      "[19] .    J Hendry, ‘ Australia's airport smartgate upgrade stalls ’, iTnews , 15 July 2019.\n",
      "[20] .    A Hawke (Assistant Minister for Home Affairs), Enormous boost to Australia’s biometric capability , media release, 19 March 2018; J Hendry, ‘ Unisys to provide Australia’s new biometrics travel platform ’, iTnews , 19 March 2018.\n",
      "[21] .    Australian Criminal Intelligence Commission (ACIC), ‘ Biometric matching ’, ACIC website, last updated 21 June 2019.\n",
      "[22] .    J Hendry, ‘ NEC loses national biometrics database project ’, iTnews , 15 June 2018; S Whyte, ‘ Biometrics deal dumped after delays, blowout ’, The Canberra Times , 16 June 2018, p. 3.\n",
      "[23] .    Australian National Audit Office (ANAO), The Australian Criminal Intelligence Commission's administration of the Biometric Identification Services project , Auditor-General Report, 24, 2018–19, 21 January 2019, p. 8.\n",
      "[24] .    Parliamentary Joint Committee on Law Enforcement, Impact of new and emerging information and communications technology: report , The Committee, Canberra, April 2019.\n",
      "[25] .    Ibid., p. 87 (recommendation 7).\n",
      "[26] .    D Coleman, ‘ Second reading speech: Identity-matching Services Bill 2019 ’, House of Representatives, Debates , (proof), 31 July 2019, p. 11.\n",
      "[27] .    P Jorna and RG Smith, Identity crime and misuse in Australia 2017 , AIC Reports—Statistical Report 10, Australian Institute of Criminology, 30 December 2018, pp. x–xi; RG Smith and P Jorna, ‘ Counting the costs of identity crime and misuse in Australia, 2015–16 ’, AIC, Statistical Bulletin , 15, 30 December 2018, p. 6 (this Bulletin provides a more detailed and precise costs breakdown). The reports use the term ‘identity crime’ broadly, as covering ‘activities/offences in which a perpetrator uses a fabricated identity, a manipulated identity, or a stolen/assumed identity to facilitate the commission of crime’.\n",
      "[28] .    Smith and Jorna, ‘ Counting the costs of identity crime and misuse in Australia, 2015–16 ’, op. cit., pp. 15–19.\n",
      "[29] .    Jorna and Smith, Identity crime and misuse in Australia 2017 , op. cit., p. 35.\n",
      "[30] .    Jorna and Smith, Identity crime and misuse in Australia 2017 , op. cit., p. xii.\n",
      "[31] .    Jorna and Smith, Identity crime and misuse in Australia 2017 , op. cit., p. 7.\n",
      "[34] .    Australian Crime Commission (ACC), Organised crime in Australia , ACC, Canberra, 2007, p. 9.\n",
      "[36] .    ACIC, Serious financial crime in Australia 2017 , ACIC, Canberra, 2017, p. 15.\n",
      "[39] .    Ibid., clauses 6 and 7.\n",
      "[44] .    DOHA, ‘ Document verification service ’, DOHA website, last updated 22 January 2019.\n",
      "[45] .    Ibid.; Document Verification Service (DVS), ‘ How the DVS works ’, DVS website, last updated 15 May 2018.\n",
      "[46] .    DVS, ‘ The DVS and consent ’, DVS website, last updated 16 January 2018.\n",
      "[47] .    G Brandis (Attorney-General), Helping business combat identity crime and streamline online services , media release, 5 May 2014.\n",
      "[48] .    M Keenan (Minister for Justice) and P Dunne (New Zealand Minister of Internal Affairs), Australia–New Zealand agreement to help combat identity crime , media release, 11 November 2015.\n",
      "[49] .    DOHA, Document verification service (DVS) commercial service: access policy , DOHA, Canberra, version 4, n.d., p. 2.\n",
      "[50] .    Jorna and Smith, Identity crime and misuse in Australia 2017 , op. cit., pp. xvi, 52–3.\n",
      "[51] .    Ibid., pp. 52–3.\n",
      "[52] .    Explanatory Memorandum , IMS Bill, p. 46.\n",
      "[53] .    The Law, Crime and Community Safety Council (LCCSC) was made up of ministers with responsibility for law and justice, police and emergency management in each Australian state and territory, as well as two ministers from the Australian and New Zealand Governments. Following a COAG review in 2016–17, the LCCSC was replaced with separate councils for Attorneys-General and Ministers for Police and Emergency Management. See: AGD, ‘ Law, Crime and Community Safety Council ’, AGD website.\n",
      "[54] .    LCCSC, Communique , COAG Meeting, Canberra, 3 October 2014, p. 2.\n",
      "[55] .    For example: LCCSC, Communique , COAG Meeting, Canberra, 22 May 2015, p. 2; LCCSC, Draft communique , COAG Meeting, Canberra, 5 November 2015, p. 2; LCCSC, Communique , COAG Meeting, Canberra, 19 May 2017, p. 7.\n",
      "[56] .    M Keenan (Minister for Justice), New $18.5 million biometrics tool to put a face to crime , media release, 9 September 2015.\n",
      "[59] .    M Keenan (Minister for Justice), New face verification service to tackle identity crime , media release, 16 November 2016.\n",
      "[61] .    Ibid., p. 4 (clause 1.1).\n",
      "[62] .    Ibid., clauses 7.2–7.6.\n",
      "[63] .    Ibid., Schedule G, A.11, p. 45; T McIlroy, ‘ Barr a lone voice for civil liberties ’, The Canberra Times , 6 October 2017, p. 4.\n",
      "[65] .    M Ryan (Queensland Minister for Police, Minister for Corrective Services), Queensland leads nation to strengthen security measures , media release, 7 March 2018.\n",
      "[66] .    J Bavas, ‘ Facial recognition system rollout was too rushed, Queensland police report reveals ’, ABC News online , 6 May 2019.\n",
      "[69] .    NSW Legislative Council Standing Committee on Law and Justice, Road Transport Amendment (National Facial Biometric Matching Capability) Bill 2018 , Report, 65, 12 November 2018, pp. 4–5.\n",
      "[72] .    Public Sector (Data Sharing) Act 2016 (SA), section 13 permits the Minister to enter into data-sharing agreements.\n",
      "[73] .    Road Safety Act 1998 (Vic), sub-paragraph 90K(a)(vi) permits the use or disclosure of information collected or received by the Roads Corporation in relation to its registration or licensing functions and activities, for the purposes of giving effect to an intergovernmental agreement.\n",
      "[74] .    DOHA, Submission to the NSW Legislative Council Standing Committee on Law and Justice, Inquiry into the Road Transport Amendment (National Facial Biometric Matching Capability) Bill 2018 , 31 October 2018, p. 3.\n",
      "[75] .    T Pilgrim (Deputy Privacy Commissioner), Privacy in Australia: challenges and opportunities , speech to Biometrics Institute, Sydney, 27 May 2010.\n",
      "[76] .    ALRC, For your information: Australian privacy law and practice , op. cit., pp. 408–9.\n",
      "[80] .    P Farrell, ‘ The Medicare machine: patient details of “any Australian” for sale on darknet ’, The Guardian (Australia) , 4 July 2017; A Andriotis and R McMillan, ‘ Equifax slammed for huge data hack ’, The Australian , 11 September 2017.\n",
      "[81] .    B Arnold, ‘ Let's face it, we'll be no safer with a national facial recognition database ’, The Conversation , 6 October 2017.\n",
      "[82] .    Privacy Act 1988 (Cth); OAIC, ‘ Privacy Act ’, OAIC website, last updated 29 July 2019; OAIC, ‘ Privacy for organisations—small business ’, OAIC website, last updated 15 August 2019.\n",
      "[83] .    OAIC, ‘ Privacy in your state ’, OAIC website, last updated 6 August 2019.\n",
      "[84] .    Privacy Act , section 6.\n",
      "[86] .    Privacy Act , Schedule 1, APP 3 and APP 6.\n",
      "[88] .    Information Integrity Solutions, National Facial Biometric Matching Capability privacy impact assessment—interoperability hub , report carried out for Attorney-General’s Department, August 2015.\n",
      "[89] .    Ibid., p. 5.\n",
      "[90] .    Ibid., pp. 5–7. For analysis of the PIA and the Government’s response, see: B Arnold, ‘ A national identity hub? The privacy impact assessment for the National Facial Biometric Matching Scheme ’, Privacy Law Bulletin , 13(3), March 2016, pp. 50–3.\n",
      "[94] .    Ibid., Schedule 2.\n",
      "[97] .    Senate Standing Committee for the Scrutiny of Bills, Scrutiny digest , 2, 2018, The Senate, 14 February 2018, pp. 14–15 (Passports Bill), 20–8 (IMS Bill).\n",
      "[98] .    Ibid., pp. 20–4.\n",
      "[101] .  Senate Standing Committee for the Scrutiny of Bills, Scrutiny digest , 5, 2018, The Senate, 9 May 2018, pp. 103–120.\n",
      "[104] .  Ibid., p. 110.\n",
      "[105] .  Victorian Government, Submission to Parliamentary Joint Committee on Intelligence and Security, Review of the Identity-matching Services Bill 2018 and the Australian Passports Amendment (Identity-matching Services) Bill 2018 , n.d., p. 3; F O’Mallon, ‘ ACT raises clash with facial recognition law ’, The Canberra Times , 13 May 2018, p. 9.\n",
      "[106] .  B Shorten (Leader of the Opposition) and J Ryan, Joint doorstop interview: Australian manufacturing; COAG; Turnbull's gas crisis; Australian wool , transcript, Melbourne, 4 October 2017.\n",
      "[110] .  Future Wise and Australian Privacy Foundation, Submission to Parliamentary Joint Committee on Intelligence and Security, Review of the Identity-matching Services Bill 2018 and the Australian Passports Amendment (Identity-matching Services) Bill 2018 , March 2018, p. 7; Australian Lawyers for Human Rights (ALHR), Submission to Parliamentary Joint Committee on Intelligence and Security, Review of the Identity-matching Services Bill 2018 and the Australian Passports Amendment (Identity-matching Services) Bill 2018 , 20 March 2018, p. 3; Law Council of Australia, Submission to Parliamentary Joint Committee on Intelligence and Security, Review of the Identity-matching Services Bill 2018 and the Australian Passports Amendment (Identity-matching Services) Bill 2018 , 21 March 2018, p. 3.\n",
      "[111] .  Future Wise and Australian Privacy Foundation, Submission to Parliamentary Joint Committee on Intelligence and Security, op. cit., p. 7.\n",
      "[112] .  Future Wise and Australian Privacy Foundation, Submission to Parliamentary Joint Committee on Intelligence and Security, op. cit., pp. 11–12; Office of the Victorian Information Commissioner (OVIC), Submission to Parliamentary Joint Committee on Intelligence and Security, Review of the Identity-matching Services Bill 2018 and the Australian Passports Amendment (Identity-matching Services) Bill 2018 , 21 March 2018; Queensland Office of the Information Commissioner (QOIC), Submission to Parliamentary Joint Committee on Intelligence and Security, Review of the Identity-matching Services Bill 2018 and the Australian Passports Amendment (Identity-matching Services) Bill 2018 , March 2018, pp. 4–5.\n",
      "[113] .  OVIC, Submission to Parliamentary Joint Committee on Intelligence and Security, op. cit., pp. 1, 3.\n",
      "[114] .  Ibid., p. 3.\n",
      "[115] .  ALHR, Submission to Parliamentary Joint Committee on Intelligence and Security, op. cit., pp. 3–4; Future Wise and Australian Privacy Foundation, Submission to Parliamentary Joint Committee on Intelligence and Security, op. cit., pp. 11–12.\n",
      "[116] .  Law Council of Australia, Submission to Parliamentary Joint Committee on Intelligence and Security, op. cit., p. 8; Joint councils for civil liberties, Submission to Parliamentary Joint Committee on Intelligence and Security, Review of the Identity-matching Services Bill 2018 and the Australian Passports Amendment (Identity-matching Services) Bill 2018 , 21 March 2018, pp. 4–5; Future Wise and Australian Privacy Foundation, Submission to Parliamentary Joint Committee on Intelligence and Security, op. cit., pp. 11–12.\n",
      "[117] .  Law Council of Australia, Submission to Parliamentary Joint Committee on Intelligence and Security, op. cit., p. 4.\n",
      "[118] .  Civil Liberties Australia, Submission to Parliamentary Joint Committee on Intelligence and Security, Review of the Identity-matching Services Bill 2018 and the Australian Passports Amendment (Identity-matching Services) Bill 2018 , 21 March 2018, p. 3; QOIC, Submission to Parliamentary Joint Committee on Intelligence and Security, op. cit., p. 3.\n",
      "[119] .  A Bergin, ‘ Information-sharing among agencies key to national security ’, Australian Strategic Policy Institute (ASPI) website, 13 October 2017.\n",
      "[120] .  Explanatory Memorandum , IMS Bill, p. 5.\n",
      "[121] .  Explanatory Statement , Financial Framework (Supplementary Powers) Amendment (Attorney-General’s Portfolio Measures No. 2) Regulations 2017; Senate Legislation and Constitutional Affairs Legislation Committee, Official committee Hansard , 26 February 2018, p. 118.\n",
      "[123] .  Ibid., Schedules A to H.\n",
      "[124] .  The Statements of Compatibility with Human Rights can be found at pages 40–58 of the Explanatory Memorandum to the IMS Bill and pages 4–7 of the Explanatory Memorandum to the Passports Bill.\n",
      "[125] .  Parliamentary Joint Committee on Human Rights, Human rights scrutiny report , 3, 2018, 27 March 2018, pp. 41–51.\n",
      "[126] .  Ibid., pp. 43–6, 49.\n",
      "[131] .  Explanatory Memorandum , IMS Bill, p. 28.\n",
      "[132] .  IMS Bill, clause 14 .\n",
      "[134] .  P Dutton, ‘ Second reading speech: Identity-matching Services Bill 2018 ’, House of Representatives, Debates , 7 February 2018, p. 485.\n",
      "[135] .  IMS Bill, clause 15 .\n",
      "[136] .  The IGA provides that other types of facial images may be included in the NDLFRS at the request of a state or territory—it provides the examples of images on firearms licences and proof of age cards (clause 6.18).\n",
      "[137] .  COAG, Intergovernmental Agreement on Identity Matching Services , op. cit., clause 6.16.\n",
      "[138] .  COAG, Intergovernmental Agreement on Identity Matching Services , op. cit., subclause 6.16(c).\n",
      "[139] .  For example, see the authorisation provisions at clauses 17 to 18 , discussed under ‘What are the limitations on access?’ below.\n",
      "[140] .  COAG, Intergovernmental Agreement on Identity Matching Services , op. cit., clause 6.15.\n",
      "[148] .  IMS Bill, clause 30 .\n",
      "[151] .  QOIC, Submission to Parliamentary Joint Committee on Intelligence and Security, op. cit., pp. 3, 5.\n",
      "[152] .  IMS Bill, subclause 5(1) .\n",
      "[153] .  IMS Bill, paragraph 5(1)(n) and clause 30 .\n",
      "[154] .  IMS Bill, subclause 5(4) .\n",
      "[156] .  IMS Bill, subclause 5(3) .\n",
      "[157] .  D Coleman, ‘ Second reading speech: Identity-matching Services Bill 2019 ’, op. cit., p. 13.\n",
      "[158] .  AGD, ‘ Face Verification Service (FVS)—access policy ’, AGD, Canberra, June 2017, pp. 2–5. This policy has not yet been updated in light of the machinery of government changes in December 2017; however, DOHA, rather than AGD, is now responsible for managing agencies’ access to identity-matching services, including through entering into MOUs and reviewing ISDAs: DOHA, ‘ Face matching services ’, DOHA website, last updated 22 January 2019.\n",
      "[161] .  Explanatory Memorandum , IMS Bill, p. 31.\n",
      "[168] .  IMS Bill, subclause 6(8) .\n",
      "[169] .  Senate Standing Committee for the Scrutiny of Bills, Scrutiny digest , 2, 2018, op. cit., p. 23.\n",
      "[170] .  Future Wise and Australian Privacy Foundation, Submission to Parliamentary Joint Committee on Intelligence and Security, op. cit., p. 5.\n",
      "[171] .  Australian Lawyers for Human Rights, Submission to Parliamentary Joint Committee on Intelligence and Security, op. cit., p. 5.\n",
      "[172] .  IMS Bill, paragraph 8(1)(b) .\n",
      "[174] .  Ibid., subclause 4.21(b), clause 4.22.\n",
      "[175] .  Explanatory Memorandum , IMS Bill, p. 16.\n",
      "[177] .  The ABF may request the service only so far as it is investigating or prosecuting an offence against the Customs Act 1901 , Crimes Act 1914 , Criminal Code or Environment Protection and Biodiversity Conservation Act 1999 : IMS Bill, paragraph 8(2)(a) .\n",
      "[178] .  IMS Bill, paragraph 8(2)(q) and subclause 8(3) .\n",
      "[179] .  IMS Bill, subclauses 7(3) and (4) .\n",
      "[181] .  Law Council of Australia, Submission to Parliamentary Joint Committee on Intelligence and Security, op. cit., p. 6.\n",
      "[183] .  Ibid., p. 5.\n",
      "[184] .  Joint councils for civil liberties, Submission to Parliamentary Joint Committee on Intelligence and Security, op. cit., pp. 5–6.\n",
      "[185] .  OVIC, Submission to Parliamentary Joint Committee on Intelligence and Security, op. cit., p. 2.\n",
      "[186] .  IMS Bill, subclause 21(1) .\n",
      "[190] .  Senate Standing Committee for the Scrutiny of Bills, Scrutiny digest , 2, 2018, op. cit., pp. 26–7.\n",
      "[192] .  Senate Standing Committee for the Scrutiny of Bills, Scrutiny digest , 5, 2018, op. cit., p. 116.\n",
      "[201] .  IMS Bill, subclauses 5(4) and 7(5) .\n",
      "[202] .  Senate Standing Committee for the Scrutiny of Bills, Scrutiny digest , 2, 2018, op. cit., p. 25.\n",
      "[204] .  Law Council, Submission to Parliamentary Joint Committee on Intelligence and Security, op. cit., pp. 4–5.\n",
      "[206] .  Senate Standing Committee for the Scrutiny of Bills, Scrutiny digest , 5, 2018, op. cit., pp. 111–2. See Office of Parliamentary Counsel (OPC), Drafting direction no. 3.8 – subordinate legislation , OPC, July 2017.\n",
      "[207] .  Ibid., p. 112.\n",
      "[210] .  IMS Bill, paragraph 28(1)(b) .\n",
      "[211] .  IMS Bill, paragraph 28(1)(c) .\n",
      "[212] .  IMS Bill, paragraph 28(1)(d) .\n",
      "[215] .  OVIC, Submission to Parliamentary Joint Committee on Intelligence and Security, op. cit., p. 2.\n",
      "[218] .  Law Council of Australia, Submission to Parliamentary Joint Committee on Intelligence and Security, op. cit., p. 7.\n",
      "[220] .  QOIC, Submission to Parliamentary Joint Committee on Intelligence and Security, op. cit., pp. 4–5.\n",
      "[221] .  Senate Standing Committee for the Scrutiny of Bills, Scrutiny digest , 2, 2018, op. cit., pp. 27–8.\n",
      "[222] .  Senate Standing Committee for the Scrutiny of Bills, Scrutiny digest , 5, 2018, op. cit., p. 118.\n",
      "[223] .  Ibid., pp. 118–9.\n",
      "[227] .  Explanatory Memorandum , IMS Bill, p. 38.\n",
      "[228] .  QOIC, Submission to Parliamentary Joint Committee on Intelligence and Security, op. cit., p. 4.\n",
      "[232] .  Ibid., subclause 23(3).\n",
      "[233] .  Passports Bill, proposed subsections 56A(1) and (2) .\n",
      "[234] .  Explanatory Memorandum , Australian Passports Amendment (Identity-matching Services) Bill 2019, p. 3.\n",
      "[237] .          For further discussion of issues associated with automated decision-making, see: S Power and A Grove, National Health Amendment (Pharmaceutical Benefits) Bill 2016 , Bills digest, 66, 2016–17, Parliamentary Library, Canberra, 22 February 2017, pp. 2–3, 6–7; C Petrie, Veterans’ Affairs Legislation Amendment (Digital Readiness and Other Measures) Bill 2016 , Bills digest, 68, 2016–17, Parliamentary Library, Canberra, 27 February 2017, pp. 3–5, 10.\n",
      "[238] .  ALHR, Submission to Parliamentary Joint Committee on Intelligence and Security, op. cit., pp. 9–11.\n",
      "[239] .  Joint councils for civil liberties, Submission to Parliamentary Joint Committee on Intelligence and Security, op. cit., pp. 14–15.\n",
      "For copyright reasons some linked items are only available to members of Parliament.\n",
      "© Commonwealth of Australia\n",
      "Creative Commons\n",
      "With the exception of the Commonwealth Coat of Arms, and to the extent that copyright subsists in a third party, this publication, its logo and front page design are licensed under a  Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Australia  licence.\n",
      "In essence, you are free to copy and communicate this work in its current form for all non-commercial purposes, as long as you attribute the work to the author and abide by the other licence terms. The work cannot be adapted or modified in any way. Content from this publication should be attributed in the following way: Author(s), Title of publication, Series Name and No, Publisher, Date.\n",
      "To the extent that copyright subsists in third party quotes it remains with the original owner and permission may be required to reuse the material.\n",
      "Inquiries regarding the licence and any use of the publication are welcome to  webmanager@aph.gov.au .\n",
      "Disclaimer:  Bills Digests are prepared to support the work of the Australian Parliament. They are produced under time and resource constraints and aim to be available in time for debate in the Chambers. The views expressed in Bills Digests do not reflect an official position of the Australian Parliamentary Library, nor do they constitute professional legal opinion. Bills Digests reflect the relevant legislation as introduced and do not canvass subsequent amendments or developments. Other sources should be consulted to determine the official status of the Bill.\n",
      "Any concerns or complaints should be directed to the Parliamentary Librarian. Parliamentary Library staff are available to discuss the contents of publications with Senators and Members and their staff. To access this service, clients may contact the author or the Library‘s Central Enquiry Point for referral.\n"
     ]
    }
   ],
   "source": [
    "print(texts[filename][\"dragnet\"][\"doc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f38e2f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bills Digest No. 21, 2019–20\n",
      "\n",
      "PDF version [869KB]\n",
      "\n",
      "Claire Petrie\n",
      "\n",
      "Law and Bills Digest Section\n",
      "\n",
      "26 August 2019\n",
      "\n",
      "Contents\n",
      "\n",
      "The Bills Digest at a glance\n",
      "\n",
      "History of the Bill\n",
      "\n",
      "Purpose of the Bill\n",
      "\n",
      "Structure of the Bill\n",
      "\n",
      "Background\n",
      "\n",
      "Committee consideration\n",
      "\n",
      "Policy position of non-government parties/independents\n",
      "\n",
      "Position of major interest groups\n",
      "\n",
      "Financial implications\n",
      "\n",
      "Statement of Compatibility with Human Rights\n",
      "\n",
      "Key issues and provisions\n",
      "\n",
      "Date introduced: 31 July 2019\n",
      "\n",
      "House: House of Representatives\n",
      "\n",
      "Portfolio: Home Affairs and Foreign Affairs and Trade\n",
      "\n",
      "Commencement: Both Bills commence the day after Royal Assent.\n",
      "\n",
      "Links: The links to the Bills, their Explanatory Memorandum and second reading speech can be found on the Bill’s home page for the Identity-matching Services Bill 2019 and Australian Passports Amendment (Identity-matching Services) Bill 2019, or through the Australian Parliament website.\n",
      "\n",
      "When Bills have been passed and have received Royal Assent, they become Acts, which can be found at the Federal Register of Legislation website.\n",
      "\n",
      "All hyperlinks in this Bills Digest are correct as at August 2019.\n",
      "\n",
      "The Bills Digest at a glance\n",
      "\n",
      "Purpose of the Bills\n",
      "\n",
      "The Identity-matching Services Bill 2019 (IMS Bill) authorises the Department of Home Affairs (DOHA) to create and maintain facilities for the sharing of facial images and other identity information between government agencies, and in some cases, private organisations.\n",
      "\n",
      "It provides a legislative basis for certain measures contained in the Intergovernmental Agreement on Identity Matching Services (IGA), agreed to by COAG leaders on 5 October 2017. This agreement aims to facilitate the ‘secure, automated and accountable’ exchange of identity information to help prevent identity crime and promote a range of law enforcement, community safety and service delivery objectives.\n",
      "\n",
      "The Australian Passports Amendment (Identity-matching Services) Bill 2019 (Passports Bill) authorises the Department of Foreign Affairs and Trade to disclose information in order to participate in identity-matching services and provides for computerised decision-making.\n",
      "\n",
      "Both Bills were introduced in the same form during the 45th Parliament, but were not debated before the dissolution of the House of Representatives in April 2019.\n",
      "\n",
      "How the IMS Bill works\n",
      "\n",
      "The IMS Bill authorises DOHA to develop, operate and maintain two centralised facilities for the provision of identity-matching services: an ‘interoperability hub’, intended to operate as a router through which participating agencies and organisations can request and transmit information and the National Driver Licence Facial Recognition Service (NDLRFS), a federated database of information contained in government identity documents such as driver licences.\n",
      "\n",
      "The Bill specifies identity-matching services which will operate through the hub. This includes the Face Verification Service (FVS), which allows users to verify a specific person’s identity, and the Face Identification Service (FIS), which involves the electronic matching of a facial image with the images of one or more people, in order to identify a person. Private sector entities and local government authorities may have access to the FVS.\n",
      "\n",
      "The Bill does not authorise certain agencies to use identity-matching services—entities seeking access will need a legal basis for collecting and disclosing personal information, and must meet access requirements set out in the IGA.\n",
      "\n",
      "The Bill creates an offence for entrusted persons to record or disclose protected information in connection with these services, and sets out circumstances where disclosure will be authorised.\n",
      "\n",
      "The Minister for Home Affairs will be required to report annually to Parliament about the use of the services. A statutory review is to be started within five years of the Act’s commencement.\n",
      "\n",
      "Key issues\n",
      "\n",
      "The Bills are currently being reviewed by the Parliamentary Joint Committee on Intelligence and Security (PJCIS). The Committee previously commenced an inquiry into the 2018 versions of the Bills, but the inquiry lapsed at the dissolution of the House of Representatives in April 2019.\n",
      "\n",
      "In relation to the 2018 Bills, the Parliamentary Joint Committee for Human Rights, Senate Standing Committee for the Scrutiny of Bills and submissions to the PJCIS inquiry raised concerns that the broad scope of the IMS Bill may enable substantial infringements on privacy rights, allowing disclosure of personal information for an extremely wide range of purposes.\n",
      "\n",
      "Stakeholders suggested the IMS Bill provides inadequate protection against misuse of this information, and queried why it does not include key safeguards contained in the IGA, such as access criteria and limitations on the amount of information released by the identity-matching systems.\n",
      "\n",
      "Another area of concern is private sector access, with submissions questioning whether this is appropriate, and arguing that there are insufficient safeguards in the Bill at present.\n",
      "\n",
      "Stakeholders also raised concerns with the computerised decision-making provision in the Passports Bill, suggesting that it is too broad and should be refined.\n",
      "\n",
      "History of the Bill\n",
      "\n",
      "The Identity-matching Services Bill 2018 (IMS Bill 2018) and Australian Passports Amendment (Identity-matching Services) Bill 2018 (Passports Bill 2018) were introduced into the House of Representatives on 7 February 2018. They were not debated, and lapsed at the dissolution of the 45th Parliament on 11 April 2019.[1]\n",
      "\n",
      "The present Bills were introduced into the House of Representatives on 31 July 2019, and are in the same terms as the 2018 Bills.\n",
      "\n",
      "A Bills digest was prepared in respect of the 2018 Bills.[2] Much of the material in the present Digest has been sourced from that earlier one.\n",
      "\n",
      "Purpose of the Bill\n",
      "\n",
      "The purpose of the Identity-matching Services Bill 2019 (IMS Bill) is to authorise the Commonwealth to facilitate the sharing of identification information, including facial images, between the Commonwealth, states and territories for the purposes of identity-matching. The Bill provides a legal basis for certain aspects of the Intergovernmental Agreement on Identity Matching Services, signed by Council of Australian Governments (COAG) leaders on 5 October 2017. The Agreement provides for sharing and matching of identity information to ‘prevent identity crime, support law enforcement, uphold national security, promote road safety, enhance community safety and improve service delivery’.[3]\n",
      "\n",
      "The purpose of the Australian Passports Amendment (Identity-matching Services) Bill 2019 (Passports Bill) is to amend the Australian Passports Act 2005 (Passports Act) to enable the Department of Foreign Affairs and Trade (DFAT) to disclose information for the purpose of participating in identity-matching services, and to authorise the use of computer programs to make decisions.\n",
      "\n",
      "Structure of the Bill\n",
      "\n",
      "The IMS Bill has five Parts:\n",
      "\n",
      "Part 1 contains a simplified outline of the Act and sets out definitions\n",
      "\n",
      "Part 2 authorises the development and operation of identity-matching facilities\n",
      "\n",
      "Part 3 authorises the collection, use and disclosure of information by the Department of Home Affairs (DOHA)\n",
      "\n",
      "Part 4 contains a disclosure offence and sets out exceptions to this\n",
      "\n",
      "Part 5 contains miscellaneous provisions relating to delegation, reporting, review of the operation of the Act and the Minister’s rule-making powers.\n",
      "\n",
      "The Passports Bill has one Schedule, which expands the circumstances in which the Minister for Foreign Affairs and Trade may disclose information and allows the Minister to arrange for the use of computer programs to make decisions.\n",
      "\n",
      "Background\n",
      "\n",
      "Biometrics and identity-matching\n",
      "\n",
      "The collection and use of biometric information is becoming increasingly prevalent in government agencies and the private sector. Biometric information can be understood as information about unique biological or behavioural characteristics which can be used to identify an individual.[4] Biometric identifiers can include ‘physiological’ identifiers such as fingerprints and palm prints, iris/retinal scans and facial images, as well as ‘behavioural’ identifiers such as gait and voice.[5]\n",
      "\n",
      "Although biometric technologies have long existed, the use of biometrics is increasing as advances in technology allow a person’s biometric data to be easily collected and matched against existing data-sets, to establish or verify their identity and allow law enforcement authorities to identify individuals of concern.[6]\n",
      "\n",
      "Facial recognition technologies\n",
      "\n",
      "The IMS Bill helps to establish a framework for the automated sharing of biometric data—particularly facial images—between federal, state and territory government agencies (and in some cases, local government and private sector organisations). While this sharing is already occurring to some extent, the Explanatory Memorandum provides:\n",
      "\n",
      "Current image-based methods of identifying an unknown person can also be slow, difficult to audit, and often involve manual tasking between requesting agencies and data holding agencies, sometimes taking several days or longer to process.[7]\n",
      "\n",
      "In contrast, the identity-matching services provided for in the Bill enable the rapid, automated sharing and matching of images held in existing government databases, including driver licence, passport and visa photographs. Law academics Monique Mann and Marcus Smith provide the following explanation of how automated facial recognition technology (AFRT) works:\n",
      "\n",
      "Traditional forensic facial mapping involves comparing measurements between facial features [...] or the similarities and differences in facial features [...]. In comparison with these techniques, AFRT involves the automated extraction, digitisation and comparison of the spatial and geometric distribution of facial features. Using an algorithm similar to the ones used in fingerprint recognition, AFRT compares an image of a face with one stored in a database. At the enrolment stage, a digital photograph of a subject's face is taken and a contour map of the position of facial features is converted into a digital template using an algorithm. AFRT systems digitise, store and compare facial templates that measure the relative position of facial features.[8] (References omitted)\n",
      "\n",
      "AFRT can be used to conduct ‘one-to-one’ matching (to verify an individual’s identity) or ‘one-to-many’ searching (in which an image of a person can be compared with all images in a database in order to ascertain their identity).[9]\n",
      "\n",
      "In other countries including the UK, US and Russia, AFRT has been integrated with CCTV systems to enable police to identify persons suspected of committing an offence or subject to an arrest warrant.[10] Similar technology has been trialled in some Australian jurisdictions, including the Northern Territory and Queensland.[11] For example, in 2015 the Northern Territory Government described its use of facial recognition technology as follows:\n",
      "\n",
      "Footage or images captured on CCTV footage can be submitted to NT Police’s facial recognition team who can load it into the facial recognition system for analysis and comparison with existing images in the database. About 100,000 images have been copied into the system database from existing Police information holdings, with the first part of the trial in early 2015 successfully identifying around 300 individuals from photos and CCTV footage.[12]\n",
      "\n",
      "Perth City Council is currently undertaking a twelve-month trial using facial recognition technology in cameras installed across East Perth. It has been reported:\n",
      "\n",
      "... success will be measured by how many times a lawful authority requested the use of the facial recognition capability and how many times a person of interest (which may include missing persons or lost children, as well as criminal suspects) is located. If successful, the council may consider expanding it.[13]\n",
      "\n",
      "Biometric collection and face recognition is already used extensively in connection with immigration control and the issuing of visas. The Migration Act 1958 authorises immigration officials to collect biometric data (referred to as ‘personal identifiers’) from citizens and non-citizens entering or leaving Australia.[14] This can include fingerprints and handprints, height and weight measurements, face images, audio or video recordings, an iris scan or signature.[15] Visa applicants located in certain countries are required to provide biometric information (usually their facial image and fingerprints) at the time they lodge their application.[16]\n",
      "\n",
      "Facial recognition technology and biometric templates are currently used by airport smartgates to verify a traveller’s identity by comparing their ePassport photo with a live image captured at the smartgate.[17] This is being further developed to allow for contactless processing, in which the face matching can take place without a person needing to produce their passport.[18] A trial of such technology at Canberra Airport was paused in July 2019.[19] In March 2018, DOHA announced a $44.2 million contract with Unisys Australia for the provision of a new Enterprise Biometric Identification Services (EBIS) system. It is reported that the new system will match face images and fingerprints of people wishing to travel to Australia against biometric watch lists, in order to identify people of concern.[20]\n",
      "\n",
      "The Australian Criminal Intelligence Commission (ACIC) also provides a number of biometric matching services to federal, state and territory police, including through the National Criminal Investigation DNA Database and National Automated Fingerprint Identification System (NAFIS).[21] However, its planned Biometric Identification Services Project (‘BIS project’), which was intended to replace the NAFIS and develop a facial recognition capability for law enforcement agencies, was terminated in June 2018 following delays and a blowout in the projected costs.[22] In January 2019, the Auditor-General released a performance audit report on the ACIC’s administration of the BIS project, which NEC Australia had been contracted to carry out. It found the ACIC had not effectively managed the project, and that none of the project’s milestones or deliverables had been met despite a total expenditure of $34 million.[23]\n",
      "\n",
      "In April 2019, the Parliamentary Joint Committee on Law Enforcement tabled the report on its inquiry into the impact of new and emerging information and communications technology.[24] It noted the termination of the BIS project, and endorsed a recommendation of the Law Council of Australia that the Australian Government take the following considerations into account when developing future strategies for biometric data and facial recognition systems:\n",
      "\n",
      "the development of an appropriate regime for detecting, auditing, reporting on, responding to and guarding against events that may breach biometric data security\n",
      "\n",
      "\n",
      "\n",
      "the use of methods for assessing the implications of any security breach and communicating the breach to both the general public and the technical, privacy and security communities and\n",
      "\n",
      "\n",
      "\n",
      "publicly releasing additional technical information about the nature of the facial matching scheme, and the process for ensuring that there are not false matches, in order to inform the public about its operation and to allow informed debate about its use and future database links.[25]\n",
      "\n",
      "Identity crime in Australia\n",
      "\n",
      "In his second reading speech for the IMS Bill, the Minister for Immigration, Citizenship, Migrant Services and Multicultural Affairs, David Coleman, stated that the identity-matching services provided for in the Bill will:\n",
      "\n",
      "... help to protect Australians from identity crime, which continues to be one of the most common crimes in Australia. One in four Australians will be a victim of identity crime at some point in their lives, with an estimated annual direct cost of more than $2 billion to the economy. The face verification service will also help people to reclaim their lost or stolen identification documents faster, without the need re-establish their identity.[26]\n",
      "\n",
      "As part of the Australian Government’s National Identity Security Strategy (NISS), the Australian Institute of Criminology (AIC) and the Australian Bureau of Statistics (ABS) have produced a series of reports on identity crime in Australia, drawing on data from federal, state and territory agencies and surveys. The most recent reports estimate the cost of identity crime in Australia in 2015–16 to be $2.65 billion.[27] This figure includes direct and indirect losses incurred by government agencies and individuals, and the cost of identity crimes recorded by police. They estimated the costs of preventing and responding to identity crime during this period for Commonwealth, state and territory agencies (excluding state and territory police) to be $271 million, and $175.7 million for state and territory police.[28]\n",
      "\n",
      "Surveys conducted by the AIC have found that over 20 per cent of respondents each year report having experienced misuse of personal information at some time in the past.[29] The AIC’s 2017 survey found a significant increase in respondents experiencing misuse of their personal information in the previous 12 months (13.1 per cent, compared with 8.5 per cent in 2016) and in the proportion of respondents incurring out-of-pocket losses as a result of this misuse (9.6 per cent, up from 4.9 per cent in 2016).[30] Personal information and identity credentials are obtained from a variety of sources, including physical theft, accidental loss, automated telemarketing calls, and online phishing and malware attacks.[31]\n",
      "\n",
      "Identity crime and national security\n",
      "\n",
      "The Government has also drawn attention to the national security implications of identity crime. In his second reading speech, Minister Coleman highlighted the connections between identity crime and organised crime, stating:\n",
      "\n",
      "Identity crime is a key enabler of serious and organised crime, including terrorism. Australians previously convicted of terrorism related offences are known to have used fake identities to purchase items such as ammunition, chemicals that can be used to manufacture explosives, and mobile phones to communicate anonymously to evade detection. Identity crime is aided by the growing sophistication of criminal syndicates and the technology now able to support them in manufacturing fake identity documents.[32]\n",
      "\n",
      "National security concerns were also emphasised by COAG at the time of the signing of the Intergovernmental Agreement on Identity Matching Services, with a Communiqué stating that the agreement:\n",
      "\n",
      "... will help to protect Australians by making it easier for security and law enforcement agencies to identify people who are suspects or victims of terrorist or other criminal activity, and prevent the use of fake or stolen identities — which is a key enabler of terrorism and other serious crime.[33]\n",
      "\n",
      "There appears to be little publicly available data regarding the connections between identity crime and organised crime. The ACIC, and previously the Australian Crime Commission (ACC), have identified identity crime as a key enabler of organised crime for some time, with the ACC’s first Organised Crime in Australia report in 2007 reporting identity crime to be increasing and ‘fundamental to many organised crime activities’.[34] Internationally, the European Union’s law enforcement agency Europol has similarly reported document fraud to be a key facilitator for organised crime, with the production and use of fraudulent documents being linked to a range of crime areas including drug and people trafficking, migrant smuggling, money laundering and terrorism.[35]\n",
      "\n",
      "The ACIC has identified identity crime as one of the key enablers of serious financial crime, and reports that personal identifying information is traded and sold by criminals to serious and organised crime groups.[36] At the same time, the ACIC suggests that identity crime is likely to become more prevalent with the increased online use and storage of personal information:\n",
      "\n",
      "As more financial services are provided online, there is a requirement for more personal identifiers, such as personal identification numbers, passwords, access codes and security questions, to be created and stored. These personal identifiers are of value to criminal entities and will continue to be harvested, sold and used in fraud and to access systems for other criminal purposes. Identity takeover is likely to emerge as the primary identity crime methodology used to facilitate financial crime, rather than identity creation. As government agencies and private institutions increase services offered online, it is likely that new identity crime enabled financial crime methodologies will be observed.[37]\n",
      "\n",
      "This highlights the difficulties faced by governments in responding to the fraudulent use of identity information, as an increased reliance on personal identifiers to verify a person’s identity also leads to large amounts of personal identification data being collected, shared and stored.\n",
      "\n",
      "National Identity Security Strategy\n",
      "\n",
      "In 2007, heads of COAG signed an Intergovernmental Agreement on a National Identity Security Strategy (NISS), aimed at combatting identity theft and the fraudulent use of stolen and assumed identities.[38] The parties agreed to strengthen government processes and standards for identifying (and verifying the identity of) persons, including through enhancing the interoperability of biometric security measures.[39]\n",
      "\n",
      "The NISS was revised in 2012.[40] The revised strategy highlights the importance of a shared approach to the protection of identity information, noting:\n",
      "\n",
      "Identity crime and misuse is a cross-border activity. It operates on a national and international scale – and will exploit weaknesses in one jurisdiction to obtain benefits in another. This is particularly relevant in Australia, where individuals build their identity with a combination of credentials. These credentials can be issued by multiple jurisdictions, and are often mutually recognised. Jurisdictions have a mutual reliance on the integrity of each other’s identity security frameworks. If one jurisdiction has a less rigorous framework for allocating an identity credential, then it can be exploited.[41]\n",
      "\n",
      "Reflecting this, one goal of the revised NISS was the development of a National Biometric Interoperability Framework, setting out guiding principles for ensuring a consistent approach to the collection, use, disclosure and management of biometrics. The Framework is intended to work within existing legislation, and improve the interoperability of biometric systems across jurisdictions.[42]\n",
      "\n",
      "Document Verification Service\n",
      "\n",
      "Another initiative arising out of the NISS was the Document Verification Service (DVS), which has been operational in the public sector since 2009.[43] The DVS enables the comparison of details on an identity document with records held by the issuing authority, to verify that the details are still valid and the document has not expired or been cancelled.[44] In a similar way to the identity-matching services provided for in the IMS Bill, data is not stored on the DVS itself; instead, requests to verify a person’s identifying information are encrypted and sent through a secure ‘DVS hub’ to the issuing authority.[45] The person must provide express consent for their personal information to be used in this way.[46]\n",
      "\n",
      "The private sector has had access to the DVS since May 2014.[47] Additionally, in November 2015 Australia reached an agreement with New Zealand to allow government agencies and businesses to verify identity documents issued by either country.[48] Businesses seeking to use the DVS must meet criteria set out in the access policy—this includes being subject to Australia’s privacy laws (or the New Zealand equivalent), having a physical presence in Australia or New Zealand, and the use or disclosure of the information being either required by an Australian law or reasonably necessary for the organisation’s activities or functions.[49]\n",
      "\n",
      "There has been a rise in both private and public sector usage of the DVS since 2014. The 2017 AIC report on Identity Crime and Misuse in Australia found that 513 private-sector organisations and 79 government entities used the service at 30 June 2017, compared with 350 private-sector organisations and 45 government agencies the previous year.[50] The DVS can be used to verify information relating to most government-issued identity credentials, including four documents identified by the report as being at particular risk of misuse: Medicare cards, driver licences, birth certificates and passports.[51]\n",
      "\n",
      "The Explanatory Memorandum to the IMS Bill identifies shortcomings in the capacity of the DVS to detect all forms of identity crime:\n",
      "\n",
      "[the DVS] helps to prevent the use of fake identities (false names, dates of birth etc) by detecting when a document does not match a record held by the issuing authority. However, this has incentivised criminals to steal genuine identities and use them for criminal purposes, rather than create entirely false identities. Organised crime groups in particular are developing increasingly sophisticated methods for replicating genuine identification documents with fake photographs, using the same technologies used by the document-issuing agency. These documents are not detected by the DVS because the biographical details are genuine.[52]\n",
      "\n",
      "National Facial Biometric Matching Capability\n",
      "\n",
      "The development of systems to support the sharing and matching of facial images across jurisdictions has been in progress for some years. In October 2014, a meeting of COAG’s then Law, Crime and Community Safety Council (LCCSC)[53] noted the Commonwealth’s plans to establish a National Facial Biometric Matching Capability (Capability), which would provide a mechanism for the cross-jurisdictional sharing of existing information collected by agencies.[54] In subsequent meetings the LCCSC affirmed its support for the Capability and took steps towards the development of an intergovernmental agreement on state and territory participation.[55]\n",
      "\n",
      "In September 2015, the Minister for Justice, Michael Keenan announced that the Commonwealth was spending $18.5 million to develop the Capability, as part of a broader series of measures to combat terrorism and identity crime.[56] The announcement—which corresponded with the release of the Identity Crime and Misuse in Australia 2013–14 report—noted that the Capability would initially involve ‘one-to-one’ image-based verification between Commonwealth agencies, with more agencies to join over time. It would then be further developed to allow ‘one-to-many’ identification matching, enabling law enforcement and security agencies to match the photograph of an unknown person against the photos in government records, to establish the person’s identity.[57] Minister Keenan stated:\n",
      "\n",
      "The report by the Attorney-General’s Department and the AIC estimates that identity crime costs Australia around $2 billion per year, and supports findings from the Australian Crime Commission that identity crime is one of the key enablers of terrorism and organised crime. ... the new capability will allow agencies to match a person’s photograph against an image on one of their government records. This will help prevent more insidious forms of identity fraud –where criminals create fake documents using their own photos, with personal information stolen from innocent victims. It will also assist victims more easily restore their compromised identities.[58]\n",
      "\n",
      "The Face Verification Service (FVS) commenced operation in November 2016, enabling the Department of Foreign Affairs and Trade (DFAT) and the Australian Federal Police (AFP) to access citizenship images held by the Immigration Department. At the time of the launch it was announced that other types of images such as visa, passport and driver licence photos would be added over time, and that access would subsequently be expanded to other government agencies.[59]\n",
      "\n",
      "Intergovernmental agreement\n",
      "\n",
      "On 5 October 2017, at a special meeting of COAG on counter-terrorism, all state and territory leaders signed the Intergovernmental Agreement on Identity Matching Services (IGA), providing for the sharing and matching of identity information across jurisdictions.[60] The objective of the IGA is to:\n",
      "\n",
      "... facilitate the secure, automated and accountable exchange of identity information, with robust privacy safeguards, in order to prevent identity crime and promote law enforcement, national security, road safety, community safety and service delivery outcomes.[61]\n",
      "\n",
      "The IGA provides for the exchange of identity information through six specified Identity Matching Services, and other services subsequently developed under the auspices of the Agreement. Of the six named services, at least two—the DVS and FVS—are already in operation. The National Identity Security Coordination Group (Coordination Group) is responsible for developing and maintaining the policies and procedures governing access to each of the services. Participating agencies will also enter into a common Participation Agreement which provides the framework within which the agencies negotiate the details of data sharing arrangements.[62]\n",
      "\n",
      "Schedules to the IGA set out the financial contributions from each state and territory as well as the particular agencies that will have access. The ACT’s participation is subject to limitations: as well as providing that its participation must be consistent with the Human Rights Act 2004 (ACT), Schedule G of the IGA states that the Territory will only allow access to its data for certain purposes, and will not participate in the ‘One Person One Licence System’.[63]\n",
      "\n",
      "Information about how the identity-matching scheme will operate is set out in the Key Issues and Provisions section below.\n",
      "\n",
      "State and territory legislation\n",
      "\n",
      "The IGA does not provide agencies with the legal authority to share information through these services—it is intended that this authorisation is to come from the laws of each state and territory. Part 8 of the IGA provides that each jurisdiction will preserve or introduce legislation as necessary, to support the collection, use and disclosure of facial images and related identity information between the parties.\n",
      "\n",
      "Queensland was the first jurisdiction to pass new legislation on this front, with the Police and Other Legislation (Identity and Biometric Capability) Amendment Act 2018 (Qld) enacted in March 2018.[64] This amended a range of transport and policing laws to authorise Queensland’s participation in the identity matching scheme. Following the passage of the Bill, the Queensland Minister for Police and Corrective Services, Mark Ryan stated that the Bill:\n",
      "\n",
      "... will be of real benefit to those tasked with the security of the Commonwealth Games, which represents a once-in-a-lifetime event that will demonstrate to the world the great things Queensland has to offer. We are expecting both international and interstate guests to attend so I encourage the Federal Government and all states and territories to ensure this legislation is passed in time for the Commonwealth Games.[65]\n",
      "\n",
      "However, an evaluation conducted by the Queensland Police Service after the 2018 Gold Coast Commonwealth Games reportedly found problems with the rollout of the system, including the following:\n",
      "\n",
      "Difficulties were experienced in data ingestion into one of the systems with the testing and availability not available until the week Operation Sentinel [the Games security operation] commenced... The inability of not having the legislation passed, both Commonwealth and state, in time for the Commonwealth Games reduced the database from an anticipated 46 million images to approximately eight million.[66]\n",
      "\n",
      "The ABC reported that while police records had been included in the system, images from Queensland’s Department of Transport and other sources had not been used. It also reported that none of the 16 ‘high-priority targets’ requested as part of the operation could be identified, and that halfway through the Games, the system was opened up to ‘basic policing’.[67]\n",
      "\n",
      "In November 2018, NSW Parliament passed the Road Transport Amendment (National Facial Biometric Matching Capability) Act 2018, which amended the Road Transport Act 2013 (NSW) to authorise certain government agencies to share information through the identity-matching scheme.[68] A Parliamentary inquiry into the Bill before it was passed noted that the NSW Government had indicated:\n",
      "\n",
      "... at the present stage Roads and Maritime Services has no plans to access or use the Capability, only to provide information to the hub. However, the witnesses noted that in the future the agency may consider signing up to the One Person One Licence Service...another identity-matching service envisaged under the Intergovernmental Agreement which will be available to assist States in upholding the integrity of driver licence and other identification systems.[69]\n",
      "\n",
      "While no other jurisdiction to date has passed legislation in relation to the scheme, the Minister’s second reading speech notes that five states now have the legislative frameworks in place to implement the IGA.[70] Tasmania has amended its driver licensing Regulations to authorise the disclosure of protected information for the purposes of identity-matching services.[71] Existing laws in South Australia[72] and Victoria[73] are also considered to facilitate implementation of the IGA.[74]\n",
      "\n",
      "Privacy and data security\n",
      "\n",
      "Biometric data and privacy concerns\n",
      "\n",
      "The increasing use of biometric systems and templates has amplified concerns regarding the privacy and data security implications of this technology. In a speech to the Biometrics Institute in 2010, the then Deputy Privacy Commissioner, Timothy Pilgrim stated that the collection and handling of biometric information attracts strong public concern because:\n",
      "\n",
      "... biometric information is about a person's physical characteristics. When we collect biometric information from a person, we are not just collecting information about that person, but information of that person. Biometric information cuts across both information privacy and physical privacy. It can reveal sensitive information about us, including information about our health, genetic background and age, and most importantly, it is intrinsic to each of us.[75]\n",
      "\n",
      "In 2008, the ALRC identified a number of general privacy concerns arising from the use of biometric technologies, including:\n",
      "\n",
      "widespread use of biometric systems enables extensive monitoring of the activities of individuals, particularly where the same form of biometric information is used to identify individuals in a number of different contexts\n",
      "\n",
      "biometric technologies, such as facial recognition technologies, may be used to identify individuals without their knowledge or consent\n",
      "\n",
      "biometric information could be used to reveal sensitive personal information, such as information about a person’s health or religious beliefs\n",
      "\n",
      "the security of biometric systems could be compromised and\n",
      "\n",
      "the accuracy and reliability of many biometric systems remains unknown, creating the potential for serious consequences for an individual who is falsely accepted or rejected by such a system.[76]\n",
      "\n",
      "As noted by the ALRC, particular concerns arise with the collection of facial data, as unlike the collection of fingerprints or DNA, facial images can be captured from a distance and without the knowledge or consent of the individual.[77] Furthermore, faces are difficult to hide or alter, and therefore the misuse of this information can be more prolonged than credit card or tax file number data, which can be replaced.[78]\n",
      "\n",
      "Public discussion and reporting on the Capability has situated it within the broader context of governmental data collection, data-matching and data security. Questions have been raised about the security of data stored and shared as part of the Capability, particularly in light of incidents which have drawn attention to potential vulnerabilities in government and non-government systems.[79] This includes reports in 2017 that the Medicare details of any Australian were being sold to order through a darknet auction site, and a mass data breach at US credit agency Equifax which exposed the personal data of 143 million US customers.[80]\n",
      "\n",
      "Bruce Arnold, a law academic and director of the Australian Privacy Foundation, has argued that Australia’s privacy laws are insufficient to protect against misuse or inadvertent disclosure of biometric information:\n",
      "\n",
      "The sharing occurs in a nation where Commonwealth, state and territory privacy law is inconsistent. That law is weakly enforced, in part because watchdogs such as the Office of the Australian Information Commissioner (OAIC) are under-resourced, threatened with closure or have clashed with senior politicians. Australia does not have a coherent enforceable right to privacy. Instead we have a threadbare patchwork of law (including an absence of a discrete privacy statute in several jurisdictions).[81]\n",
      "\n",
      "Privacy Act and biometric data\n",
      "\n",
      "The proposed identity-matching services will be subject to existing privacy laws. The Privacy Act 1988 (Cth), and the Australian Privacy Principles (APPs) made under this Act regulate the handling of personal information by Commonwealth government agencies as well as private sector organisations with an annual turnover of more than $3 million, all private health service providers and some other small businesses.[82] Most states and territories also have privacy laws regulating their respective public sector agencies.[83]\n",
      "\n",
      "Under the Privacy Act, biometric information used for the purpose of automated biometric verification or identification, as well as biometric templates, is classified as ‘sensitive information’.[84] Sensitive information is generally afforded a higher level of protection than other personal information, in recognition of the adverse consequences which may flow from the inappropriate handling of such information.[85] Limitations include that sensitive information can only be collected with consent (unless a specified exception applies) and can only be used or disclosed for a secondary purpose to which it was collected if this is directly related to the primary purpose of collection.[86] However, it is an exception to these restrictions if the collection, use or disclosure is required or authorised by an Australian law.\n",
      "\n",
      "Notifiable data breaches scheme\n",
      "\n",
      "The Notifiable Data Breaches scheme came into effect on 22 February 2018, and applies to agencies and organisations with obligations under the APPs. It requires entities to notify the Australian Information Commissioner and affected individuals about data breaches which are likely to cause serious harm. The notification must include recommendations about the steps individuals should take in response to the breach.[87]\n",
      "\n",
      "Privacy impact assessments\n",
      "\n",
      "In August 2015, a privacy impact assessment (PIA) was carried out in relation to the design and initial operation of the interoperability hub system, through which agencies can request and share facial image data, during its early stages of development.[88] The PIA, conducted by Information Integrity Solutions Pty Ltd (IIS), found that the hub design process and proposed governance arrangements were generally consistent with the requirements of the APPs. At the same time, it highlighted the broad scope of the Capability and the privacy risks associated with the proposed system as a whole:\n",
      "\n",
      "... it is important to recognise that the Hub will have an impact on the circumstances in which facial biometric information is shared, by whom and the volume of images shared, and these risks will have to be actively managed. There is also the risk, which IIS considers is low, that the Hub and the metadata generated by transactions performed through it could potentially allow for some tracking or surveillance of individuals’ everyday activities. However, it is the view of IIS that the privacy impacts of the whole system could well be greater than the risks at individual agency or Hub level. As such, IIS considers that strong, widely respected governance of the system as a whole as, particularly as it evolves over time, is equally and potentially more important than governance of the individual participating agencies and the Hub.[89]\n",
      "\n",
      "In recognition of these risks, the PIA made a series of recommendations to strengthen privacy practices in the design and operation of the hub. This included limiting the metadata generated by the hub, strictly controlling access to one-to-many matching and clarifying the limits on the initial scope of the Capability, as well as including an independent representative on relevant governance bodies to provide the ‘people’s voice’.[90] The AGD accepted or partially accepted all recommendations, though did not support the suggestion of a people’s representative, stating that the public interest would be represented through the OAIC’s involvement in the Coordination Group, and consultation with state and territory privacy commissioners and/or ombudsmen.[91]\n",
      "\n",
      "In 2016, AGD commissioned an independent PIA on the initial use of the Face Verification Service by federal government departments to access citizenship and visa data held by the (then) Department of Immigration and Border Protection. It reported that the PIA found the exchange of data via the FVS to be ‘privacy positive’, due to the service controlling the disclosure of data and maintaining clear audit trails. The PIA made five recommendations to address privacy risks and concerns that may be heightened with increasing use of the FVS.[92] A copy of the PIA has not been publicly released.\n",
      "\n",
      "A Memorandum of Understanding is currently in place between the OAIC and the Attorney-General’s Department for the OAIC to conduct privacy assessments of:\n",
      "\n",
      "the AGD’s management of the interoperability hub and\n",
      "\n",
      "the governance, operation and information security of the National Driver Licence Facial Recognition Solution, provided for in the IMS Bill.[93]\n",
      "\n",
      "The first report was due to be completed by 1 October 2018, but does not appear to have been publicly released. The second is due by 1 October 2019.[94]\n",
      "\n",
      "Committee consideration\n",
      "\n",
      "Parliamentary Joint Committee on Intelligence and Security\n",
      "\n",
      "A review by the Parliamentary Joint Committee on Intelligence and Security (PJCIS) into the 2018 Bills lapsed at the dissolution of the House of Representatives on 11 April 2019.[95] The inquiry had received 20 submissions and had held two public hearings at the time it lapsed.\n",
      "\n",
      "The PJCIS is currently undertaking a review of the reintroduced Bills, and has accepted as evidence all submissions and transcripts from the previous review.[96] Further details can be found at the inquiry homepage.\n",
      "\n",
      "Senate Standing Committee for the Scrutiny of Bills\n",
      "\n",
      "The Senate Standing Committee for the Scrutiny of Bills has not yet reported on the current Bills, but issued a report on the 2018 Bills on 14 February 2018.[97] A key area of concern identified by the Committee was the privacy implications of the IMS Bill, and the fact that a number of safeguards identified in the explanatory materials (and in the IGA) are not included in the Bill itself.[98] The Committee noted that the IMS Bill’s provisions would:\n",
      "\n",
      "... give a broad power for the Home Affairs department to collect, use and disclose personal information for a wide range of purposes to a wide range of government agencies (and some local government authorities and private entities) ... The Bill has clear implications for the privacy of the millions of individuals whose facial images and other biographical information will be available for collection, use and disclosure.[99]\n",
      "\n",
      "Although acknowledging that the explanatory materials provided a detailed analysis of the Bill’s privacy implications, and set out a number of safeguards to help protect privacy, the Committee raised concerns that the Bill may ‘unduly trespass on personal rights and liberties’ due to the breadth of the authorised disclosures. It noted that potential safeguards such as access criteria, requirements for privacy impact assessments and limitations on the amount of information released by the systems, are contained in the IGA but not in the Bill. The Committee sought the Minister’s advice as to whether the intended policy and administrative safeguards could be included as legal requirements in the Bill, or alternatively whether the Bill could include a requirement that such safeguards be implemented by agencies seeking access to identity-matching services.[100]\n",
      "\n",
      "The Minister for Home Affairs responded to the Committee’s comments on 4 April 2018, and the Committee considered this response in its report on 9 May 2018.[101] On the issue of privacy safeguards, the Minister stated that the protections contained in the Bill, and obligations imposed by the IGA, already provide a ‘strong degree of protection for the information transmitted through the identity-matching services’.[102] He further noted that the identity-matching services will be ‘supported by a broad system of controls and arrangements that govern the provision and use of the services’, with the IMS Bill being just one aspect of this.[103] In response, the Committee reiterated its concerns about the adequacy of safeguards in the IMS Bill.[104]\n",
      "\n",
      "Concerns raised by the Committee in relation to specific provisions are discussed in the Key Issues and Provisions section below.\n",
      "\n",
      "Policy position of non-government parties/independents\n",
      "\n",
      "The Australian Labor Party does not appear to have commented on the Bills directly. The IGA was agreed to by all state and territory leaders, including Labor leaders in Queensland, Victoria, Northern Territory, ACT, Western Australia and South Australia. However, the ACT and Victorian Governments have both stated that the IMS Bill goes beyond the scope of the IGA.[105]\n",
      "\n",
      "At the time the IGA was reached, then Opposition Leader Bill Shorten offered cautious support for the identity-matching system, stating:\n",
      "\n",
      "We think that biometric technology can be a real addition in terms of keeping Australians safe. But of course, when it comes to the final detail, we'll wait to see what the final detail from the Government is. But I just want to reassure Australians that Labor takes a bipartisan approach to good ideas about keeping Australians safe.[106]\n",
      "\n",
      "Shadow Attorney-General, Mark Dreyfus has also stated:\n",
      "\n",
      "... on the face of it, these measures appear sensible; but we will wait to see the detail of what is being proposed ... It is important that the balance between security and privacy is maintained in the face of new threats and there are appropriate protections in place.[107]\n",
      "\n",
      "The Australian Greens have expressed opposition to the measures, with justice spokesperson Senator Nick McKim stating: ‘creating a massive database of people’s photographs is a privacy invasion that creates a honeypot for hackers’.[108]\n",
      "\n",
      "Other minor parties and independents have not commented on the measures to date.\n",
      "\n",
      "Position of major interest groups\n",
      "\n",
      "Civil liberties and privacy organisations have expressed strong concern about the privacy implications of the identity-matching scheme in general. In October 2017, immediately following the signing of the IGA, organisations including the Australian Privacy Foundation, Digital Rights Watch and state and territory civil liberties groups issued a joint statement condemning the creation of a national facial database. The statement described the database as ‘an unnecessary and disproportionate invasion of the privacy rights of all Australians’ and ‘fundamentally incompatible with a free and open society’.[109]\n",
      "\n",
      "These concerns were reiterated in submissions to the PJCIS inquiry in 2018. A number of submissions argued that the IMS Bill is not a proportionate response to the harms it is purporting to address, and may enable substantial infringements on the privacy rights of individuals.[110] A joint submission by Future Wise and the Australian Privacy Foundation contended that the broad purposes of the Bill—which include removing duplicate records and targeting avoidance of traffic fines as well as detecting terrorism—undermine a case for the proportionality of the Bill’s measures:\n",
      "\n",
      "There appears to be no need, for example, to expose all Australian citizens to biometric data matching to remove duplicate records. It is incumbent on government to design other methods of record management that do not involve significant privacy incursions. ... The extent of the law enforcement activities contemplated by the Bill should therefore be re-examined, to be limited to those absolutely necessary for public safety—rather than those that are simply convenient or ‘efficient’.[111]\n",
      "\n",
      "Interest groups have expressed doubts about the adequacy of the governance frameworks for the identity-matching services, and the safeguards contained in the IMS Bill.[112] One particular concern has been that many of the rules for access to the services will be contained in access policies and participation agreements made under the intergovernmental agreement. These are not referenced in the Bill. The Office of the Victorian Information Commissioner expressed concern that managing compliance through such instruments ‘may not be sufficiently robust’, noting that they may not be enforceable and could allow ‘fundamental controls to be amended without parliamentary oversight’.[113] This point was similarly made by the Queensland Office of the Information Commissioner, which submitted that the IMS Bill ‘does not adequately embed into law the core intents of the regime to which the Governments have agreed’.[114]\n",
      "\n",
      "In addition to questions about the adequacy of safeguards built into the scheme, some stakeholders also suggested that Australia’s privacy laws do not provide sufficient protection against possible misuse of information under the scheme.[115] A number of submissions raised the possibility of establishing an independent authority responsible for oversight of the retention, collection and use of biometric information, citing the UK’s creation of a Commissioner for the Retention and Use of Biometric Material.[116]\n",
      "\n",
      "It was also suggested that further information about the identity-matching scheme may be required to enable proper consideration of the IMS Bill. For example, the Law Council of Australia argued that insufficient information is available regarding the technical aspects of scheme:\n",
      "\n",
      "It is difficult ... to comment further on the nature and operation of the Interoperability Hub or various identity matching services as there has been very little information released by the Government on their technical development. ...The Law Council is of the view that additional technical information about the nature of the identity matching services and the process for ensuring that there are not false matches should be released publicly to allow informed debate about the proposed legislation.[117]\n",
      "\n",
      "Other organisations, including Civil Liberties Australia and the Queensland Office of the Information Commissioner, raised concerns that Privacy Impact Assessments have not yet been completed and published in relation to all services referred to in the Bill and the various uses to be made of them.[118]\n",
      "\n",
      "Support for the measures has been largely based on a security rationale. Anthony Bergin, a senior analyst at the Australian Strategic Policy Institute (ASPI), expressed support for the scheme as provided for in the IGA, arguing that ‘most Australians would be surprised to learn that police don’t have this capability and would be disturbed by the heightened risks faced by our law enforcement officers’.[119]\n",
      "\n",
      "Stakeholder comments in relation to specific provisions of the two Bills are discussed under the Key issues and Provisions section below.\n",
      "\n",
      "Financial implications\n",
      "\n",
      "The Explanatory Memorandum to the IMS Bill states that it does not propose any new expenditure and the overall financial impact is low.[120]\n",
      "\n",
      "As indicated in the background, the Capability received funding of $18.5 million over four years in the 2014–15 Mid-Year Economic and Fiscal Outlook. Further funding of $2.5 million was provided in the 2017–18 Budget to complete the Capability’s build.[121]\n",
      "\n",
      "The IGA specifies that the Commonwealth is responsible for the establishment costs for this system and for 50 per cent of annual operating and maintenance costs. It will also be responsible for the ongoing costs of maintaining and operating the DVS hub and interoperability hub.[122] Each state and territory has committed to a specific financial contribution towards the ongoing operating and maintenance costs of the National Driver Licence Facial Recognition Solution.[123]\n",
      "\n",
      "Statement of Compatibility with Human Rights\n",
      "\n",
      "As required under Part 3 of the Human Rights (Parliamentary Scrutiny) Act 2011 (Cth), the Government has assessed the Bills’ compatibility with the human rights and freedoms recognised or declared in the international instruments listed in section 3 of that Act. The Government considers that the Bills are compatible.[124]\n",
      "\n",
      "Parliamentary Joint Committee on Human Rights\n",
      "\n",
      "The Parliamentary Joint Committee on Human Rights has not yet reported on the Bills, but reported on the 2018 Bills on 27 March 2018.[125] The Committee queried whether the measures are a proportionate limitation on the right to privacy, and sought advice from the Minister for Home Affairs (in relation to the IMS Bill) and Minister for Foreign Affairs (in relation to the Passports Bill) on this point.\n",
      "\n",
      "The Committee raised particular concerns about the scope of the IMS Bill and queried whether the provisions governing access to facial images and other biometric data are sufficiently circumscribed for each of the identity matching services.[126] It noted:\n",
      "\n",
      "As the Hub will permit access to driver licences, the personal information of a significant proportion of the adult Australian population will be retained. A centralised facility for searching such large repositories of facial images and biometric data is a very extensive limitation on the right to privacy... There is a serious question as to whether having databases of, and facilitating access to, facial images of a very significant portion of the population in case they are needed is the least rights restrictive approach to achieving the stated objectives of the measure.[127]\n",
      "\n",
      "The Committee also raised questions about the types of information which may be used—such as social media photographs and historical facial images—and the extent to which the hub will effectively protect against misuse of such information, particularly in relation to vulnerable groups.[128] It noted that international human rights case law has raised concerns about the compatibility of biometric data retention programs with the right to privacy, where the programs involve an indiscriminate or open-ended retention of data.[129] It further queried whether the Privacy Act provides an adequate safeguard for the purposes of international human rights law.[130]\n",
      "\n",
      "Key issues and provisions\n",
      "\n",
      "The IMS Bill is intentionally limited in scope—it is not designed to give effect to the spectrum of information-sharing arrangements and procedures envisioned under the IGA. Instead, it should be seen as one piece of a patchwork of laws and policies which will regulate the use of identity-matching services.\n",
      "\n",
      "The Bill establishes an express legal basis for the Department of Home Affairs (DOHA) to provide identity-matching services and places restrictions on the circumstances in which the services may be used and types of information involved. It does not authorise particular agencies to use the services. Organisations seeking access must be authorised to collect, use and disclose identification information by some other federal, state or territory law. They will also need to meet criteria as specified in the IMS Bill, IGA and in various access policies and agreements made under the IGA.\n",
      "\n",
      "How does the system work?\n",
      "\n",
      "Identity-matching facilities\n",
      "\n",
      "The IMS Bill expressly authorises DOHA to develop, operate and maintain two facilities through which identity-matching services are provided. The system is intended to operate based on a ‘hub and spoke’ model, in which the Commonwealth operates the centralised facilities through which state and territory agencies (and other participating entities) communicate with each other to request or provide information.[131] Details about how these facilities will operate is largely contained in the IGA, rather than in the provisions of the Bill.\n",
      "\n",
      "Clause 14 of the Bill provides that DOHA may develop, operate and maintain the interoperability hub, through which agencies and organisations may electronically relay requests for the provision of identity-matching services, and transmit information in response to such requests.[132] Agencies will access the hub (at least initially) via a web-based user interface into which they log in to manually enter search requests. The IGA provides that over time, the hub will also be able to receive requests via ‘system-to system connections with Agencies’ existing systems’.[133] Identification information of an individual is not stored in the hub itself—in his second reading speech for the 2018 IMS Bill, Minister for Home Affairs, Peter Dutton explained:\n",
      "\n",
      "The hub is not a database and does not conduct any facial biometric matching. Rather it acts like a router, transmitting matching requests received from user agencies to facial image databases. These databases conduct the matching using facial recognition software and return a response back via the hub.[134]\n",
      "\n",
      "The second facility provided for in the Bill is the National Driver Licence Facial Recognition Solution (NDLFRS).[135] This is a federated database of the identity information contained in government identification documents, such as (but not necessarily limited to) driver licences.[136] Each state and territory road agency will have its own partitioned data store, with individual agency-based access controls. Unlike the interoperability hub, the NDLFRS will store identification information contributed by state and territory agencies. It will be connected to the interoperability hub to facilitate data sharing with other agencies.[137]\n",
      "\n",
      "The IGA provides that the Commonwealth, though it hosts and operates the database, will not have the ability to view or modify the information within each partitioned data store.[138] However, the Bill itself does not place any express restrictions on DOHA’s ability to access, collect or disclose information held in the system.[139] Furthermore, the NDLFRS will also include common facial biometric matching software and ‘a central store of biometric templates, derived from facial images replicated by the states and territories using the facial biometric matching software’. Both the software and templates will be managed by the Commonwealth Data Hosting Agency (CDHA).[140]\n",
      "\n",
      "Identity-matching services\n",
      "\n",
      "The Bill provides that the interoperability hub is to be used for the purposes of requesting and providing ‘identity-matching services’.[141] Subclause 7(1) states that an identity-matching service is any of the following:\n",
      "\n",
      "a face identification service ( FIS ), defined under subclause 8(1) as a service which involves electronically comparing the facial image of a person with the identification information of one or more persons contained in government identification documents (often referred to as ‘one to many’ matching)[142]\n",
      "\n",
      "( ), defined under as a service which involves electronically comparing the facial image of a person with the identification information of one or more persons contained in government identification documents (often referred to as ‘one to many’ matching)[142] a face verification service ( FVS ), defined under subclause 10(1) as a service comparing the identification information about a person with information contained in a particular government identification document, where a facial image of the person is included in the request and/or in a response to the request (also known as ‘one to one’ matching).[143] Unlike FIS , the service is aimed at verifying—rather than ascertaining—a person’s identity\n",
      "\n",
      "( ), defined under as a service comparing the identification information about a person with information contained in a particular government identification document, where a facial image of the person is included in the request and/or in a response to the request (also known as ‘one to one’ matching).[143] Unlike , the service is aimed at verifying—rather than ascertaining—a person’s identity a facial recognition analysis utility service ( FRAUS ), defined under clause 9 as the electronic comparison of a person’s facial image with identification information about the person supplied by the same state or territory authority, which is included in a database in the NDLFRS. The comparison must be for the purpose of assessing the accuracy or quality of information held by the relevant authority[144]\n",
      "\n",
      "( ), defined under as the electronic comparison of a person’s facial image with identification information about the person supplied by the same state or territory authority, which is included in a database in the NDLFRS. The comparison must be for the purpose of assessing the accuracy or quality of information held by the relevant authority[144] the One Person One Licence service ( OPOLS ), in which a person’s facial image and other identification information is compared with information included in a NDLFRS database, for the purpose of determining whether the person holds multiple government identification documents[145] and\n",
      "\n",
      "( ), in which a person’s facial image and other identification information is compared with information included in a NDLFRS database, for the purpose of determining whether the person holds multiple government identification documents[145] and an identity data sharing service (IDSS), defined under clause 11 as a service, other than the four services listed above, which involves a disclosure of a person’s identification information through the interoperability hub. The disclosure must be between Commonwealth, state or territory authorities and for the purpose of an identity or community protection activity (explained below).[146]\n",
      "\n",
      "Minister’s power to prescribe additional services\n",
      "\n",
      "Additionally, paragraph 7(1)(f) gives the Minister the power to make rules prescribing other services as identity-matching services, where they:\n",
      "\n",
      "involve the collection, use and disclosure of identification information and\n",
      "\n",
      "involve the interoperability hub or NDLFRS.[147]\n",
      "\n",
      "Any such rules are in the form of a disallowable legislative instrument.[148] The Minister may prescribe services which permit access by local government authorities or non-government entities if the purpose of the service is for identity verification and certain other conditions are met (these are discussed under ‘private sector access’).[149] The Bill requires the Minister to consult with the Human Rights Commissioner and Information Commissioner about the proposed rules, though does not provide further guidance as to the nature of any consultation.[150]\n",
      "\n",
      "The Queensland Office of the Information Commissioner has raised concerns that the breadth of the rule-making power under paragraph 7(1)(f) may allow the Minister to prescribe ‘many-to-many’ matching services or blanket surveillance. It has recommended that the provision expressly exclude such services.[151]\n",
      "\n",
      "What information may be shared?\n",
      "\n",
      "Identification information\n",
      "\n",
      "The IMS Bill provides for the collection, use and disclosure of identification information. The scope of this term is set out under clause 5, which provides that it may be information about a living, dead, real or fictitious person and encompasses:\n",
      "\n",
      "current and former names and addresses, place and date of birth, and age (including an age range)\n",
      "\n",
      "the current or former sex, gender identity or intersex status of the person\n",
      "\n",
      "information about whether the person is alive or dead\n",
      "\n",
      "any information contained in or associated with a person’s driver licence, or other licence or identity document issued by a state or territory authority\n",
      "\n",
      "the person’s current or former citizenship, any information about a visa the person holds or has held, and any information contained in or associated with an Australian or foreign travel document and\n",
      "\n",
      "a facial image of the person, biometric template derived from the image or the result of a biometric comparison involving such an image.[152]\n",
      "\n",
      "The Minister may also make rules (in the form of a disallowable legislative instrument) prescribing other types of information to be identification information.[153] Before doing so, the Minister must be satisfied that the information that can be used to identify an individual (whether alone or in conjunction with other information), is reasonably necessary for the provision of an identity-matching service and assists one or more identity or community protection activities. The Minister must also consult with the Human Rights Commissioner and Information Commissioner.[154]\n",
      "\n",
      "Additionally, the IMS Bill specifies information which is not identification information and which therefore cannot be collected, used or disclosed under the Bill. This includes information or an opinion about a person’s:\n",
      "\n",
      "racial or ethnic origin\n",
      "\n",
      "political opinions, philosophical beliefs or religious beliefs or affiliations\n",
      "\n",
      "membership of a political association, professional or trade association or trade union\n",
      "\n",
      "sexual orientation or practices\n",
      "\n",
      "criminal record or\n",
      "\n",
      "health or genetics.[155]\n",
      "\n",
      "However, where information is not primarily one of the above kinds, but nonetheless allows such information about a person to be reasonably inferred (for example, where a person’s racial or ethnic origin may be inferred through their name or place of birth), this may still be identification information and subject to disclosure.[156]\n",
      "\n",
      "What are the limitations on access?\n",
      "\n",
      "As indicated in Minister Coleman’s second reading speech, the IMS Bill does not in itself authorise government agencies or other entities to use identity-matching services, though it provides a broad framework under which the services can operate.[157] An agency or organisation must have a separate legal basis on which it is authorised to disclose information for the purpose of participating in identity-matching services.\n",
      "\n",
      "As indicated above, in addition to legislative authorisation to disclose information, an agency’s ability to access these services will be based on a combination of requirements set out in either or both the Bill and IGA. In particular, the IGA (but not the Bill) provides that participating bodies must meet the criteria set out in the relevant Access Policy, developed by the Coordination Group.\n",
      "\n",
      "Face Verification Service—access policy The Access Policy for the Face Verification Service was issued in June 2017, and provides an example of the criteria an agency must meet in order to participate in identity-matching services. In order to gain access to the FVS, an agency must: provide a statement referencing legislation that provides the legal basis for using and/or disclosing identity information via the FVS\n",
      "\n",
      "that provides the legal basis for using and/or disclosing identity information via the FVS undertake or contribute to a privacy impact assessment (PIA) to account for every information flow which occurs through the FVS, to which the agency is a party (unless the agency’s use of the FVS is exempt from the relevant Commonwealth, state or territory privacy laws)\n",
      "\n",
      "to account for every information flow which occurs through the FVS, to which the agency is a party (unless the agency’s use of the FVS is exempt from the relevant Commonwealth, state or territory privacy laws) enter into an Interagency Data Sharing Arrangement (IDSA) with each agency with which it intends to share information via the FVS. The Access Policy states that where possible, classes of agencies with like functions should enter into common, multilateral agreements\n",
      "\n",
      "with each agency with which it intends to share information via the FVS. The Access Policy states that where possible, classes of agencies with like functions should enter into common, multilateral agreements maintain a register of Nominated Users who are authorised to submit queries via the FVS, ensure the users undertake training in security awareness and privacy obligations, and ensure that any IT systems connected with the hub receive and maintain appropriate security accreditation\n",
      "\n",
      "who are authorised to submit queries via the FVS, ensure the users undertake training in security awareness and privacy obligations, and ensure that any IT systems connected with the hub receive and maintain appropriate security accreditation have an independent audit conducted of all its data sharing via the FVS at least once every financial year and\n",
      "\n",
      "conducted of all its data sharing via the FVS at least once every financial year and enter into a memorandum of understanding with DOHA in relation to the services through the interoperability hub.[158] The content of the IDSA must include details of the IDSA’s agreed duration, arrangements for dispute settlement, non-compliance and termination, as well as arrangements for assigning costs associated with the FVS (where relevant). The IDSA must identify the scope of the data-sharing arrangements (such as the accreditation requirements and access permissions for users, maximum number of Nominated Users and method of access, and agreed maximum number of transactions) and the arrangements for protecting personal information shared via the FVS.[159] DOHA is responsible for reviewing IDSAs to ensure consistency with the Access Policy, and for reviewing audit and compliance reports.\n",
      "\n",
      "Authorisations\n",
      "\n",
      "Although the IMS Bill does not authorise particular agencies to participate in the identity-matching services, Part 3 of the Bill does provide authorisation for DOHA to collect, use and disclose identification information in connection with these services and articulates the scope of the Department’s powers in this area.\n",
      "\n",
      "Clause 17 authorises DOHA to collect identification information where the collection is via an electronic communication to the interoperability hub or the NDLFRS, and for one of the purposes set out in subclause 17(2). The purposes for which collection is authorised include:\n",
      "\n",
      "providing or developing an identity-matching service for the purpose of an identity or community protection activity (explained below)\n",
      "\n",
      "(explained below) developing, operating or maintaining the NDLFRS or\n",
      "\n",
      "protecting a person who has acquired an assumed identity under the Crimes Act 1914 (Cth) or is involved in a Commonwealth, state or territory witness protection program.[160]\n",
      "\n",
      "Clause 18 enables DOHA to use or disclose identification information collected through an electronic communication to the interoperability hub or NDLFRS, or held in or generated using the NDLFRS. Again, the use or disclosure must be for one of the purposes set out in subclause 17(2).\n",
      "\n",
      "Clause 19 specifies that where a state or territory law limits the disclosure of identification information by a state or territory authority (or by a body or person acting on behalf of the authority), but provides an exemption for disclosures authorised by a Commonwealth law, then such an authority, body or person will be permitted to disclose identification information to DOHA for inclusion in the NDLFRS. The Explanatory Memorandum states this is intended to facilitate the disclosure of driver licence data by states and territories, where the existing legislation allows disclosures authorised by Commonwealth law:\n",
      "\n",
      "This is to reduce the number of states and territories that would need to amend their own legislation before Home Affairs could develop the database.[161]\n",
      "\n",
      "Identity or community protection activity\n",
      "\n",
      "As explained above, DOHA will be authorised to collect, use and disclose identification information in developing or providing an identity-matching service for the purpose of an identity or community protection activity. Additionally, certain identity-matching services provided for in the Bill—in particular the FIS and IDSS—can only be accessed in the course of such an activity.\n",
      "\n",
      "Clause 6 provides a definition of identity or community protection activity, as an activity covered by one of the following categories:\n",
      "\n",
      "preventing and detecting identity-related fraud, including the use of stolen or fraudulently obtained government identification documents (or identification information from such documents)[162]\n",
      "\n",
      "law enforcement—that is, the preventing, detecting, investigating or prosecuting an offence against a Commonwealth, state or territory law or in relation to proceedings (or potential proceedings) under the Proceeds of Crime Act 2002[163]\n",
      "\n",
      "national security—conducting an investigation or gathering intelligence relevant to Australia’s national security[164]\n",
      "\n",
      "protective security—promoting the security of an asset, facility or person associated with government, including by checking the background of a person with access to such an asset/facility or by protecting a person under witness protection/with a legally assumed identity[165]\n",
      "\n",
      "community safety—promoting community safety, including by identifying an individual who has suffered or is reasonably believed to be at risk of suffering physical harm or an individual who is reasonably believed to be involved with a significant risk to public health or safety[166]\n",
      "\n",
      "road safety activities, including promoting the integrity of driver licensing systems[167] and\n",
      "\n",
      "verifying the identity of an individual.[168]\n",
      "\n",
      "The Scrutiny of Bills Committee noted the breadth of some of these purposes, arguing that the sharing of information in relation to any federal, state or territory offence, for road safety or for identity information more broadly:\n",
      "\n",
      "... could allow state and territory agencies to share and seek to match facial images and other biographical information for persons suspected of involvement in very minor offences, such as jaywalking, or for verifying the identity of an individual for any purpose.[169]\n",
      "\n",
      "Submissions to the PJCIS inquiry also raised concerns about the breadth of these categories. The joint submission by Future Wise and the Australian Privacy Foundation suggested that terms such as community safety or road safety:\n",
      "\n",
      "... are defined so widely as to potentially draw almost all activities within the Bill’s ambit. The effect is that biometric matching might be deployed for almost any purpose without limit.[170]\n",
      "\n",
      "Australian Lawyers for Human Rights noted that many of the purposes under clause 6 ‘relate not to uncovering of wrongdoing that has already occurred, but ‘prevention’ and ‘promotion’ activities’, and objected to the use of identity-matching services where there is no clear connection to a likely offence.[171]\n",
      "\n",
      "Face identification service (FIS)\n",
      "\n",
      "The FIS, in providing for one-to-many matches, is one of the more controversial measures in the IGA, as it can involve the use and disclosure of images (and other personal information) of multiple persons who may have no connection to the person in the original image. Reflecting this, the IMS Bill and IGA place greater restrictions on use of this service than on the other services which form part of the scheme.\n",
      "\n",
      "One restriction, noted above, is that the FIS can only be used for the purpose of identifying the individual in the original image, or determining whether they have multiple identities, in the course of an identity or community protection activity covered by any of subclauses 6(2) to 6(6).[172] This will capture most categories of the definition of identity and community protection activity set out above, but will not allow access for the purposes of road safety activities or identity verification.\n",
      "\n",
      "This largely reflects the IGA’s list of permitted purposes for which agencies may use the FIS.[173] One notable difference is in relation to the ‘law enforcement activities’ category—the IGA states that where the sharing is between agencies in different jurisdictions, the service may only be used for activities relating to an offence which carries a maximum penalty of at least three years imprisonment.[174] This limitation is not replicated in the Bill. The Explanatory Memorandum notes this but does not explain the reason for the omission, stating:\n",
      "\n",
      "The Bill will not specifically restrict this activity to offences that carry a maximum penalty of not less than three years imprisonment ... but it is intended that this restriction will apply on a policy basis. Any amendment to the provisions of the IGA ... will be by agreement between the Commonwealth and the states and territories. As with all of the identity or community protection activities, state or territory agreement will be required before a jurisdiction’s data can be used in relation to additional offences.[175]\n",
      "\n",
      "The absence of any lower limit in the Bill in regards to offences appears to envision future changes to the IGA that expand the offences for which the FIS may be used. Possibly in connection with this, the IGA provides that twelve months after the FIS commences operation, the Coordination Group will review the definition and operation of the general law enforcement purpose, and ‘should consider whether the definition maximises the utility of the FIS for law enforcement agencies, while maintaining appropriate privacy safeguards’.[176] Without amendments to the IGA, it is unlikely—but theoretically possible—that agencies could use the FIS to ascertain the identity of a person suspected of committing a minor infringement.\n",
      "\n",
      "A second restriction is in relation to who may access the FIS. Subclause 8(2) provides a list of authorised agencies—this includes the Australian Border Force;[177] Australian Crime Commission; Australian Federal Police; ASIO; a federal Department administered by a Minister administering citizenship, migration or passports legislation; and state and territory police forces and anti-corruption agencies. The Minister may prescribe further authorities in the rules, but only where satisfied that the authority has a function previously performed by one of the specified state or territory agencies.[178]\n",
      "\n",
      "Private sector access\n",
      "\n",
      "Another concern that has been raised in relation to the IGA and IMS Bill is the extent to which they allow the private sector to access personal information contained in government databases. The use of identity-matching services by private sector entities and local government authorities will be regulated by a combination of provisions under the IMS Bill, the IGA and access policies developed under the agreement.\n",
      "\n",
      "Restrictions under the Bill\n",
      "\n",
      "The IMS Bill provides that, of the five services expressly provided for under the IGA, non-government entities and local government authorities can potentially access the face verification service (FVS) only. Such organisations will be able to request information about an individual through the FVS if:\n",
      "\n",
      "verifying the individual’s identity is reasonably necessary for one or more of the organisation’s functions or activities\n",
      "\n",
      "the individual has consented to the organisation using and disclosing their identification information for the purpose of verifying their identity\n",
      "\n",
      "the organisation carries on activities in Australia from premises located in Australia, or resides in Australia and\n",
      "\n",
      "either the Privacy Act applies to the organisation, or in the case of a local government authority, it is bound by a state or territory law or has entered into a written agreement with DOHA which provides for the protection of personal information (and means of recourse for affected individuals) comparable to that provided by the Australian Privacy Principles.[179]\n",
      "\n",
      "Restrictions under the IGA\n",
      "\n",
      "Additionally, the IGA states that private sector access to the FVS to match information held by the states and territories is subject to:\n",
      "\n",
      "the express approval of the relevant minister in each state or territory to use their jurisdiction’s information for this purpose\n",
      "\n",
      "the outcomes of a privacy impact assessment covering the types of organisations to be given access\n",
      "\n",
      "compliance with a ‘FVS Commercial Service Access Policy’ developed by the Coordination Group (including a fee for service arrangement) and\n",
      "\n",
      "an FVS Commercial Service audit and compliance program, overseen by the Coordination Group.[180]\n",
      "\n",
      "The Law Council of Australia has argued that these restrictions provided for in the IGA are ‘important safeguards that should be incorporated into the Bill’.[181] Furthermore, it notes that the Bill does not provide for penalties for private organisations where they make an unauthorised use of the hub or identification information, and suggests the existing controls are insufficient.[182]\n",
      "\n",
      "On the issue of consent, the Law Council has suggested that further information is needed as to how informed consent will be recorded and verified to a standard that enables access to the FVS.[183] Other interest groups have questioned the adequacy of this consent requirement. The joint submission to the PJCIS inquiry by the Australian councils for civil liberties, which opposed private sector access to the identity-matching services, argued:\n",
      "\n",
      "In all cases, consent should be valid, free and voluntary. This is quite often not the case when no real choice or alternative is offered and there is little or no opportunity to opt out.[184]\n",
      "\n",
      "The Office of the Victorian Information Commissioner has also raised concerns about private sector and local government access to the scheme, stating:\n",
      "\n",
      "The variation in the quality of governance and security that can be expected, particularly from local government, raises issues in relation to the adequacy of information management practices and personal information protection. The potential for scope creep—in that personal information may be used for additional purposes other than those for which it was initially collected—is also a significant concern.[185]\n",
      "\n",
      "What protections are in place?\n",
      "\n",
      "Disclosure offence\n",
      "\n",
      "The IMS Bill creates an offence of recording or disclosing protected information when the person making the record or disclosure has obtained the information in their capacity as an entrusted person.[186] The maximum sentence for the offence is imprisonment for two years. It is an exception to the offence where the conduct is either authorised by, or in compliance with, a Commonwealth, state or territory law.[187]\n",
      "\n",
      "An entrusted person is defined broadly as:\n",
      "\n",
      "the Secretary or an APS employee in DOHA\n",
      "\n",
      "an officer or employee of a Commonwealth agency or authority, state, territory or foreign government or authority, or public international organisation, whose services are made available to DOHA or\n",
      "\n",
      "a contractor engaged to provide services to DOHA in connection with the interoperability hub or NDLFRS (or officer or employee of such a contractor).[188]\n",
      "\n",
      "Protected information is:\n",
      "\n",
      "identification information obtained from the NDLFRS or from an electronic communication to or from the NDLFRS or interoperability hub\n",
      "\n",
      "information about the making, content or addressing of such an electronic communication, or about identification information held in the NDLFRS or\n",
      "\n",
      "information that enables access to the hub or NDLFRS.[189]\n",
      "\n",
      "The Scrutiny of Bills Committee raised concerns with the provision, in which authorised disclosure of information is an exception to the offence, rather than the offence being drafted to apply only to ‘unauthorised’ disclosures. The Committee has pointed out that the Criminal Code Act 1995 provides that a defendant who wishes to rely on an exception bears an evidential burden.[190] This means that a defendant who believes the disclosure or recording was authorised must raise evidence on this point (though does not need to positively prove the matter). The Committee has noted that the explanatory materials do not address the issue and asked the Minister to advise why an ‘offence-specific defence’ is being used in this instance. It has suggested:\n",
      "\n",
      "... it may be appropriate if proposed subclause 21(1) was amended to provide that a person commits the offence if the conduct is not authorised by, or in compliance with a requirement under, a law of the Commonwealth or of a State or Territory.[191]\n",
      "\n",
      "In response, the Minister stated that if this defence was included as an element of the offence itself, ‘it would be extremely difficult for the prosecution to establish that the conduct was not authorised under any law’, whereas an entrusted person should be aware of the legislative basis on which they are relying when disclosing information.[192] The Minister suggested the Bill ensures that in handling protected information, the onus is on an entrusted person to show a level of care commensurate with the sensitivity of the information.[193] The Committee requested that this information be included in the Explanatory Memorandum, and reiterated its concerns about the appropriateness of reversing the evidential burden of proof in this case.[194] The Explanatory Memorandum for the 2019 Bill does not provide further information on this point.\n",
      "\n",
      "When will disclosure be authorised?\n",
      "\n",
      "Clauses 22 to 25 set out circumstances in which the recording and disclosure of protected information will be authorised, and therefore act as exceptions to the disclosure offence under clause 21. An entrusted person may disclose or record protected information:\n",
      "\n",
      "for the purposes of the Identity-matching Services Act 2018 or in the course of exercising powers or performing functions or duties in relation to the interoperability hub or NDLFRS[195]\n",
      "\n",
      "if the person reasonably believes the disclosure is necessary to lessen or prevent a serious and imminent threat to the life or health of an individual, and makes the disclosure for this purpose[196]\n",
      "\n",
      "where the disclosure is to the Integrity Commissioner in relation to a corruption issue (within the meaning of the Law Enforcement Integrity Commissioner Act 2006)[197] or\n",
      "\n",
      "where the information relates to the affairs of a person and the person has consented to the recording or disclosure (and the recording or disclosure is in accordance with that consent).[198]\n",
      "\n",
      "Minister’s rule-making power and the obligation to consult\n",
      "\n",
      "Clause 30 provides that the Minister may, by legislative instrument, make rules prescribing matters:\n",
      "\n",
      "required or permitted by the Act to be prescribed by the rules or\n",
      "\n",
      "necessary and convenient to carry out or give effect to the Act.\n",
      "\n",
      "There are some specified limitations on the rules—they cannot create an offence or civil penalty; provide powers of arrest or detention, entry, search or seizure; impose a tax or create an appropriation; or directly amend the text of the Act.[199] The rules are subject to disallowance as well as sunsetting.[200]\n",
      "\n",
      "As explained above, in exercising his power to make rules prescribing additional types of identification information or additional identity-matching services, the Minister will be required to consult the Information Commissioner and Human Rights Commissioner.[201]\n",
      "\n",
      "The Scrutiny of Bills Committee welcomed the Bill’s inclusion of this requirement to consult. However, the Committee suggested that the requirement be strengthened by making such consultation a condition of the validity of the legislative instrument. [202] The Committee also queried the inclusion of significant matters such as this in a rule rather than in Regulations, noting that Regulations are subject to a higher level of executive scrutiny as they must be drafted by the Office of Parliamentary Counsel and approved by the Federal Executive Council.[203]\n",
      "\n",
      "The Law Council raised similar concerns, suggesting that there are risks that through these provisions, the scope of the identity-matching scheme could be determined by delegated rather than primary legislation. It has also queried whether either the Australian Human Rights Commission or Office of the Australian Information Commissioner are sufficiently resourced to take on this additional consultation role.[204] The Law Council recommended that the consultation requirement be amended to include a requirement for the Minister to report to the public on the results of these consultations, and any reasons for departing from advice provided by the commissioners, before making a relevant rule.[205]\n",
      "\n",
      "In response to the concerns raised by the Scrutiny of Bills Committee, the Minister accepted the Committee’s recommendation that the Minister be required to have regard to any submissions made by the commissioners prior to making the rules, and if the rules depart from the commissioners’ advice, provide reasons for this. He indicated he would propose Government amendments to this effect.[206] However, no changes have been made to the 2019 IMS Bill to incorporate such a requirement. On the question of the appropriateness of rules rather than Regulations, the Minister pointed to the Office of Parliamentary Counsel’s Drafting Direction No. 3.8 – Subordinate Legislation, which provides that its starting point is that subordinate instruments should be made in the form of legislative instruments (as distinct from Regulations), and noted that the Bill expressly prohibits certain matters from being prescribed in rules.[207] The Committee stated it would make no further comment on the matter.[208]\n",
      "\n",
      "Annual reporting requirement\n",
      "\n",
      "Clause 28 requires the Secretary of DOHA to give a report to the Minister at the end of each financial year, for tabling in each House of Parliament, with statistics relating to all requests from Commonwealth, state and territory authorities (except ASIO) for an FIS, FVS or OPOLS. The statistics are to be broken down by requesting authority, service requested, number of requests in which information (or confirmation of identity) was provided and those in which no information or confirmation was provided, and in the case of the FIS, the kind of identity or community protection activity for which the service was requested.[209]\n",
      "\n",
      "The Secretary must similarly report statistics on requests made by non-government entities for an FVS. However, this data is not required to identify the particular organisations, but rather the total number of requests and total number of entities (as well as the number in which information was or was not provided).[210]\n",
      "\n",
      "Additionally, for each government authority (other than ASIO) which used an IDSS to disclose or collect identification information, the Secretary must provide the name of the authority, a brief description of the nature of the information and an indication whether the authority collected or disclosed that information.[211] The report must also include any other information required by the Minister in relation to an identity-matching service or administration of the Act.[212]\n",
      "\n",
      "Subclause 28(2) provides that the report must not ‘unreasonably’ disclose personal information about an individual. The Explanatory Memorandum notes that this is aimed at ensuring the report does not disclose personal information ‘that is not reasonably required for accountability purposes’.[213] It states that this is not intended to prevent the inclusion of publicly available information about an individual.[214]\n",
      "\n",
      "A number of stakeholders and interest groups have suggested that this reporting requirement be further strengthened. The Office of the Victorian Information Commissioner has noted that clause 28 does not expressly require reporting on data breaches or misuse of the services:\n",
      "\n",
      "... it tells the public about the quantum of requests but little about the security of the data or the compliance of participants in the IMS ecosystem.[215]\n",
      "\n",
      "Noting that the new Notifiable Data Breaches scheme will not capture all agencies and bodies accessing the identity matching services (such as state and territory government organisations), the Office suggested that another mechanism be inserted into the Bill to include specific reporting relating to instances of unauthorised or inappropriate access and the remedial action taken in response.[216] It suggests that the complex nature of the identity-matching scheme makes this particularly important:\n",
      "\n",
      "...The inter-related nature of the Bill, the IGA and the other agreements also makes assurance of compliance activities more complex, and is another reason for more transparent reporting.[217]\n",
      "\n",
      "The Law Council has criticised the fact that the reporting requirements do not capture non-government entities or ASIO. Although noting that the Explanatory Memorandum states this is due to considerations of commercial confidentiality, it has argued that ‘the public have a right to know which non-government entities have access to the Face Verification Service’.[218] It has further suggested that restrictions on the reporting of ASIO-related data ‘should be determined on a case by case basis and not included ... as a blanket exception’.[219] The Queensland Office of the Information Commissioner has similarly recommended that the reporting requirement be expanded to capture data breaches and incidents as well as non-government access to the FVS.[220]\n",
      "\n",
      "The Scrutiny of Bills Committee queried whether the reporting requirement should be extended to capture instances where information is disclosed pursuant to clause 23 (disclosures to lessen or prevent a threat to life or health) or clause 24 (disclosures relating to a corruption issue).[221] In response, the Minister accepted the suggestion in relation to clause 23, and indicated that he would propose an amendment to the Bill to accommodate this.[222] However, no such change has been included in the 2019 IMS Bill. In relation to reporting on information disclosed pursuant to clause 24, the Minister noted that such a requirement could jeopardise the confidentiality of disclosures, which may occur without the Secretary’s knowledge, and that the Integrity Commissioner already has reporting requirements in relation to these types of disclosures under the Law Enforcement Integrity Commissioner Act 2006.[223] The Committee requested this information be included in the Explanatory Memorandum, and stated it would not comment further on the matter.[224] The Explanatory Memorandum for the 2019 IMS Bill does not include further information on this point.\n",
      "\n",
      "Statutory review\n",
      "\n",
      "The IMS Bill requires the Minister to cause a review of the operation of the Act and the provision of identity-matching services to be started within five years of the Act’s commencement.[225] The report is to be tabled in each House of Parliament within 15 sitting days after it is received by the Minister.\n",
      "\n",
      "This is a longer timeframe than specified in the IGA, which provides that a general review into the operation of the identity-matching services will be conducted three years from the commencement of the agreement. The IGA states that the review is to assess matters including the effectiveness of the services in progressing the objectives of the agreement, the effectiveness of governance arrangements, the privacy impacts and effectiveness of privacy safeguards in protecting personal information.[226] The terms of reference are to be set by the Coordination Group and the review is to be published online by the Commonwealth.\n",
      "\n",
      "It is unclear whether the review provided for in the Bill is intended to be separate to that in the IGA, and the explanatory materials do not directly discuss this point. The Explanatory Memorandum states that a five year timeframe is necessary as:\n",
      "\n",
      "... it may take some time for all of the states and territories to commence participation in the identity-matching services, and sufficient operating time is needed to ensure that the functioning of the services in relation to all jurisdictions can be assessed adequately.[227]\n",
      "\n",
      "The Queensland Office of the Information Commissioner has stated it would be preferable for the review to commence two years after commencement of the legislation, noting that this was recommended by the Queensland Parliamentary Legal Affairs and Community Safety Committee following its consideration of the Queensland Bill.[228] It has also suggested that it may be appropriate for the IMS Bill to specify ‘critical components’ of the review, such as ‘expansion of services within the IMS regime, abuse of the system, mistakes arising from false positives ,[and] unintended outcomes of the IMS’.[229]\n",
      "\n",
      "Passports Bill\n",
      "\n",
      "Identity-matching capability\n",
      "\n",
      "The Passports Bill amends the Passports Act to allow for the disclosure of personal information in relation to identity-matching services. Currently, section 46 of that Act provides that the Minister for Foreign Affairs may disclose personal information for a number of specified purposes—this includes law enforcement, confirming or verifying information about a passport applicant or facilitating a person’s international travel.[230] Disclosure is limited to the types of information and persons specified by the Minister under the Australian Passports Determination 2015, and this is dependent on the particular purpose of disclosure.[231] There are currently three classes of information which may be disclosed (though not in all circumstances):\n",
      "\n",
      "data page information , which means information contained on the data page of an Australian travel document, such as the document number, expiry date, and the name, data of birth, photograph and signature of the document holder\n",
      "\n",
      ", which means information contained on the data page of an Australian travel document, such as the document number, expiry date, and the name, data of birth, photograph and signature of the document holder status information , which means information about whether the document is currently valid, including whether it has been lost or stolen or has restrictions on its use and\n",
      "\n",
      ", which means information about whether the document is currently valid, including whether it has been lost or stolen or has restrictions on its use and authenticity information, which is information necessary to establish the authenticity of a person applying for or holding an Australian travel document.[232]\n",
      "\n",
      "Item 1 of the Passports Bill inserts proposed paragraph 46(da) into the Passports Act to provide that the Minister may disclose personal information for the purposes of participating in a service to share or match information relating to a person’s identity. The service must be specified or of a kind specified in the Minister’s determination.\n",
      "\n",
      "The amendment does not appear to significantly expand the Minister’s power to disclose personal information—section 46 already permits the disclosure of photographs to a wide range of federal, state and territory government agencies as well as Interpol and foreign border authorities. Proposed paragraph 46(da), in providing a broad authority for disclosures expressly in relation to identity-matching services, will cover any existing gaps which might limit DFAT’s capacity to participate in identity-matching services.\n",
      "\n",
      "Computerised decision-making\n",
      "\n",
      "Item 3 of the Passports Bill inserts proposed section 56A into the Passports Act to provide for computerised decision-making. This empowers the Minister to arrange for the use of computer programs to make decisions or exercise other powers of the Minister under the Act (or associated legislative instruments). The Minister is taken to have made the decision or exercised the relevant power that was made or exercised by the computer program.[233] Proposed subsection 56A(3) enables the Minister to substitute a decision for a decision made by a computer program, where satisfied that the decision made by the computer program is incorrect.\n",
      "\n",
      "The Explanatory Memorandum provides that it is intended that automation will be used for ‘low-ris\n"
     ]
    }
   ],
   "source": [
    "print(texts[filename][\"newspaper3k\"][\"doc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fa5bfdbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.aph.gov.au/Parliamentary_Business/Bills_Legislation/bd/bd1920a/20bd021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "title                 AUSTRALIAN IDENTITY-MATCHING SERVICES BILL\n",
       "country                                                Australia\n",
       "documentUrl    https://www.aph.gov.au/Parliamentary_Business/...\n",
       "startDate                                                 2019.0\n",
       "endDate                                                      NaN\n",
       "oecdId                                                     26841\n",
       "Name: 120, dtype: object"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = oecd.loc[int(filename[:-5])]\n",
    "print(tmp[\"documentUrl\"])\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65b82a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
